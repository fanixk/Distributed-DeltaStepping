<?xml version="1.0"?>
<doc>
    <assembly>
        <name>MPI</name>
    </assembly>
    <members>
        <member name="T:MPI.AttributeDuplication">
            <summary>
            Enumeration describing how a given attribute should be copied
            (or not) when the communicator is cloned (duplicated).
            </summary>
        </member>
        <member name="F:MPI.AttributeDuplication.None">
            <summary>
            When the communicator is duplicated, this value is not copied.
            
            For value types, small value types (less than the size of 
            a native pointer) will be kept in the low-level MPI communicator.
            With larger value types, the low-level MPI communicator's 
            attribute will store a pointer to the value, which will be
            automatically allocated and deallocated as appropriate. This
            behavior can be used to interoperate with C MPI programs that
            store attributes on the communicator.
            </summary>
        </member>
        <member name="F:MPI.AttributeDuplication.Shallow">
            <summary>
            When the communicator is duplicated, the new communicator will
            hold a "shallow" copy of the underlying data, so that both
            communicators reference the same value. 
            
            For value types smaller than a native pointer, the value will
            be stored directly in the low-level communicator. This approach 
            can be used for interoperability with C MPI programs that store
            primitive types or MPI handles directly in the attributes.
            
            For object types and "large" value types, the value will be 
            stored on the (garbage-collected) heap. Copying entails
            creating new references to the object.
            </summary>
        </member>
        <member name="F:MPI.AttributeDuplication.Deep">
            <summary>
            When the communicator is duplicated, the new communicator will
            contain a completely-new, distinct copy of the underlying data.
            
            For value types, each low-level MPI communicator will store a 
            pointer to the data, which will be deallocated when the
            MPI communicator is destroyed. This approach can be used for 
            interoperability with C MPI programs that store attributes as 
            pointers to primitives or structures.
            
            For class types, MPI.NET will use the <c>ICloneable</c> interface
            to clone objects for a deep copy.
            </summary>
        </member>
        <member name="T:MPI.Attribute">
            <summary>
            Attributes are key/value pairs that can be attached to communicators, 
            and are generally used to maintain communicator-specific information 
            across different libraries or different languages.
            </summary>
            
            <remarks>Each instance
            of the <c>Attribute</c> class is the "key" for a different kind of
            data, and can be used to index the <see cref="T:MPI.AttributeSet"/> associated
            with a communicator to query or modify the corresponding value. Each
            attribute is created with the type of data its value will store (e.g., 
            an integer, a string, an object) and a duplication policy, which 
            describes how a value of that type is propagated when communicators
            are cloned.
            
            <para>
              Attributes can be used for interoperability with native code. Any
              attribute whose type is a value type (e.g., primitive type or structure)
              that has either deep-copy or no-copy semantics will be stored on the
              low-level MPI communicator. These attributes can then be accessed by
              native code directly via the low-level MPI interface.
            </para>
            </remarks>
        </member>
        <member name="M:MPI.Attribute.#ctor(MPI.AttributeDuplication)">
            <summary>
            Creates a new attribute. Users need to use the factory function <see cref="M:MPI.Attribute.Create``1(MPI.AttributeDuplication)"/>.
            </summary>
            <param name="duplication"></param>
        </member>
        <member name="M:MPI.Attribute.Create``1(MPI.AttributeDuplication)">
            <summary>
            Create a new attribute, which can be attached to MPI communicators.
            </summary>
            <typeparam name="T">
              The type of value that will be stored with the attribute.
            </typeparam>
            <param name="duplication">
              Describes how (or if) an attribute is copied when a communicator
              is cloned.
            </param>
            <returns>
              A new attribute whose value type is <c>T</c>. This attribute
              can be attached to any communicator.
            </returns>
        </member>
        <member name="M:MPI.Attribute.Dispose">
            <summary>
            Dispose of this attribute. 
            This operation should not be performed until the attribute has
            been deleted from all communicators.
            </summary>
        </member>
        <member name="M:MPI.Attribute.Update(System.Object,System.Object)">
            <summary>
            Updates the object stored in <paramref name="storedValue"/> with a new
            <paramref name="value"/>, and returns the new stored value.
            </summary>
            <param name="storedValue">The stored value to update.</param>
            <param name="value">The new value.</param>
            <returns>The new value to store.</returns>
        </member>
        <member name="F:MPI.Attribute.Duplication">
            <summary>
            How this attribute will be transferred when a communicator
            is cloned/duplicated (via <see cref="M:MPI.Communicator.Clone"/>).
            </summary>
        </member>
        <member name="T:MPI.LowLevelAttribute">
            <summary>
            An attribute attached to the low-level communicator. This abstract class
            only exists so that we can deal with the various different instantiations of
            the <see cref="T:MPI.StoredAttribute`1"/> class without having to know the
            type <c>T</c>.
            </summary>
        </member>
        <member name="M:MPI.LowLevelAttribute.#ctor(MPI.AttributeDuplication)">
            <summary>
            Construct a low-level attribute object. 
            </summary>
        </member>
        <member name="M:MPI.LowLevelAttribute.Set(System.Int32,System.Object)">
            <summary>
            Set this value's attribute to a particular value.
            </summary>
            <param name="comm">The communicator to modify.</param>
            <param name="value">The value to store.</param>
        </member>
        <member name="M:MPI.LowLevelAttribute.Get(System.Int32)">
            <summary>
            Retrieve this attribute's value from a particular communicator.
            </summary>
            <param name="comm">The communicator to query.</param>
            <returns>The object stored in the communicator, if any.</returns>
        </member>
        <member name="M:MPI.LowLevelAttribute.Remove(System.Int32)">
            <summary>
            Remove this attribute from a particular communicator.
            </summary>
            <param name="comm">The communicator storing the attribute.</param>
        </member>
        <member name="M:MPI.LowLevelAttribute.GetIntPtr(System.Int32)">
            <summary>
            Retrieve the value of this attribute in a communicator,
            if it exists.
            </summary>
            <param name="comm">The communicator to query.</param>
            <returns>The value of this attribute in the communicator, if any.</returns>
        </member>
        <member name="M:MPI.LowLevelAttribute.SetIntPtr(System.Int32,System.IntPtr)">
            <summary>
            Set the value of this attribute in a communicator.
            </summary>
            <param name="comm">The communicator to modify.</param>
            <param name="value">The new value.</param>
        </member>
        <member name="F:MPI.LowLevelAttribute.keyval">
            <summary>
            Internal key value. This value will be the key value returned from
            <see cref="M:MPI.Unsafe.MPI_Keyval_create(MPI.Unsafe.MPI_Copy_function,MPI.Unsafe.MPI_Delete_function,System.Int32@,System.IntPtr)"/>. 
            </summary>
        </member>
        <member name="T:MPI.StoredAttribute`1">
            <summary>
            An attribute with value type <c>T</c> stored on the low-level communicator.
            </summary>
            <typeparam name="T">The type of value stored with the attribute.</typeparam>
        </member>
        <member name="M:MPI.StoredAttribute`1.#ctor(MPI.AttributeDuplication,System.Boolean)">
            <summary>
            Creates a new attribute that will be stored inside the low-level communicator.
            </summary>
            <param name="duplication">How this attribute will be duplicated.</param>
            <param name="onHeap">Whether this attribute will be allocated on the heap.</param>
        </member>
        <member name="M:MPI.StoredAttribute`1.ShallowCopy(System.Int32,System.Int32,System.IntPtr,System.IntPtr,System.IntPtr,System.Int32@)">
            <summary>
            Copies an attribute value or pointer directly. This is the C# equivalent of MPI's 
            <c>MPI_DUP_FN</c>, which we don't have access to from within C#.
            </summary>
        </member>
        <member name="M:MPI.StoredAttribute`1.DeepCopy(System.Int32,System.Int32,System.IntPtr,System.IntPtr,System.IntPtr,System.Int32@)">
            <summary>
            Makes a "deep" copy of an attribute that is referenced by a pointer.
            </summary>
        </member>
        <member name="M:MPI.StoredAttribute`1.DeleteAttributeMemory(System.Int32,System.Int32,System.IntPtr,System.IntPtr)">
            <summary>
            Delete the memory associated with an attribute allocated on the heap.
            Use as a parameter to <see cref="M:MPI.Unsafe.MPI_Keyval_create(MPI.Unsafe.MPI_Copy_function,MPI.Unsafe.MPI_Delete_function,System.Int32@,System.IntPtr)"/>
            </summary>
        </member>
        <member name="M:MPI.StoredAttribute`1.Dispose">
            <summary>
            Free the low-level attribute associated with this attribute.
            </summary>
        </member>
        <member name="F:MPI.StoredAttribute`1.onHeap">
            <summary>
            Whether the attribute's value is stored on the heap or not.
            <para>
              When the attribute is stored on the heap, a pointer to the
              value is stored in the low-level communicator, and will be 
              freed when the attribute is deleted.
            </para>
            
            <para>
              When the attribute is not stored on the heap, the value itself
              will be placed into the communicator directly. The value
              must not be larger than the size of an <c>IntPtr</c>.
            </para>
            </summary>
        </member>
        <member name="T:MPI.ShallowValueAttribute`1">
            <summary>
            A value-type attribute allocated with shallow copy semantics. These attributes 
            are stored as (boxed) objects in the <see cref="T:MPI.AttributeSet"/>. When we update
            their values, we copy those values directly into the object, rather than 
            creating a new object, so we achieve shallow semantics.
            </summary>
            <typeparam name="T">The value type of the attribute.</typeparam>
        </member>
        <member name="T:MPI.AttributeSet">
            <summary>
            Contains the attributes attached to a communicator.
            </summary>
            
            <remarks>Each communicator
            can contain key/value pairs with extra information about the communicator
            that can be queried from other languages and compilers. The keys in the 
            attribute set are instances of the <see cref="T:MPI.Attribute"/> class, each of 
            which will be associated with a specific type of value. The values associated
            with any attribute can be added, modified, queried, or removed for a 
            particular communicator.
            
            <para>
              When a communicator is cloned, the attributes are copied to the new
              communicator. When creating an <see cref="T:MPI.Attribute"/>, decide whether
              not to copy the attribute (<see cref="F:MPI.AttributeDuplication.None"/>),
              to copy only a reference to the attribute (<see cref="F:MPI.AttributeDuplication.Shallow"/>),
              or make a clone of the attribute (<see cref="F:MPI.AttributeDuplication.Deep"/>).
            </para>
            </remarks>
        </member>
        <member name="M:MPI.AttributeSet.Remove(MPI.Attribute)">
            <summary>
            Remove the given attribute from this communicator. If not such attribute exists,
            the communicator will not be changed and no exception will be thrown.
            </summary>
            <param name="attribute">The attribute to remove.</param>
        </member>
        <member name="M:MPI.AttributeSet.CopyAttributesFrom(MPI.AttributeSet)">
            <summary>
            Copies the attributes from the communicator being duplicated to
            this attribute set (which is associated with the new communicator).
            </summary>
            <param name="otherAttributes">
              The attributes from the communicator being duplicated.
            </param>
        </member>
        <member name="F:MPI.AttributeSet.comm">
            <summary>
            The low-level MPI communicator with which this attribute
            set is associated.
            </summary>
        </member>
        <member name="F:MPI.AttributeSet.objectAttributes">
            <summary>
            Extra attributes not stored within the low-level MPI communicator.
            </summary>
        </member>
        <member name="P:MPI.AttributeSet.Item(MPI.Attribute)">
            <summary>
            Access or modify the value associated with the given attribute.
            </summary>
            <param name="attribute">The attribute key.</param>
            <returns>
              The value associated with the given attribute, 
              or <c>null</c> if the attribute hasn't been set for this 
              communicator.
            </returns>
        </member>
        <member name="T:MPI.Comparison">
            <summary>
              The result of a comparison between two MPI objects.
            </summary>
        </member>
        <member name="F:MPI.Comparison.Identical">
            <summary>
            The two MPI objects are identical.
            </summary>
        </member>
        <member name="F:MPI.Comparison.Congruent">
            <summary>
            The two MPI objects are not identical, but they have the same properties.
            For example, two <see cref="T:MPI.Communicator"/>s that contain the same set of
            processes with the same ranks.
            </summary>
        </member>
        <member name="F:MPI.Comparison.Similar">
            <summary>
            The two MPI objects are similar, but are not identical and do not match
            exactly. For example, two <see cref="T:MPI.Communicator"/>s that contain the 
            same set of processes but with different rank order.
            </summary>
        </member>
        <member name="F:MPI.Comparison.Unequal">
            <summary>
            The two MPI objects are different.
            </summary>
        </member>
        <member name="T:MPI.CartesianCommunicator">
            <summary>
              A CartesianCommunicator is a form of <see cref="T:MPI.Intracommunicator"/> whose 
              processes are arranged in a grid of arbitrary dimensions.
            </summary>
            
            <remarks>
            Each node in a CartesianCommunicator has 
            not only a rank but also coordinates indicating its place in an n-dimensional grid. 
            Grids may be specified as periodic (or not) in any dimension, allowing cylinder and 
            torus configurations as well as simple grids. Cartesian communicators are often used in
            applications where the data is distributed across a logical grid of the processes. These
            applications can become both simpler and more efficient due to the use of Cartesian 
            communicators, which provide the ability to translate between process coordinates and ranks
            and can provide improved placement of MPI processes on the processing elements on the
            parallel computer.
            </remarks>
        </member>
        <member name="T:MPI.TopologicalCommunicator">
            <summary>
              A communicator with extra topological information that describes the typical
              communication patterns among the processes.
            </summary>
            
            <remarks>
            This class is abstract, as topological information in MPI is provided either via 
            a Cartesian topology (<see cref="T:MPI.CartesianCommunicator"/>) or an arbitrary Graph
            topology (<see cref="T:MPI.GraphCommunicator"/>).
            </remarks>
        </member>
        <member name="T:MPI.Intracommunicator">
            <summary>
              Intracommunicators provide communication among a set of MPI processes.
            </summary>
              
            <remarks>
              Intracommunicators are the most commonly used form of communicator in MPI. 
              Each intracommunicator contains a set of processes, each of which is identified by its
              "rank" within the communicator. The ranks are numbered 0 through Size-1. 
              Any process in the communicator can send a message to another process
              within the communicator or receive a message from any other process in 
              the communicator. Intracommunicators also support a variety of collective operations
              that involve all of the processes in the communicator. Most MPI communication occurs
              within intracommunicators, with very few MPI programs requiring intercommunicators.
            </remarks>
        </member>
        <member name="T:MPI.Communicator">
            <summary>
              Provides communication among a set of MPI processes.
            </summary>
            
            <remarks>
              The communicator class abstracts a set of communicating processes in MPI.
              Communicators are the most fundamental types in MPI, because they are the basis
              of all inter-process communication. Each communicator provides a separate communication
              space for a set of processes, so that the messages transmitted with one communicator 
              will not collide with messages transmitted with another communicator. As such, different
              libraries and different tasks can all use MPI without colliding, so long as they are using
              different communicators. There are two important kinds of communicators: intracommunicators 
              and intercommunicators.
            
              <para><see cref="T:MPI.Intracommunicator"/>s are the most commonly used form of communicator. 
              Each intracommunicator contains a set of processes, each of which is identified by its
              "rank" within the communicator. The ranks are numbered 0 through <see cref="P:MPI.Communicator.Size"/>-1. 
              Any process in the communicator can <see cref="M:MPI.Communicator.Send``1(``0,System.Int32,System.Int32)"/> a message 
              to another process
              within the communicator or <see cref="M:MPI.Communicator.Receive``1(System.Int32,System.Int32)"/> a message from any other process in 
              the communicator. Intracommunicators also support a variety of collective operations
              that involve all of the processes in the communicator. Most MPI communication occurs
              within intracommunicators, with very few MPI programs requiring intercommunicators.</para>
            
              <para><see cref="T:MPI.Intercommunicator"/>s differ from intracommunicators in that intercommunicators
              contain two disjoint groups of processes, call them A and B. Any process in group A can send
              a message to or receive a message from any process in group B, and vice-versa. However, there
              is no way to use an intercommunicator to send messages among the processes within a group.
              Intercommunicators are often useful in large MPI applications that tie together many, smaller
              modules. Typically, each module will have its own intracommunicators and the modules will 
              interact with each other via intercommunicators.</para>
            </remarks>
        </member>
        <member name="M:MPI.Communicator.#ctor">
            <summary>
            Communicators can only be constructed from other communicators or adopted
            from low-level communicators.
            </summary>
        </member>
        <member name="M:MPI.Communicator.Adopt(System.Int32)">
            <summary>
              Adopts a low-level MPI communicator that was created with any of the low-level MPI facilities.
              The resulting <c>Communicator</c> object will manage the lifetime of the low-level MPI communicator,
              and will free the communicator when it is disposed or finalized.
            </summary>
            <remarks>
              This constructor should only be used in rare cases where the program 
              is creating MPI communicators through the low-level MPI interface.
            </remarks>
        </member>
        <member name="M:MPI.Communicator.AttachToComm">
            <summary>
            Attaches this communicator to the low-level MPI communicator in <c>comm</c>,
            setting up the shadow communicator and error handlers as appopriate.
            </summary>
        </member>
        <member name="M:MPI.Communicator.Finalize">
            <summary>
            Finalizer that frees the MPI communicator.
            </summary>
        </member>
        <member name="M:MPI.Communicator.Dispose">
            <summary>
            Free the MPI communicator.
            </summary>
        </member>
        <member name="M:MPI.Communicator.Create(MPI.Group)">
            <summary>
              Creates a new communicator containing all of the processes in the given group. 
              The resulting communicator may be <c>null</c>, if the calling process is not in this group.
            </summary>
            <param name="group">
              The group of processes that will be contained in the resulting communicator.
              This must be a subgroup of <see cref="P:MPI.Communicator.Group"/>.
            </param>
        </member>
        <member name="M:MPI.Communicator.Clone">
            <summary>
              Clones the communicator, creating a new (but distinct) communicator with the
              same processes. The attributes associated with the communicator are copied
              to the new communicator; which attributes are copied (and how) is dependent
              on how the <see cref="T:MPI.Attribute"/>s were created.
            </summary>
        </member>
        <member name="M:MPI.Communicator.Split(System.Int32,System.Int32)">
            <summary>
              Splits a communicator into several different communicators, each of which is identified
              by a particular color value. 
            </summary>
            <remarks>
              This is a collective operation, where each process may provide a different 
              <paramref name="color"/> and <paramref name="key"/>. Each distinct color value will 
              correspond to a new communicator, which contains all processes that provided that 
              specific color. The ranks of the processes in the resulting communicators will be 
              determined by the key value, with the lower ranks in the communicator assigned to
              the processes providing the smaller key values; ties are broken based on the ranks of
              the processes in the original communicator.
            </remarks>
            <param name="color">
               A non-negative color value, designating which new communicator the calling process
               will belong two. All processes that provide the same color will be a part of the 
               same resulting communicator.
            </param>
            <param name="key">
              A value that determines how processes with the same color are ranked in the resulting
              communicator. Smaller keys correspond to lower ranks, with ties broken by the ranks of
              the processes in the original communicator.
            </param>
            <returns>
              The communicator that the calling process is in, corresponding to its color value.
            </returns>
        </member>
        <member name="M:MPI.Communicator.Abort(System.Int32)">
            <summary>
            Terminates all processes in this communicator. In most systems, this terminates all processes.
            </summary>
            <param name="errorcode">An error code that will be returned to the invoking environment.</param>
        </member>
        <member name="F:MPI.Communicator.world">
            <summary>
              The "world" communicator contains all of the processes that were
              originally created by MPI.
            </summary>
            <remarks>
              Use the "world" communicator to determine
              how many processes the user started for your MPI job, and as the 
              basis for creating the other communicators in your program. Many
              simple programs will use only the "world" communicator.
            </remarks>
        </member>
        <member name="F:MPI.Communicator.self">
            <summary>
              The "self" communicator is a simple communicator that contains only
              the currently-executing process. 
            </summary>
            <remarks>
              Each process will have a different "self" communicator, which only 
              permits communication with itself.
            </remarks>
        </member>
        <member name="M:MPI.Communicator.Compare(MPI.Communicator)">
            <summary>
              Compare two MPI communicators.
            </summary>
            <list>
              <listheader>
                <term>Value</term>
                <description>Description</description>
              </listheader>
            <item>
              <term><see cref="F:MPI.Comparison.Identical"/></term>
              <description>The two <c>Communicator</c> objects represent the same communicator.</description>
            </item>
            <item>
              <term><see cref="F:MPI.Comparison.Congruent"/></term>
              <description>
                The two <c>Communicator</c> objects contain the same processes with the same ranks,
                but represent different communicators.
              </description>
            </item>
            <item>
              <term><see cref="F:MPI.Comparison.Similar"/></term>
              <description>
                The two <c>Communicator</c> objects contain the same processes, but with different ranks.
              </description>
            </item>
            <item>
              <term><see cref="F:MPI.Comparison.Unequal"/></term>
              <descsription>The two <c>Communicator</c> objects are different.</descsription>
            </item>
            </list>
        </member>
        <member name="M:MPI.Communicator.Send``1(``0,System.Int32,System.Int32)">
            <summary>
              Send a message to a particular processor.
            </summary>
            <remarks>
              The basic <c>Send</c> operation will block until this message data has been transferred from <c>value</c>. 
              This might mean that the <c>Send</c> operation will return immediately, before the receiver has actually received
              the data. However, it is also possible that <c>Send</c> won't return until it matches a <see cref="M:MPI.Communicator.Receive``1(System.Int32,System.Int32)"/>
              operation. Thus, the <paramref name="dest"/> parameter should not be equal to <see cref="P:MPI.Communicator.Rank"/>, because a send-to-self
              operation might never complete. 
            </remarks>
            <typeparam name="T">The type of the value to be sent. This type must be serializable.</typeparam>
            <param name="value">The value that will be transmitted with the message.</param>
            <param name="dest">
              The rank of the process that will receive this message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), but it should not be equal to <see cref="P:MPI.Communicator.Rank"/>.
            </param>
            <param name="tag">A message "tag" that identifies this particular kind of message. The receive must use the same message tag to receive this message.</param>
        </member>
        <member name="M:MPI.Communicator.SendReceive``1(``0,System.Int32,System.Int32,``0@)">
            <summary>
              Simultaneously send and receive a value from another process.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValue">The value to be sent.</param>
            <param name="dest">The rank of the process the data will be sent to and received from.</param>
            <param name="tag">A message "tag" that identifies this particular kind of message.</param>
            <param name="outValue">The value to be received.</param>
        </member>
        <member name="M:MPI.Communicator.SendReceive``1(``0,System.Int32,System.Int32,System.Int32,System.Int32,``0@)">
            <summary>
              Simultaneously send and receive a value from another process.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValue">The value to be sent.</param>
            <param name="dest">The rank of the process the data will be sent to.</param>
            <param name="sendTag">A message "tag" that identifies the particular kind of message being sent.</param>
            <param name="source">The rank of the process the data will received from.</param>
            <param name="recvTag">A message "tag" that identifies the particular kind of message being received.</param>
            <param name="outValue">The value to be received.</param>
        </member>
        <member name="M:MPI.Communicator.SendReceive``1(``0,System.Int32,System.Int32,System.Int32,System.Int32,``0@,MPI.CompletedStatus@)">
            <summary>
              Simultaneously send and receive a value from another process (which need not be the same).
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValue">The value to be sent.</param>
            <param name="dest">The rank of the process the data will be sent to.</param>
            <param name="sendTag">A message "tag" that identifies the particular kind of message being sent.</param>
            <param name="source">The rank of the process the data will received from.</param>
            <param name="recvTag">A message "tag" that identifies the particular kind of message being received.</param>
            <param name="outValue">The value to be received.</param>
            <param name="status">Receives information about the completed receive operation.</param>
        </member>
        <member name="M:MPI.Communicator.SendReceive``1(``0[],System.Int32,System.Int32,``0[]@)">
            <summary>
              Simultaneously send and receive an array of values from another process.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The values to be sent.</param>
            <param name="dest">The rank of the process the data will be sent to and received from.</param>
            <param name="tag">A message "tag" that identifies the particular kind of message being sent and received.</param>
            <param name="outValues">An array in which to store the values to be received. The array must be large enough to contain the received data.</param>
        </member>
        <member name="M:MPI.Communicator.SendReceive``1(``0[],System.Int32,System.Int32,System.Int32,System.Int32,``0[]@)">
            <summary>
              Simultaneously send and receive an array of values from another process.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The values to be sent.</param>
            <param name="dest">The rank of the process the data will be sent to.</param>
            <param name="sendTag">A message "tag" that identifies the particular kind of message being sent.</param>
            <param name="source">The rank of the process the data will received from.</param>
            <param name="recvTag">A message "tag" that identifies the particular kind of message being received.</param>
            <param name="outValues">An array in which to store the values to be received. The array must be large enough to contain the received data.</param>
        </member>
        <member name="M:MPI.Communicator.SendReceive``1(``0[],System.Int32,System.Int32,System.Int32,System.Int32,``0[]@,MPI.CompletedStatus@)">
            <summary>
              Simultaneously send and receive an array of values from another process.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The values to be sent.</param>
            <param name="dest">The rank of the process the data will be sent to.</param>
            <param name="sendTag">A message "tag" that identifies the particular kind of message being sent.</param>
            <param name="source">The rank of the process the data will received from.</param>
            <param name="recvTag">A message "tag" that identifies the particular kind of message being received.</param>
            <param name="outValues">An array in which to store the values to be received. The array must be large enough to contain the received data.</param>
            <param name="status">Receives information about the completed receive operation.</param>
        </member>
        <member name="M:MPI.Communicator.Receive``1(System.Int32,System.Int32)">
            <summary>
              Receive a message from another process. The <c>Receive</c> operation is a blocking operation that will 
              not complete until it has completely received a message. 
            </summary>
            <typeparam name="T">The type of the value that will be received. This type must be serializable.</typeparam>
            <param name="source">
              The process that sent (or that will send) the message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), but it should not be equal to <see cref="P:MPI.Communicator.Rank"/>.
              Alternatively, this might have the special value <see cref="F:MPI.Communicator.anySource"/>, if the message can be
              received from any other process in this communicator.
            </param>
            <param name="tag">
              A message "tag" that identifies this particular kind of message. Only messages sent with this tag
              will be received by this call. The special value <see cref="F:MPI.Communicator.anyTag"/> permits messages sent with
              any tag value to be received.
            </param>
            <returns>
            The value received.
            </returns>
        </member>
        <member name="M:MPI.Communicator.Receive``1(System.Int32,System.Int32,``0@)">
            <summary>
              Receive a message from another process. The <c>Receive</c> operation is a blocking operation that will 
              not complete until it has completely received a message. 
            </summary>
            <typeparam name="T">The type of the value that will be received. This type must be serializable.</typeparam>
            <param name="source">
              The process that sent (or that will send) the message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), but it should not be equal to <see cref="P:MPI.Communicator.Rank"/>.
              Alternatively, this might have the special value <see cref="F:MPI.Communicator.anySource"/>, if the message can be
              received from any other process in this communicator.
            </param>
            <param name="tag">
              A message "tag" that identifies this particular kind of message. Only messages sent with this tag
              will be received by this call. The special value <see cref="F:MPI.Communicator.anyTag"/> permits messages sent with
              any tag value to be received.
            </param>
            <param name="value">
              When <c>Receive</c> completes, this parameter will contain the value
              transmitted as part of the message.
            </param>
        </member>
        <member name="M:MPI.Communicator.Receive``1(System.Int32,System.Int32,``0@,MPI.CompletedStatus@)">
            <summary>
              Receive a message from another process. The <c>Receive</c> operation is a blocking operation that will 
              not complete until it has completely received a message. 
            </summary>
            <typeparam name="T">The type of the value that will be received. This type must be serializable.</typeparam>
            <param name="source">
              The process that sent (or that will send) the message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), but it should not be equal to <see cref="P:MPI.Communicator.Rank"/>.
              Alternatively, this might have the special value <see cref="F:MPI.Communicator.anySource"/>, if the message can be
              received from any other process in this communicator.
            </param>
            <param name="tag">
              A message "tag" that identifies this particular kind of message. Only messages sent with this tag
              will be received by this call. The special value <see cref="F:MPI.Communicator.anyTag"/> permits messages sent with
              any tag value to be received.
            </param>
            <param name="value">
              When <c>Receive</c> completes, this parameter will contain the value
              transmitted as part of the message.
            </param>
            <param name="status">
            Receives information about the completed receive operation.
            </param>
        </member>
        <member name="M:MPI.Communicator.Send``1(``0[],System.Int32,System.Int32)">
            <summary>
              Send a message to a particular processor.
            </summary>
            <remarks>
              The basic <c>Send</c> operation will block until this message data has been transferred from <c>values</c>. 
              This might mean that the <c>Send</c> operation will return immediately, before the receiver has actually received
              the data. However, it is also possible that <c>Send</c> won't return until it matches a <see cref="M:MPI.Communicator.Receive``1(System.Int32,System.Int32,``0[]@)"/>
              operation. Thus, the <paramref name="dest"/> parameter should not be equal to <see cref="P:MPI.Communicator.Rank"/>, because a send-to-self
              operation might never complete. 
            </remarks>
            <typeparam name="T">The type of the value to be sent. This type must be serializable.</typeparam>
            <param name="values">The values that will be transmitted with the message.</param>
            <param name="dest">
              The rank of the process that will receive this message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), but it should not be equal to <see cref="P:MPI.Communicator.Rank"/>.
            </param>
            <param name="tag">A message "tag" that identifies this particular kind of message. The receive must use the same message tag to receive this message.</param>
        </member>
        <member name="M:MPI.Communicator.Receive``1(System.Int32,System.Int32,``0[]@)">
            <summary>
              Receive a message containing an array of values from another process.
              
              <para><b>Important</b>: if the type <c>T</c> is a value type (e.g., a primitive type or a struct of primitive types), then 
              the <paramref name="values"/> array must be pre-allocated with enough storage to store all of the
              values that will be received. If the message contains more data than can be received into the
              array, this method will throw a <see cref="T:MPI.MessageTruncatedException"/> or, in some cases, crash.
              Providing an array that is longer than the received message is allowable; to determine the actual
              amount of data received, use <see cref="M:MPI.Communicator.Receive``1(System.Int32,System.Int32,``0[]@,MPI.CompletedStatus@)"/>.
              Note that this restriction does not apply when <c>T</c> is a reference type that is serialized.</para>
              
              <para>The <c>Receive</c> operation is a blocking operation, that will not complete until it has
              completely received a message. </para>
            </summary>
            <typeparam name="T">The type of the value that will be received. This type must be serializable.</typeparam>
            <param name="source">
              The process that sent (or that will send) the message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), but it should not be equal to <see cref="P:MPI.Communicator.Rank"/>.
              Alternatively, this might have the special value <see cref="F:MPI.Communicator.anySource"/>, if the message can be
              received from any other process in this communicator.
            </param>
            <param name="tag">
              A message "tag" that identifies this particular kind of message. Only messages sent with this tag
              will be received by this call. The special value <see cref="F:MPI.Communicator.anyTag"/> permits messages sent with
              any tag value to be received.
            </param>
            <param name="values">
              When <c>Receive</c> completes, this parameter will contain the values
              transmitted as part of the message. The <paramref name="values"/> array
              must be large enough to contain all of the values that will be received.
            </param>
        </member>
        <member name="M:MPI.Communicator.Receive``1(System.Int32,System.Int32,``0[]@,MPI.CompletedStatus@)">
            <summary>
              Receive an array from another process.
              
              <para><b>Important</b>: if the type <c>T</c> is a value type (e.g., a primitive type or a struct of primitive types), then 
              the <paramref name="values"/> array must be pre-allocated with enough storage to store all of the
              values that will be received. If the message contains more data than can be received into the
              array, this method will throw a <see cref="T:MPI.MessageTruncatedException"/> or, in some cases, crash.
              Providing an array that is longer than the received message is allowable; to determine the actual
              amount of data received, retrieve the message length from the <paramref name="status"/> result.
              Note that this restriction does not apply when <c>T</c> is a reference type that is serialized.</para>
              
              <para>The <c>Receive</c> operation is a blocking operation, that will not complete until it has
              completely received a message.</para>
            </summary>
            <typeparam name="T">The type of the value that will be received. This type must be serializable.</typeparam>
            <param name="source">
              The process that sent (or that will send) the message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), but it should not be equal to <see cref="P:MPI.Communicator.Rank"/>.
              Alternatively, this might have the special value <see cref="F:MPI.Communicator.anySource"/>, if the message can be
              received from any other process in this communicator.
            </param>
            <param name="tag">
              A message "tag" that identifies this particular kind of message. Only messages sent with this tag
              will be received by this call. The special value <see cref="F:MPI.Communicator.anyTag"/> permits messages sent with
              any tag value to be received.
            </param>
            <param name="values">
              When <c>Receive</c> completes, this parameter will contain the values
              transmitted as part of the message. The <paramref name="values"/> array
              must be large enough to contain all of the values that will be received.
            </param>
            <param name="status">Contains information about the received message.</param>
        </member>
        <member name="M:MPI.Communicator.ImmediateSend``1(``0,System.Int32,System.Int32)">
            <summary>
            Non-blocking send of a single value. This routine will initiate communication and
            then return immediately with a <see cref="T:MPI.Request"/> object that can be used to
            query the status of the communication. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">The value that will be transmitted.</param>
            <param name="dest">The rank of the destination process.</param>
            <param name="tag">The tag used to identify this message.</param>
            <returns>
              A new request object that can be used to manipulate this non-blocking
              communication.
            </returns>
        </member>
        <member name="M:MPI.Communicator.ImmediateSend``1(``0[],System.Int32,System.Int32)">
            <summary>
            Non-blocking send of an array of data. This routine will initiate communication and
            then return immediately with a <see cref="T:MPI.Request"/> object that can be used to
            query the status of the communication. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              The array of values that will be transmitted. If <c>T</c> is a value type, 
              do not change the values in this array before the resulting request has been 
              completed, because the implementation may send these values at any time.
            </param>
            <param name="dest">The rank of the destination process.</param>
            <param name="tag">The tag used to identify this message.</param>
            <returns>
              A new request object that can be used to manipulate this non-blocking
              communication.
            </returns>
        </member>
        <member name="M:MPI.Communicator.ImmediateReceive``1(System.Int32,System.Int32)">
            <summary>
            Non-blocking receive of a single value. This routine will initiate a request to receive
            data and then return immediately. The data may be received in the background. To test for
            or force the completion of the communication, then access the received data, use the 
            returned <see cref="T:MPI.ReceiveRequest"/> object.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="source">
              Rank of the source process to receive data from. Alternatively, use <see cref="F:MPI.Communicator.anySource"/> to
              receive a message from any other process.
            </param>
            <param name="tag">
              The tag that identifies the message to be received. Alternatively, use <see cref="F:MPI.Communicator.anyTag"/>
              to receive a message with any tag.
            </param>
            <returns>
              A request object that allows one to test or force the completion of this receive request,
              and retrieve the resulting value.
            </returns>
        </member>
        <member name="M:MPI.Communicator.ImmediateReceive``1(System.Int32,System.Int32,``0[])">
            <summary>
            Non-blocking receive of an array of values. This routine will initiate a request to receive
            data and then return immediately. The data may be received in the background. To test for
            or force the completion of the communication, then access the received data, use the 
            returned <see cref="T:MPI.ReceiveRequest"/> object.
            
              <para><b>Important</b>: if the type <c>T</c> is a value type (e.g., a primitive type or a struct of primitive types), then 
              the <paramref name="values"/> array must be pre-allocated with enough storage to store all of the
              values that will be received. If the message contains more data than can be received into the
              array, this method will throw a <see cref="T:MPI.MessageTruncatedException"/> or, in some cases, crash.
              Providing an array that is longer than the received message is allowable; to determine the actual
              amount of data received, retrieve the message length from the <see cref="T:MPI.CompletedStatus"/>
              structure provided when the request is completed.</para>
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="source">
              Rank of the source process to receive data from. Alternatively, use <see cref="F:MPI.Communicator.anySource"/> to
              receive a message from any other process.
            </param>
            <param name="tag">
              The tag that identifies the message to be received. Alternatively, use <see cref="F:MPI.Communicator.anyTag"/>
              to receive a message with any tag.
            </param>
            <param name="values">
              An array into which the values will be received. This array must be large enough to
              accommodate all of the data sent by the source process.
            </param>
            <returns>
              A request object that allows one to test or force the completion of this receive request,
              and retrieve the resulting value. The object retrieved from the request object will be
              <paramref name="values"/>.
            </returns>
        </member>
        <member name="M:MPI.Communicator.Probe(System.Int32,System.Int32)">
            <summary>
            Wait for a message from the given source and with the specified tag to become
            available, but don't try to receive the message. This routine will wait indefinitely
            for a message meeting the given criteria to arrive, so it should only be invoked
            when you know a message is coming. If you just want to check whether a message is
            available use <see cref="M:MPI.Communicator.ImmediateProbe(System.Int32,System.Int32)"/>.
            </summary>
            <param name="source">
              The process that sent (or that will send) the message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), or the special value <see cref="F:MPI.Communicator.anySource"/>. If it is
              <see cref="F:MPI.Communicator.anySource"/>, then we can match a message sent by any other process in
              this communicator.
            </param>
            <param name="tag">
              A message "tag" that identifies this particular kind of message. Only messages sent with this tag
              will be matched by this call. The special value <see cref="F:MPI.Communicator.anyTag"/> permits messages sent with
              any tag value to be received.
            </param>
            <returns>A <see cref="T:MPI.Status"/> object containing information about the message.</returns>
        </member>
        <member name="M:MPI.Communicator.ImmediateProbe(System.Int32,System.Int32)">
            <summary>
            Determine whether a message from the given source and with the specified tag is
            available, but don't try to receive the message. This routine will return 
            immediately, regardless of whether a message is available, so it is useful for
            polling to determine whether any messages need to be handled at this time. If
            your program can't do any work until a message arrives (and the message is 
            guaranteed to arrive, eventually), use <see cref="M:MPI.Communicator.Probe(System.Int32,System.Int32)"/> instead.
            </summary>
            <param name="source">
              The process that sent (or that will send) the message. This must be a value in 
              [<c>0</c>, <c>Size-1</c>), or the special value <see cref="F:MPI.Communicator.anySource"/>. If it is
              <see cref="F:MPI.Communicator.anySource"/>, then we can match a message sent by any other process in
              this communicator.
            </param>
            <param name="tag">
              A message "tag" that identifies this particular kind of message. Only messages sent with this tag
              will be matched by this call. The special value <see cref="F:MPI.Communicator.anyTag"/> permits messages sent with
              any tag value to be received.
            </param>
            <returns>
              If a message is available, a <see cref="T:MPI.Status"/> object containing information about that message.
              Otherwise, returns <c>null</c>.
            </returns>
        </member>
        <member name="M:MPI.Communicator.Barrier">
            <summary>
              Wait until all processes in the communicator have reached the same barrier.
            </summary>
            <remarks>
              This collective operation should be called by all processes within the communicator.
              Each process will wait within the barrier until all processes have entered the barrier,
              at which point all of the processes will be released. Use barriers when all of the 
              processes need to synchronize at some particular point in your program flow.
            </remarks>
            <example>
              Barriers are often used after each process has completed some large, independent
              chunk of work:
            <code>
              public void Superstep(Communicator comm) 
              {
                // Perform a large chunk of work  locally
                DoLocalWork();
            
                // Synchronize with everyone else
                comm.Barrier();
            
                // Okay, move on to the next piece of work.
              }
            </code>
            </example>
        </member>
        <member name="M:MPI.Communicator.Broadcast_impl``1(System.Boolean,``0@,System.Int32)">
            <summary>
            Broadcast a value from the <paramref name="root"/> process to all other processes.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="isRoot">
              Whether this is the root process or not.
            </param>
            <param name="value">
              The value to be broadcast. At the <paramref name="root"/> process, this value is
              read (but not written); at all other processes, this value will be replaced with
              the value at the root.
            </param>
            <param name="root">
              The rank of the process that is broadcasting the value out to
              all of the non-<paramref name="root"/> processes.
            </param>
        </member>
        <member name="M:MPI.Communicator.Broadcast_impl``1(System.Boolean,``0[]@,System.Int32)">
            <summary>
            Broadcast an array from the <paramref name="root"/> process to all other processes.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="isRoot">
              Whether this is the root process.
            </param>
            <param name="values">
              The array of values to be broadcast. At the <paramref name="root"/> process, this value is
              read (but not written); at all other processes, this value will be replaced with
              the value at the root. Note that the receiving processes must already have allocated
              enough space in the array to receive data from the root, prior to calling <c>Broadcast</c>.
            </param>
            <param name="root">
              The rank of the process that is broadcasting the values out to
              all of the non-<paramref name="root"/> processes.
            </param>
        </member>
        <member name="M:MPI.Communicator.Gather_impl``1(System.Boolean,System.Int32,``0,System.Int32,``0[]@)">
            <summary>
            Gather the values from each process into an array of values at the 
            <paramref name="root"/> process. This version of Gather does not distinguish
            between Intra- and Intercommunicators, and should be used as a base for Gather
            in derived communicators.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="isRoot">Whether this process is root (== root for Intracommunicators, or 
              ==Intercommunicator.Root for Intercommunicators).</param>
            <param name="size">
              The number of processes from which the root will collect information.
            </param>
            <param name="inValue">The value contributed by this process.</param>
            <param name="root">
              The rank of the process that will receive values for all of the
              processes in the communicator.
            </param>
            <param name="outValues">
              An array that will store the values contributed by each process.
              This value is only significant at the <paramref name="root"/>, and
              can be omitted by non-root processes.
              Supply this argument when you have pre-allocated space for the resulting array.
            </param> 
        </member>
        <member name="M:MPI.Communicator.GatherFlattened_impl``1(System.Boolean,System.Int32,``0[],System.Int32[],System.Int32,``0[]@)">
            <summary>
              This is an internal implementation of the GatherFlattened methods, designed to be called by derived classes.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="isRoot">Whether this process is the root process.</param>
            <param name="size">The number of processes to gather from.</param>
            <param name="inValues">The values contributed by this process, if any.</param>
            <param name="counts">The number of elements to be gathered from each process. This parameter is only significant at the root process.</param>
            <param name="root">The rank of the root process.</param>
            <param name="outValues">An array to store the gathered values in. Can be, but does not have to be, preallocated.</param>
        </member>
        <member name="M:MPI.Communicator.Reduce_impl``1(System.Boolean,System.Int32,``0,MPI.ReductionOperation{``0},System.Int32)">
             <summary>
               <c>Reduce</c> is a collective algorithm that combines the values stored by each process into a 
               single value available at the designated <paramref name="root"/> process. The values are combined 
               in a user-defined way, specified via a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> 
               are the values provided by the N processes in the communicator, the result will be the value 
               <c>value1 op value2 op ... op valueN</c>. This result is only
               available to the <paramref name="root"/> process. If all processes need the result of the reduction,
               use <see cref="M:MPI.Intracommunicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/>.
             </summary>
             <example>
               This example computes the sum of the ranks of all of the processes using 
               <see cref="M:MPI.Intracommunicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)"/>.
               <code>
             using System;
             using MPI;
             
             class Reduce
             {
               static void Main(string[] args)
               {
                 using (MPI.Environment env = new MPI.Environment(ref args))
                 {
                   Communicator world = Communicator.world;
            
                   int root = 0;
                   if (world.Rank == root)
                   {
                     int sum;
                     world.Reduce(world.Rank, out sum, Operation&lt;int&gt;.Add, root);
                     System.Console.WriteLine("Sum of ranks = " + sum);
                   }
                   else 
                   {
                     world.Reduce(world.Rank, Operation&lt;int&gt;.Add, root);
                   }
                 }
               }
             };
               </code>
             </example>
             <typeparam name="T">Any serializable type.</typeparam>
             <param name="isRoot">
               Whether this process is the root process.
             </param>
             <param name="size">
               The number of items that will be collected.
             </param>
             <param name="value">The local value that will be combined with the values provided by other processes.</param>
             <param name="op">
               The operation used to combine two values. This operation must be associative.
             </param>
             <param name="root">
               The rank of the process that is the root of the reduction operation, which will receive the result
               of the reduction operation in its <paramref name="outValue"/> argument.
             </param>
             <returns>
               On the root, returns the result of the reduction operation. The other processes receive a default value.
             </returns>
        </member>
        <member name="F:MPI.Communicator.Attributes">
            <summary>
            The set of attributes attached to this communicator.
            </summary>
        </member>
        <member name="F:MPI.Communicator.comm">
            <summary>
            The low-level MPI communicator handle.
            </summary>
        </member>
        <member name="F:MPI.Communicator.shadowComm">
            <summary>
            Secondary low-level MPI communicator that will be used for communication of
            data within MPI.NET, such as collectives implemented over point-to-point or
            the data message associated with a Send of serialized data.
            </summary>
        </member>
        <member name="F:MPI.Communicator.tagAllocator">
            <summary>
            Tag allocator used to allocate tags for collective operations and serialized
            send operations.
            </summary>
        </member>
        <member name="F:MPI.Communicator.anySource">
            <summary>
              Special value for the <c>source</c> argument to <c>Receive</c> that
              indicates that the message can be received from any process in the communicator.
            </summary>
        </member>
        <member name="F:MPI.Communicator.anyTag">
            <summary>
              Special value for the <c>tag</c> argument to <c>Receive</c> that
              indices that the message with any tag can be received by the receive operation.
            </summary>
        </member>
        <member name="M:MPI.Communicator.IntToBool(System.Int32[])">
            <summary>
              Convert an array of integers to an array of booleans. Used for some low-level calls 
              that return an array of logical ints to express boolean values.
            </summary>
            <param name="arr">
              An array to convert.
            </param>
            <returns>
              An array of bools corresponding to the logical values in the input array.
            </returns>
        </member>
        <member name="M:MPI.Communicator.BoolToInt(System.Boolean[])">
            <summary>
              Convert an array of booleans to an array of integers. Used for some low-level calls 
              that require an array of logical ints to express boolean values.
            </summary>
            <param name="arr">
              An array to convert.
            </param>
            <returns>
              An array of ints with values of 0 and/or 1 corresponding to the values in the input array.
            </returns>
        </member>
        <member name="P:MPI.Communicator.Size">
            <summary>
              Returns the number of processes within this communicator. 
            </summary>
            <remarks>
              All of the processes within this communicator will have the same 
              value for Size.
            </remarks>
        </member>
        <member name="P:MPI.Communicator.Rank">
            <summary>
              Returns the rank of the currently executing process within this
              communicator. 
            </summary>
            <remarks>
              The rank is a number between 0 and Size-1 that is used 
              to identify this process, e.g., to send messages to it. The value of
              Rank will differ from one process to the next.
            </remarks>
        </member>
        <member name="P:MPI.Communicator.Group">
            <summary>
            Retrieve the group containing all of the processes in this communicator.
            </summary>
        </member>
        <member name="P:MPI.Communicator.Parent">
            <summary>
              Provides the parent communicator if this Intercommunicator was created by <see cref="!:Intercommunicator.Spawn()"/> or <see cref="!:Intercommunicator.SpawnMultiple"/>. Otherwise, returns null.
            </summary>
        </member>
        <member name="M:MPI.Intracommunicator.#ctor">
            <summary>
            Intracommunicators can only be constructed from other communicators or adopted
            from low-level intracommunicators.
            </summary>
        </member>
        <member name="M:MPI.Intracommunicator.Adopt(System.Int32)">
            <summary>
              Adopts a low-level MPI intercommunicator that was created with any of the low-level MPI facilities.
              The resulting <c>Intercommunicator</c> object will manage the lifetime of the low-level MPI intercommunicator,
              and will call <see cref="M:MPI.Unsafe.MPI_Comm_free(System.Int32@)"/> when it is disposed or finalized.
            </summary>
            <remarks>
              This constructor should only be used in rare cases where the program 
              is manipulating MPI intercommunicators through the low-level MPI interface.
            </remarks>
        </member>
        <member name="M:MPI.Intracommunicator.Allgather``1(``0)">
            <summary>
            Gathers invididual values provided by each processor into an array of values in which the
            ith element of the array corresponds to the value provided by the processor with rank i.
            The resulting array of values is available to all of the processors. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">
              The value supplied by this processor, which will be placed into the element with index <see cref="P:MPI.Communicator.Rank"/>
              in the returned array.
            </param>
            <returns> 
              An array of values, where the ith value in the array is a copy of the <paramref name="value"/>
              argument provided by the processor with rank i.
            </returns>
        </member>
        <member name="M:MPI.Intracommunicator.Allgather``1(``0,``0[]@)">
            <summary>
            Gathers invididual values provided by each processor into an array of values in which the
            ith element of the array corresponds to the value provided by the processor with rank i.
            The resulting array of values is available 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValue">
              The value supplied by this processor, which will be placed into the element with index <see cref="P:MPI.Communicator.Rank"/>
              in the returned array.
            </param>
            <param name="outValues">
              An array of values, where the ith value in the array is a copy of the <paramref name="inValue"/>
              argument provided by the processor with rank i.
              Supply this argument when you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.AllgatherFlattened``1(``0[],System.Int32)">
            <summary>
            Gathers invididual values provided by each processor into an array of values.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The values supplied by this processor.
            </param>
            <param name="count">
              The number of items to be received by each process.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.AllgatherFlattened``1(``0[],System.Int32[])">
            <summary>
            Gathers invididual values provided by each processor into an array of values.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The values supplied by this processor.
            </param>
            <param name="counts">
              The numbers of items to be received by each process.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.AllgatherFlattened``1(``0[],System.Int32,``0[]@)">
            <summary>
            Gathers invididual values provided by each processor into an array of values.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The values supplied by this processor.
            </param>
            <param name="count">
              The number of items to be received by each process.
            </param>
            <param name="outValues">
              An array of values supplied by all processes. Supply this argument when 
              you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.AllgatherFlattened``1(``0[],System.Int32[],``0[]@)">
            <summary>
            Gathers invididual values provided by each processor into an array of values.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The values supplied by this processor.
            </param>
            <param name="counts">
              The numbers of items to be received by each process.
            </param>
            <param name="outValues">
              An array of values supplied by all processes. Supply this argument when 
              you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.Allreduce``1(``0,MPI.ReductionOperation{``0})">
             <summary>
               <c>Allreduce</c> is a collective algorithm that combines the values stored by each process into a 
               single value available to all processes. The values are combined in a user-defined way, specified via 
               a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> are the values provided by the 
               N processes in the communicator, the result will be the value <c>value1 op value2 op ... op valueN</c>.
             
               An <c>Allreduce</c> is equivalent to a <see cref="M:MPI.Intracommunicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)"/> 
               followed by a <see cref="M:MPI.Intracommunicator.Broadcast``1(``0@,System.Int32)"/>.
             </summary>
             <example>
               This example computes the sum of the ranks of all of the processes using 
               <see cref="M:MPI.Intracommunicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/>.
               <code>
             using System;
             using MPI;
             
             class Allreduce
             {
               static void Main(string[] args)
               {
                 using (MPI.Environment env = new MPI.Environment(ref args))
                 {
                   Communicator world = Communicator.world;
            
                   int sum = world.Allreduce(world.Rank, Operation&lt;int&gt;.Add);
                   System.Console.WriteLine("Sum of ranks = " + sum);
                 }
               }
             };
               </code>
             </example>
             <typeparam name="T">Any serializable type.</typeparam>
             <param name="value">The local value that will be combined with the values provided by other processes.</param>
             <param name="op">
               The operation used to combine two values. This operation must be associative.
             </param>
             <returns>The result of the reduction. The same value will be returned to all processes.</returns>
        </member>
        <member name="M:MPI.Intracommunicator.Allreduce``1(``0[],MPI.ReductionOperation{``0})">
            <summary>
              <c>Allreduce</c> is a collective algorithm that combines the values stored by each process into a 
              single value available to all processes. The values are combined in a user-defined way, specified via 
              a delegate. When provided with arrays, <c>Allreduce</c> combines the ith value of each of the arrays 
              passed to each process. Thus, <c>Allreduce</c> on arrays is the same as calling 
              <see cref="M:MPI.Intracommunicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/> for each <c>inValues[i]</c>.
            
              An <c>Allreduce</c> is equivalent to a <see cref="M:MPI.Intracommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32,``0[]@)"/> 
              followed by a <see cref="M:MPI.Intracommunicator.Broadcast``1(``0[]@,System.Int32)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">The local values that will be combined with the values provided by other processes.</param>
            <param name="op">
              The operation used to combine two values. This operation must be associative.
            </param>
            <returns>
              The values that result from combining all of the values in <paramref name="values"/>
              element-by-element. If needed, this array will be resized to the same size as <paramref name="inValues"/>.
            </returns>
        </member>
        <member name="M:MPI.Intracommunicator.Allreduce``1(``0[],MPI.ReductionOperation{``0},``0[]@)">
            <summary>
              <c>Allreduce</c> is a collective algorithm that combines the values stored by each process into a 
              single value available to all processes. The values are combined in a user-defined way, specified via 
              a delegate. When provided with arrays, <c>Allreduce</c> combines the ith value of each of the arrays 
              passed to each process. Thus, <c>Allreduce</c> on arrays is the same as calling 
              <see cref="M:MPI.Intracommunicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/> for each <c>inValues[i]</c>.
            
              An <c>Allreduce</c> is equivalent to a <see cref="M:MPI.Intracommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32,``0[]@)"/> 
              followed by a <see cref="M:MPI.Intracommunicator.Broadcast``1(``0[]@,System.Int32)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The local values that will be combined with the values provided by other processes.</param>
            <param name="op">
              The operation used to combine two values. This operation must be associative.
            </param>
            <param name="outValues">
              The values that result from combining all of the values in <paramref name="inValues"/>
              element-by-element. If needed, this array will be resized to the same size as <paramref name="inValues"/>.
              Supply this argument when you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.Alltoall``1(``0[])">
            <summary>
            Collective operation in which every process sends data to every other process. <c>Alltoall</c>
            differs from <see cref="M:MPI.Intracommunicator.Allgather``1(``0)"/> in that a given process can send different
            data to all of the other processes, rather than contributing the same piece of data to all
            processes. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              The array of values that will be sent to each process. The ith value in this array
              will be sent to the process with rank i.
            </param>
            <returns>
              An array of values received from all of the other processes. The jth value in this
              array will be the value sent to the calling process from the process with rank j.
            </returns>
        </member>
        <member name="M:MPI.Intracommunicator.Alltoall``1(``0[],``0[]@)">
            <summary>
            Collective operation in which every process sends data to every other process. <c>Alltoall</c>
            differs from <see cref="M:MPI.Intracommunicator.Allgather``1(``0)"/> in that a given process can send different
            data to all of the other processes, rather than contributing the same piece of data to all
            processes. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The array of values that will be sent to each process. The ith value in this array
              will be sent to the process with rank i.
            </param>
            <param name="outValues">
              The array of values received from all of the other processes. The jth value in this
              array will be the value sent to the calling process from the process with rank j.
              Supply this argument when you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.AlltoallFlattened``1(``0[],System.Int32)">
            <summary>
            Collective operation in which every process sends data to every other process. <c>Alltoall</c>
            differs from <see cref="M:MPI.Intracommunicator.Allgather``1(``0)"/> in that a given process can send different
            data to all of the other processes, rather than contributing the same piece of data to all
            processes. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The array of values that will be sent to each process. sendCounts[i] worth of data will
              be sent to process i.
            </param>
            <param name="count">
              The number of items to be sent and received by each process.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.AlltoallFlattened``1(``0[],System.Int32,``0[]@)">
            <summary>
            Collective operation in which every process sends data to every other process. <c>Alltoall</c>
            differs from <see cref="M:MPI.Intracommunicator.Allgather``1(``0)"/> in that a given process can send different
            data to all of the other processes, rather than contributing the same piece of data to all
            processes. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The array of values that will be sent to each process. sendCounts[i] worth of data will
              be sent to process i.
            </param>
            <param name="count">
              The number of items to be sent and received by each process.
            </param>
            <param name="outValues">
              The array of values received from all of the other processes.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.AlltoallFlattened``1(``0[],System.Int32[],System.Int32[])">
            <summary>
            Collective operation in which every process sends data to every other process. <c>Alltoall</c>
            differs from <see cref="M:MPI.Intracommunicator.Allgather``1(``0)"/> in that a given process can send different
            data to all of the other processes, rather than contributing the same piece of data to all
            processes. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The array of values that will be sent to each process. sendCounts[i] worth of data will
              be sent to process i.
            </param>
            <param name="sendCounts">
              The numbers of items to be sent to each process.
            </param>
              The numbers of items to be received by each process.
            <param name="recvCounts">
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.AlltoallFlattened``1(``0[],System.Int32[],System.Int32[],``0[]@)">
            <summary>
            Collective operation in which every process sends data to every other process. <c>Alltoall</c>
            differs from <see cref="M:MPI.Intracommunicator.Allgather``1(``0)"/> in that a given process can send different
            data to all of the other processes, rather than contributing the same piece of data to all
            processes. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The array of values that will be sent to each process. sendCounts[i] worth of data will
              be sent to process i.
            </param>
            <param name="sendCounts">
              The numbers of items to be sent to each process.
            </param>
              The numbers of items to be received by each process.
            <param name="recvCounts">
            </param>
            <param name="outValues">
              The array of values received from all of the other processes.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.Broadcast``1(``0@,System.Int32)">
            <summary>
            Broadcast a value from the <paramref name="root"/> process to all other processes.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">
              The value to be broadcast. At the <paramref name="root"/> process, this value is
              read (but not written); at all other processes, this value will be replaced with
              the value at the root.
            </param>
            <param name="root">
              The rank of the process that is broadcasting the value out to
              all of the non-<paramref name="root"/> processes.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.Broadcast``1(``0[]@,System.Int32)">
            <summary>
            Broadcast an array from the <paramref name="root"/> process to all other processes.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              The array of values to be broadcast. At the <paramref name="root"/> process, this value is
              read (but not written); at all other processes, this value will be replaced with
              the value at the root. Note that the receiving processes must already have allocated
              enough space in the array to receive data from the root, prior to calling <c>Broadcast</c>.
            </param>
            <param name="root">
              The rank of the process that is broadcasting the values out to
              all of the non-<paramref name="root"/> processes.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.ExclusiveScan``1(``0,MPI.ReductionOperation{``0})">
            <summary>
              <c>ExclusiveScan</c> is a collective algorithm that performs partial reductions on data provided by
              each process in the communicator. <c>ExclusiveScan</c> combines the values stored by each process into 
              partial results delivered to each process. The values are combined 
              in a user-defined way, specified via a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> 
              are the values provided by the N processes in the communicator, the resulting value for the process with 
              rank P &gt; 0 will be <c>value1 op value2 op ... op valueP</c>. The rank 0 process receives a default value.
            
              <c>ExclusiveScan</c> is sometimes called an "exclusive" scan, because the result returned to each process
              does not include the contribution of that process. For an "inclusive" scan (that does include the contribution
              of the calling process in its result), use <see cref="M:MPI.Intracommunicator.Scan``1(``0,MPI.ReductionOperation{``0})"/>.
              Note that the results provided by <c>ExclusiveScan</c> are the same as those provided by <c>Scan</c>, except that
              with <c>ExclusiveScan</c>, the process with rank i will receive the result that rank i-1 would have received from
              <c>Scan</c>, i.e., <c>ExclusiveScan</c> gives the same results but shifted to the right.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">The value contributed by the calling process.</param>
            <param name="op">
              Operation used to combine two values from different processes.
              This operation must be associative.
            </param>
            <returns>
              If the rank of the calling process is not zero, the value <c>value1 op value2 op ... op valueP</c>, where P 
              is the rank of the calling process. Otherwise, returns a default value.
            </returns>
        </member>
        <member name="M:MPI.Intracommunicator.ExclusiveScan``1(``0[],MPI.ReductionOperation{``0})">
            <summary>
              <c>ExclusiveScan</c> is a collective algorithm that performs partial reductions on data provided by
              each process in the communicator. <c>ExclusiveScan</c> combines the arrays stored by each process into 
              partial results delivered to each process. The values are combined in a user-defined way, specified via 
              a delegate that will be applied elementwise to the values in the arrays. If <c>array1</c>, <c>array2</c>, ..., <c>arrayN</c> 
              are the values provided by the N processes in the communicator, the resulting value for the process with 
              rank P &gt; 0 will be <c>array1 op array2 op ... op arrayP</c>. The rank 0 process receives a <c>null</c> array.
            
              <c>ExclusiveScan</c> is sometimes called an "exclusive" scan, because the result returned to each process
              does not include the contribution of that process. For an "inclusive" scan (that does include the contribution
              of the calling process in its result), use <see cref="M:MPI.Intracommunicator.Scan``1(``0[],MPI.ReductionOperation{``0})"/>.
              Note that the results provided by <c>ExclusiveScan</c> are the same as those provided by <c>Scan</c>, except that
              with <c>ExclusiveScan</c>, the process with rank i will receive the result that rank i-1 would have received from
              <c>Scan</c>, i.e., <c>ExclusiveScan</c> gives the same results but shifted to the right.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              The values contributed by the calling process. The lengths of the arrays provided by all of the
              processes must be the same.
            </param>
            <param name="op">
              Operation used to combine two values from different processes.
              This operation must be associative.
            </param>
            <returns>
              If the rank of the calling process is not zero, the value <c>array1 op array2 op ... op arrayP</c>, where P 
              is the rank of the calling process. Otherwise, returns a <c>null</c> array.
            </returns>
        </member>
        <member name="M:MPI.Intracommunicator.ExclusiveScan``1(``0[],MPI.ReductionOperation{``0},``0[]@)">
            <summary>
              <c>ExclusiveScan</c> is a collective algorithm that performs partial reductions on data provided by
              each process in the communicator. <c>ExclusiveScan</c> combines the arrays stored by each process into 
              partial results delivered to each process. The values are combined in a user-defined way, specified via 
              a delegate that will be applied elementwise to the values in the arrays. If <c>array1</c>, <c>array2</c>, ..., <c>arrayN</c> 
              are the values provided by the N processes in the communicator, the resulting value for the process with 
              rank P &gt; 0 will be <c>array1 op array2 op ... op arrayP</c>. The rank 0 process receives a <c>null</c> array.
            
              <c>ExclusiveScan</c> is sometimes called an "exclusive" scan, because the result returned to each process
              does not include the contribution of that process. For an "inclusive" scan (that does include the contribution
              of the calling process in its result), use <see cref="M:MPI.Intracommunicator.Scan``1(``0[],MPI.ReductionOperation{``0})"/>.
              Note that the results provided by <c>ExclusiveScan</c> are the same as those provided by <c>Scan</c>, except that
              with <c>ExclusiveScan</c>, the process with rank i will receive the result that rank i-1 would have received from
              <c>Scan</c>, i.e., <c>ExclusiveScan</c> gives the same results but shifted to the right.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The values contributed by the calling process. The lengths of the arrays provided by all of the
              processes must be the same.
            </param>
            <param name="op">
              Operation used to combine two values from different processes.
              This operation must be associative.
            </param>
            <param name="outValues">
              If the rank of the calling process is not zero, this will receive the value <c>array1 op array2 op ... op arrayP</c>, where P 
              is the rank of the calling process. Otherwise, this value will be replaced with <c>null</c>.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.Gather``1(``0,System.Int32)">
            <summary>
              Gather the values from each process into an array of values at the 
              <paramref name="root"/> process. On the root process, the pth element of the result
              will be equal to the <paramref name="value"/> parameter of the process
              with rank <c>p</c> when this routine returns.
            </summary>
            <example>
            This example demonstrates the use of <c>Gather</c> to gather the ranks of 
            all processes together at root 0.
            <code>
            using System;
            using MPI;
            
            class Gather
            {
                static void Main(string[] args)
                {
                    using (MPI.Environment env = new MPI.Environment(ref args))
                    {
                        Intracommunicator world = Communicator.world;
                        if (world.Rank == 0)
                        {
                            int[] ranks = world.Gather(world.Rank, 0);
                            System.Console.Write("Ranks of all of the processes:");
                            for (int i = 0; i &lt; ranks.Length; ++i)
                                System.Console.Write(" " + i);
                            System.Console.WriteLine();
                        }
                        else
                        {
                            world.Gather(world.Rank, 0);
                        }
                    }
                }
            }
            </code>
            </example>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">The value contributed by this process.</param>
            <param name="root">
              The rank of the process that will receive values for all of the
              processes in the communicator.
            </param>
            <returns>
              At the root, an array containing the <paramref name="value"/>s supplied by each of the processes
              in the communicator. All other processes receive <c>null</c>.
            </returns>
        </member>
        <member name="M:MPI.Intracommunicator.Gather``1(``0,System.Int32,``0[]@)">
            <summary>
              Gather the values from each process into an array of values at the 
              <paramref name="root"/> process. On the root process, <c>outValues[p]</c>
              will be equal to the <paramref name="value"/> parameter of the process
              with rank <c>p</c> when this routine returns. Use this variant of the routine 
              when you want to pre-allocate storage for the <paramref name="outValues"/> array.
            </summary>
            <example>
            This example demonstrates the use of <c>Gather</c> to gather the ranks of 
            all processes together at root 0.
            <code>
            using System;
            using MPI;
            
            class Gather
            {
                static void Main(string[] args)
                {
                    using (MPI.Environment env = new MPI.Environment(ref args))
                    {
                        Intracommunicator world = Communicator.world;
                        if (world.Rank == 0)
                        {
                            int[] ranks = new int[world.Size];
                            world.Gather(world.Rank, 0, ref ranks);
                            System.Console.Write("Ranks of all of the processes:");
                            for (int i = 0; i &lt; ranks.Length; ++i)
                                System.Console.Write(" " + i);
                            System.Console.WriteLine();
                        }
                        else
                        {
                            world.Gather(world.Rank, 0);
                        }
                    }
                }
            }
            </code>
            </example>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValue">The value contributed by this process.</param>
            <param name="root">
              The rank of the process that will receive values for all of the
              processes in the communicator.
            </param>
            <param name="outValues">
              An array that will store the values contributed by each process.
              This value is only significant at the <paramref name="root"/>, and
              can be omitted by non-root processes.
              Supply this argument when you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.GatherFlattened``1(``0[],System.Int32)">
            <summary>
              Similar to <see cref="M:MPI.Intracommunicator.Gather``1(``0,System.Int32)"/> but here all values are aggregated into one large array.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The values to be contributed by this process.</param>
            <param name="root">The rank of the root node.</param>
            <returns>The aggregated gathered values.</returns>
        </member>
        <member name="M:MPI.Intracommunicator.GatherFlattened``1(``0[],System.Int32[],System.Int32)">
            <summary>
              Similar to <see cref="M:MPI.Intracommunicator.Gather``1(``0,System.Int32)"/> but here all values are aggregated into one large array.
              In this variant, the number of elements contributed need not be identical for each process.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The values to be contributed by this process.</param>
            <param name="counts">
              The number of elements contributed by each process.
            </param>
            <param name="root">The rank of the root node.</param>
            <returns>The aggregated gathered values.</returns>
        </member>
        <member name="M:MPI.Intracommunicator.GatherFlattened``1(``0[],System.Int32,``0[]@)">
            <summary>
              Similar to <see cref="M:MPI.Intracommunicator.Gather``1(``0,System.Int32)"/> but here all values are aggregated into one large array.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The values to be contributed by this process.</param>
            <param name="root">The rank of the root node.</param>
            <param name="outValues">
              The array to write the gathered values to; use this parameter when you have preallocated
              space for the array.
            </param>
            <returns>The aggregated gathered values.</returns>
        </member>
        <member name="M:MPI.Intracommunicator.GatherFlattened``1(``0[],System.Int32[],System.Int32,``0[]@)">
            <summary>
              Similar to <see cref="M:MPI.Intracommunicator.Gather``1(``0,System.Int32)"/> but here all values are aggregated into one large array.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The values to be contributed by this process.</param>
            <param name="counts">
              The number of elements contributed by each process.
            </param>
            <param name="root">The rank of the root node.</param>
            <param name="outValues">
              The array to write the gathered values to; use this parameter when you have preallocated
              space for the array.
            </param>
            <returns>The aggregated gathered values.</returns>
        </member>
        <member name="M:MPI.Intracommunicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)">
             <summary>
               <c>Reduce</c> is a collective algorithm that combines the values stored by each process into a 
               single value available at the designated <paramref name="root"/> process. The values are combined 
               in a user-defined way, specified via a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> 
               are the values provided by the N processes in the communicator, the result will be the value 
               <c>value1 op value2 op ... op valueN</c>. This result is only
               available to the <paramref name="root"/> process. If all processes need the result of the reduction,
               use <see cref="M:MPI.Intracommunicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/>.
             </summary>
             <example>
               This example computes the sum of the ranks of all of the processes using 
               <see cref="M:MPI.Intracommunicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)"/>.
               <code>
             using System;
             using MPI;
             
             class Reduce
             {
               static void Main(string[] args)
               {
                 using (MPI.Environment env = new MPI.Environment(ref args))
                 {
                   Intracommunicator world = Communicator.world;
            
                   int root = 0;
                   if (world.Rank == root)
                   {
                     int sum;
                     world.Reduce(world.Rank, out sum, Operation&lt;int&gt;.Add, root);
                     System.Console.WriteLine("Sum of ranks = " + sum);
                   }
                   else 
                   {
                     world.Reduce(world.Rank, Operation&lt;int&gt;.Add, root);
                   }
                 }
               }
             };
               </code>
             </example>
             <typeparam name="T">Any serializable type.</typeparam>
             <param name="value">The local value that will be combined with the values provided by other processes.</param>
             <param name="op">
               The operation used to combine two values. This operation must be associative.
             </param>
             <param name="root">
               The rank of the process that is the root of the reduction operation, which will receive the result
               of the reduction operation in its <paramref name="outValue"/> argument.
             </param>
             <returns>
               On the root, returns the result of the reduction operation. The other processes receive a default value.
             </returns>
        </member>
        <member name="M:MPI.Intracommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32,``0[]@)">
            <summary>
              <c>Reduce</c> is a collective algorithm that combines the values stored by each process into a 
              single value available at the designated <paramref name="root"/> process. This particular variant of
              <see cref="M:MPI.Intracommunicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)"/> applies to each of the elements
              of the provided arrays. Each process must provide arrays of the same length, and the values at each
              array index are combined 
              in a user-defined way, specified via a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> 
              are the ith values provided by the N processes in the communicator, the ith result will be the value 
              <c>value1 op value2 op ... op valueN</c>. The resulting array is only
              available to the <paramref name="root"/> process. If all processes need the result of the reduction,
              use <see cref="M:MPI.Intracommunicator.Allreduce``1(``0[],MPI.ReductionOperation{``0},``0[]@)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The local values that will be combined with the values provided by other processes.</param>
            <param name="op">
              The operation used to combine two values. This operation must be associative.
            </param>
            <param name="root">
              The rank of the process that is the root of the reduction operation, which will receive the result
              of the reduction operation in its <paramref name="outValue"/> argument.
            </param>
            <param name="outValues">
              The variable that will receive the results of the reduction operation. Only the <paramref name="root"/>
              process will receive the actual result; all other processes will receive a default-initialized value.
              When this collective operation returns, <paramref name="outValues"/> will be of the same length as
              <paramref name="inValues"/>.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32)">
            <summary>
              <c>Reduce</c> is a collective algorithm that combines the values stored by each process into a 
              single value available at the designated <paramref name="root"/> process. This particular variant of
              <see cref="M:MPI.Intracommunicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)"/> applies to each of the elements
              of the provided arrays. Each process must provide arrays of the same length, and the values at each
              array index are combined 
              in a user-defined way, specified via a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> 
              are the ith values provided by the N processes in the communicator, the ith result will be the value 
              <c>value1 op value2 op ... op valueN</c>. The resulting array is only
              available to the <paramref name="root"/> process. If all processes need the result of the reduction,
              use <see cref="M:MPI.Intracommunicator.Allreduce``1(``0[],MPI.ReductionOperation{``0},``0[]@)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">The local values that will be combined with the values provided by other processes.</param>
            <param name="op">
              The operation used to combine two values. This operation must be associative.
            </param>
            <param name="root">
              The rank of the process that is the root of the reduction operation, to which this routine will
              return the result of the reduction operation. 
            </param>
            <returns>
              On the root, an array that contains the result of elementwise reduction on the <paramref name="values"/>
              arrays provided by each process. On all other processes, <c>null</c>.
            </returns>
        </member>
        <member name="M:MPI.Intracommunicator.ReduceScatter``1(``0[],MPI.ReductionOperation{``0},System.Int32[])">
            <summary>
            A collective operation that first performs a reduction on the given <paramref name="values"/> 
            (see <see cref="M:MPI.Intracommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32)"/> and then scatters
            the results by sending some elements to each process. The reduction will be performed on 
            the entire array of <paramref name="values"/> (like the array form of 
            <see cref="M:MPI.Intracommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32)"/>). Then, the array will
            be scattered, with process i receiving <paramref name="counts"/>[i] elements. The process
            with rank 0 will receive the first <c>counts[0]</c> elements, the process with rank 1 will 
            receive the next <c>counts[1]</c> elements, and so on.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              An array of values that will be reduced. The length of this array must equal to 
              sum of the counts in <paramref name="counts"/>.
            </param>
            <param name="op">
              The operation used to combine the elements in <paramref name="values"/>.
              This operation must be associative.
            </param>
            <param name="counts">
              An array whose ith element states the number of elements from the reduced result 
              that will be returned to the process with rank i. 
            </param>
            <returns>
              An array of length <c>counts[Rank]</c> containing the reduced results destined for
              the calling process.
            </returns>
        </member>
        <member name="M:MPI.Intracommunicator.ReduceScatter``1(``0[],MPI.ReductionOperation{``0},System.Int32[],``0[]@)">
            <summary>
            A collective operation that first performs a reduction on the given <paramref name="values"/> 
            (see <see cref="M:MPI.Intracommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32,``0[]@)"/> and then scatters
            the results by sending some elements to each process. The reduction will be performed on 
            the entire array of <paramref name="values"/> (like the array form of 
            <see cref="M:MPI.Intracommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32,``0[]@)"/>). Then, the array will
            be scattered, with process i receiving <paramref name="counts"/>[i] elements. The process
            with rank 0 will receive the first <c>counts[0]</c> elements, the process with rank 1 will 
            receive the next <c>counts[1]</c> elements, and so on.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              An array of values that will be reduced. The length of this array must equal to 
              sum of the counts in <paramref name="counts"/>.
            </param>
            <param name="op">
              The operation used to combine the elements in <paramref name="inValues"/>.
              This operation must be associative.
            </param>
            <param name="counts">
              An array whose ith element states the number of elements from the reduced result 
              that will be returned to the process with rank i. 
            </param>
            <param name="outValues">
              An array of length <c>counts[Rank]</c> containing the reduced results destined for
              the calling process.
            </param>
        </member>
        <member name="M:MPI.Intracommunicator.Scan``1(``0,MPI.ReductionOperation{``0})">
            <summary>
              <c>Scan</c> is a collective algorithm that performs partial reductions on data provided by
              each process in the communicator. Scan combines the values stored by each process into 
              partial results delivered to each process. The values are combined 
              in a user-defined way, specified via a delegate. If <c>value(0)</c>, <c>value(1)</c>, ..., <c>value(N-1)</c> 
              are the values provided by the N processes in the communicator, the resulting value for the process with 
              rank P will be <c>value(0) op value(1) op ... op value(P)</c>. The processor with rank <c>N-1</c> will
              receive the same result as if one had performed a <see cref="M:MPI.Intracommunicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)"/> 
              operation with root <c>N-1</c>.
            
              <c>Scan</c> is sometimes called an "inclusive" scan, because the result returned to each process
              includes the contribution of that process. For an "exclusive" scan (that does not include the contribution
              of the calling process in its result), use <see cref="M:MPI.Intracommunicator.ExclusiveScan``1(``0,MPI.ReductionOperation{``0})"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">The value contributed by the calling process.</param>
            <param name="op">Operation used to combine two values from different processes.</param>
            <returns>The value <c>value(0) op value(1) op ... op value(Rank)</c></returns>
        </member>
        <member name="M:MPI.Intracommunicator.Scan``1(``0[],MPI.ReductionOperation{``0})">
            <summary>
              <c>Scan</c> is a collective algorithm that performs partial reductions on data provided by
              each process in the communicator. Scan combines the arrays stored by each process into 
              partial results delivered to each process. The arrays are combined in a user-defined way, 
              specified via a delegate that will be applied elementwise to the values in each arrays. 
              If <c>array(0)</c>, <c>array(1)</c>, ..., <c>array(N-1)</c> are the arrays provided by the 
              N processes in the communicator, the resulting array for the process with 
              rank P will be <c>array(0) op array(1) op ... op array(P)</c>. The processor with rank <c>N-1</c> will
              receive the same result as if one had performed a 
              <see cref="M:MPI.Intracommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32,``0[]@)"/> 
              operation with root <c>N-1</c>.
            
              <c>Scan</c> is sometimes called an "inclusive" scan, because the result returned to each process
              includes the contribution of that process. For an "exclusive" scan (that does not include the contribution
              of the calling process in its result), use <see cref="M:MPI.Intracommunicator.ExclusiveScan``1(``0[],MPI.ReductionOperation{``0})"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              The array contributed by the calling process. The arrays provided by each process must
               have the same length.
            </param>
            <param name="op">Operation used to combine two values from different processes.</param>
            <returns>The array <c>array(0) op array(1) op ... op array(Rank)</c></returns>
        </member>
        <member name="M:MPI.Intracommunicator.Scan``1(``0[],MPI.ReductionOperation{``0},``0[]@)">
            <summary>
              <c>Scan</c> is a collective algorithm that performs partial reductions on data provided by
              each process in the communicator. Scan combines the arrays stored by each process into 
              partial results delivered to each process. The arrays are combined in a user-defined way, 
              specified via a delegate that will be applied elementwise to the values in each arrays. 
              If <c>array(0)</c>, <c>array(1)</c>, ..., <c>array(N-1)</c> are the arrays provided by the 
              N processes in the communicator, the resulting array for the process with 
              rank P will be <c>array(0) op array(1) op ... op array(P)</c>. The processor with rank <c>N-1</c> will
              receive the same result as if one had performed a 
              <see cref="M:MPI.Intracommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32,``0[]@)"/> 
              operation with root <c>N-1</c>.
            
              <c>Scan</c> is sometimes called an "inclusive" scan, because the result returned to each process
              includes the contribution of that process. For an "exclusive" scan (that does not include the contribution
              of the calling process in its result), use <see cref="M:MPI.Intracommunicator.ExclusiveScan``1(``0[],MPI.ReductionOperation{``0},``0[]@)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The array contributed by the calling process. The arrays provided by each process must
               have the same length.
            </param>
            <param name="op">Operation used to combine two values from different processes.</param>
            <param name="outValues">The array <c>array(0) op array(1) op ... op array(Rank)</c></param>
        </member>
        <member name="M:MPI.Intracommunicator.Scatter``1(``0[])">
            <summary>
            Scatters an array of values by sending the ith value of the array to processor i. 
            This variant of <c>Scatter</c> can only be called by the root process. Other processes
            should call the non-root variant of <see cref="M:MPI.Intracommunicator.Scatter``1(System.Int32)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              An array of values of length <see cref="P:MPI.Communicator.Size"/>, which is only significant at the root. 
              The ith value of this array (at the root process) will be sent to the ith processor.
            </param>
            <returns>
              The ith value of the <paramref name="values"/> array provided by the root process, where i is
              the rank of the calling process.
            </returns>
        </member>
        <member name="M:MPI.Intracommunicator.Scatter``1(System.Int32)">
            <summary>
              Scatters an array of values by sending the ith value of the array to processor i. 
              This variant of <c>Scatter</c> can only be called by a non-root process. The root process
              should either call the root-only variant of <see cref="M:MPI.Intracommunicator.Scatter``1(``0[])"/> or the
              general <see cref="M:MPI.Intracommunicator.Scatter``1(``0[],System.Int32)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <returns>
              The ith value of the <paramref name="values"/> array provided by the root process, where i is
              the rank of the calling process.
            </returns>
        </member>
        <member name="M:MPI.Intracommunicator.Scatter``1(``0[],System.Int32)">
            <summary>
            Scatters an array of values by sending the ith value of the array to processor i. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              An array of values of length <see cref="P:MPI.Communicator.Size"/>, which is only significant at the root. 
              The ith value of this array (at the root process) will be sent to the ith processor.
            </param>
            <param name="root">
              Rank of the "root" process, which will supply the array of values to be scattered.
            </param>
            <returns>
              The ith value of the <paramref name="values"/> array provided by the root process, where i is
              the rank of the calling process.
            </returns>
        </member>
        <member name="M:MPI.Intracommunicator.Scatter``1(System.Boolean,``0[],System.Int32)">
            <summary>
            Scatters an array of values by sending the ith value of the array to processor i.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="isRoot">Whether this process is root (== root for Intracommunicators, or 
              ==Intercommunicator.Root for Intercommunicators).</param>
            <param name="values">
              An array of values of length <see cref="P:MPI.Communicator.Size"/>, which is only significant at the root. 
              The ith value of this array (at the root process) will be sent to the ith processor.
            </param>
            <param name="root">
              Rank of the "root" process, which will supply the array of values to be scattered.
            </param>
            <returns>
              The ith value of the <paramref name="values"/> array provided by the root process, where i is
              the rank of the calling process.
            </returns>
        </member>
        <member name="M:MPI.Intracommunicator.ScatterFromFlattened``1(``0[],System.Int32,System.Int32)">
            <summary>
              Scatter a one dimensional array to all processes, where multiple items are sent to each process.
              (If the number of items to be sent is different, see 
              <see cref="M:MPI.Intracommunicator.ScatterFromFlattened``1(``0[],System.Int32[],System.Int32,``0[]@)"/>
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The array to be scattered. Only significant at the root.</param>
            <param name="count">
              The number of items to be received by each process. If T is a value type (primitive or
              structure) count must be the same at each process (not just at the root). If T must 
              be serialized, count is ignored at processes other than the root.
            </param>
            <param name="root">The rank of the root process.</param>      
        </member>
        <member name="M:MPI.Intracommunicator.ScatterFromFlattened``1(``0[],System.Int32[],System.Int32)">
            <summary>
              Scatter a one dimensional array to all processes, where the number of data items sent to 
              each process may differ.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The array to be scattered. Only significant at the root.</param>
            <param name="counts">
              The number of items to be received by each process. If T must 
              be serialized (i.e. is not a value type), counts is ignored at processes other than the root.
            </param>
            <param name="root">The rank of the root process.</param>       
        </member>
        <member name="M:MPI.Intracommunicator.ScatterFromFlattened``1(``0[],System.Int32,System.Int32,``0[]@)">
            <summary>
              Scatter a one dimensional array to all processes, where multiple items are sent to each process.
              (If the number of items to be sent is different, see 
              <see cref="M:MPI.Intracommunicator.ScatterFromFlattened``1(``0[],System.Int32[],System.Int32,``0[]@)"/>
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The array to be scattered. Only significant at the root.</param>
            <param name="count">
              The number of items to be received by each process. If T is a value type (primitive or
              structure) count must be the same at each process (not just at the root). If T must 
              be serialized, count is ignored at processes other than the root.
            </param>
            <param name="root">The rank of the root process.</param>
            <param name="outValues">The array to write to at the receiving process. Does not have to be preallocated.</param>
        </member>
        <member name="M:MPI.Intracommunicator.ScatterFromFlattened``1(``0[],System.Int32[],System.Int32,``0[]@)">
            <summary>
              Scatter a one dimensional array to all processes, where the number of data items sent to 
              each process may differ.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The array to be scattered. Only significant at the root.</param>
            <param name="counts">
              The number of items to be received by each process. If T must 
              be serialized (i.e. is not a value type), counts is ignored at processes other than the root.
            </param>
            <param name="root">The rank of the root process.</param>
            <param name="outValues">The array to write to at the receiving process. Does not have to be preallocated.</param>
        </member>
        <member name="M:MPI.Intracommunicator.Spawn(System.String,System.String[],System.Int32,System.Int32)">
            <summary>
              Spawns child MPI processes, providing an <see cref="T:MPI.Intercommunicator"/> for communicating with them.
            </summary>
            <param name="command">The name of the program to execute.</param>
            <param name="argv">A list of arguments to pass to the program.</param>
            <param name="maxprocs">The number of processes to be created.</param>
            <param name="root">The process from which to execute the child programs.</param>
            <param name="array_of_errorcodes">An array to store the success of each child process. Should have length equal to <paramref name="maxprocs"/>.</param>
            <returns>An Intercommunicator with the original Intracommunicator processes in one group and the child processes in the other group.</returns>
        </member>
        <member name="M:MPI.TopologicalCommunicator.NeighborsOf(System.Int32)">
            <summary>
              Retrieve the neighbors of another process.
            </summary>
            <param name="rank">
              The rank of the process to be queried.
            </param>
            <returns>
              The neighbors of the process.
            </returns>
        </member>
        <member name="P:MPI.TopologicalCommunicator.Neighbors">
            <summary>
              The neighbors of the current process.
            </summary>
        </member>
        <member name="P:MPI.TopologicalCommunicator.NumEdges">
            <summary>
              The number of edges in the communicator.
            </summary>
        </member>
        <member name="M:MPI.CartesianCommunicator.ComputeDimensions(System.Int32,System.Int32,System.Int32[]@)">
            <summary>
              Suggests a size for each dimension for a Cartesian topology, given the number of nodes and dimensions.
            </summary>
            <param name="nnodes">
              The number of nodes in the grid.
            </param>
            <param name="ndims">
              The number of dimensions.
            </param>
            <param name="dims">
              An array of size <paramref name="ndims"/> to store the suggested sizes. Nonzero entries will be
              taken as given sizes.
            </param>
        </member>
        <member name="M:MPI.CartesianCommunicator.Map(MPI.Intracommunicator,System.Int32,System.Int32[],System.Boolean[])">
            <summary>
              Returns a recommended configuration for a new Cartesian grid.
            </summary>
            <param name="oldcomm">
              The existing communicator.
            </param>
            <param name="ndims">
              The number of dimensions for the Cartesian grid.
            </param>
            <param name="dims">
              An array of length <paramref name="ndims"/> indicating the size of the grid in each dimension.
            </param>
            <param name="periods">
              A logical array of length <paramref name="ndims"/> indicating whether the grid is periodic in any given dimension.
            </param>
            <returns>
              The new rank of the calling process.
            </returns>
        </member>
        <member name="M:MPI.CartesianCommunicator.#ctor">
            <summary>
            CartesianCommunicators can only be constructed from other communicators or adopted
            from low-level Cartesian communicators.
            </summary>
        </member>
        <member name="M:MPI.CartesianCommunicator.#ctor(MPI.Intracommunicator,System.Int32,System.Int32[],System.Boolean[],System.Boolean)">
            <summary>
              Creates a <see cref="T:MPI.CartesianCommunicator"/>.
            </summary>
            <param name="oldComm">
              An existing Intracommunicator from which to create the new communicator (e.g. <see cref="F:MPI.Communicator.world"/>).
            </param>
            <param name="ndims">
              The number of dimensions for the Cartesian grid.
            </param>
            <param name="dims">
              An array of length <paramref name="ndims"/> indicating the size of the grid in each dimension.
            </param>
            <param name="periods">
              A logical array of length <paramref name="ndims"/> indicating whether the grid is periodic in any given dimension.
            </param>
            <param name="reorder">
              Logical indicating whether ranks may be reordered or not.
            </param>
        </member>
        <member name="M:MPI.CartesianCommunicator.Subgrid(System.Int32[])">
            <summary>
              Construct a lower dimensional subgrid from an existing CartesianCommunicator.
            </summary>
            <param name="remain_dims">
               Logical array with an entry for each dimension indicating whether a dimension 
               should be kept or dropped in the new communicator.
            </param>
            <returns>
              The new lower-dimensional communicator.
            </returns>
        </member>
        <member name="M:MPI.CartesianCommunicator.AttachToComm">
            <summary>
              Initializes values for Property accessors.
            </summary>
        </member>
        <member name="M:MPI.CartesianCommunicator.GetCartesianRank(System.Int32[])">
            <summary>
              Returns a process' rank given its coordinates in the CartesianCommunicator's grid.
            </summary>
            <param name="coords">
              An integer array specifying the processes' coordinates.
            </param>
            <returns>
              The processes' rank in the communicator.
            </returns>
        </member>
        <member name="M:MPI.CartesianCommunicator.GetCartesianCoordinates(System.Int32)">
            <summary>
              Provides the coordinates in the communicator's grid of a process, given its rank.
            </summary>
            <param name="rank">
              The processes' rank in the communicator.
            </param>
            <returns>
              An array of ints giving the coordinates in each dimension.
            </returns>
        </member>
        <member name="M:MPI.CartesianCommunicator.NeighborsOf(System.Int32)">
            <summary>
              Retrieve the neighbors of another process.
            </summary>
            <param name="Rank">
              The rank of the process to be queried.
            </param>
            <returns>
              The neighbors of the process.
            </returns>
        </member>
        <member name="M:MPI.CartesianCommunicator.Shift(System.Int32,System.Int32,System.Int32@,System.Int32@)">
            <summary>
              Finds the source and destination ranks necessary to shift data along the grid. 
            </summary>
            <param name="direction">
              The dimension in which to shift.
            </param>
            <param name="disp">
              The distance along the grid to shift. Positive values indicate upward shifts,
               negative values downward shifts.
            </param>
            <param name="rank_source">
              Will contain the rank of the source process.
            </param>
            <param name="rank_dest">
              Will contain the rank of the destination process.
            </param>
        </member>
        <member name="F:MPI.CartesianCommunicator.NDims">
            <summary>
              The number of dimensions of the communicator.
            </summary>
        </member>
        <member name="F:MPI.CartesianCommunicator.Dims">
            <summary>
              The dimensions of the communicator.
            </summary>
        </member>
        <member name="F:MPI.CartesianCommunicator.Periods">
            <summary>
              The peridocity of the communicator in each dimension.
            </summary>
        </member>
        <member name="F:MPI.CartesianCommunicator.Coords">
            <summary>
              The coordinates in the communicator of the current process.
            </summary>
        </member>
        <member name="P:MPI.CartesianCommunicator.Dimensions">
            <summary>
              The size in each dimension of the Cartesian grid.
            </summary>
        </member>
        <member name="P:MPI.CartesianCommunicator.Periodic">
            <summary>
              Whether the Cartesian grid is periodic in any given dimension.
            </summary>
        </member>
        <member name="P:MPI.CartesianCommunicator.Coordinates">
            <summary>
              The coordinates of the current process.
            </summary>
        </member>
        <member name="P:MPI.CartesianCommunicator.NumEdges">
            <summary>
              The number of edges in the communicator.
            </summary>
        </member>
        <member name="P:MPI.CartesianCommunicator.Neighbors">
            <summary>
              The neighbors of the current process.
            </summary>
        </member>
        <member name="T:MPI.DatatypeCache">
            <summary>
            Provides a mapping from .NET types to their corresponding MPI datatypes.
            This class should only be used by experts in both MPI's low-level (C) 
            interfaces and the interaction between managed and unmanaged code in .NET.
            </summary>
        </member>
        <member name="M:MPI.DatatypeCache.BuildDatatype(System.Type)">
            <summary>
            Builds a new MPI datatype by using reflection on the given type.
            </summary>
        </member>
        <member name="M:MPI.DatatypeCache.GetDatatype(System.Type)">
            <summary>
            Get the MPI datatype associated with the given type. 
            </summary>
            <param name="type">
              The type for which we want to build or find an MPI datatype.
            </param>
            <returns>
              <see cref="F:MPI.Unsafe.MPI_DATATYPE_NULL"/> if there is no corresponding MPI 
              datatype for this type. Otherwise, returns the MPI datatype that can be
              used to transfer objects of this type without serialization.
            </returns>
        </member>
        <member name="M:MPI.DatatypeCache.BuiltinDatatypes">
            <summary>
            Builds a dictionary containing all of the built-in datatypes.
            </summary>
            <returns></returns>
        </member>
        <member name="F:MPI.DatatypeCache.cache">
            <summary>
            Contains a mapping from value types to their MPI datatype equivalents
            </summary>
        </member>
        <member name="T:MPI.DatatypeCache.Packed">
            <summary>
            Placeholder type that is used to indicate that data being sent by one of the
            low-level MPI routines is packed by MPI.
            </summary>
            <remarks>
            Advanced users can explicitly pack data to be transmitted via MPI with the 
            <see cref="M:MPI.Unsafe.MPI_Pack(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32@,System.Int32)"/> routine, then unpack that data via
            <see cref="M:MPI.Unsafe.MPI_Unpack(System.IntPtr,System.Int32,System.Int32@,System.IntPtr,System.Int32,System.Int32,System.Int32)"/>.
            </remarks>
        </member>
        <member name="T:MPI.Threading">
            <summary>
            Enumeration describing the level of threading support provided by the MPI implementation.
            The MPI environment should be initialized with the minimum threading support required
            for your application, because additional threading support can have a negative impact
            on performance.
            
            The four options providing monotonically-increasing levels of freedom for the MPI
            program. Thus, a program implemented based on the <c>Threading.Single</c> semantics
            will work perfectly well (although perhaps less efficiently) with <c>Threading.Multiple</c>.
            </summary>
        </member>
        <member name="F:MPI.Threading.Single">
            <summary>
            The program is single-threaded, and does not ever create additional threads
            (even those that don't use MPI).
            </summary>
        </member>
        <member name="F:MPI.Threading.Funneled">
            <summary>
            The program is multi-threaded, but only one of those threads (the main thread)
            will make calls into the MPI library. Thus, all of the MPI calls are "funneled"
            into that one thread. One can determine if the currently-executing thread is
            the main thread via <see cref="P:MPI.Environment.IsMainThread"/>.
            </summary>
        </member>
        <member name="F:MPI.Threading.Serialized">
            <summary>
            The program is multi-threaded, and more than one thread will make calls into 
            the MPI library. However, these calls into MPI are "serialized", in that
            no two threads will call the MPI library at the same time. This mode essentially
            requires the user program to have the equivalent of a global lock around all calls
            to MPI.
            </summary>
        </member>
        <member name="F:MPI.Threading.Multiple">
            <summary>
            The program is multi-threaded, and more than one thread will make calls into
            the MPI library. The program is free to call the MPI library routines from
            any thread at any time, including having multiple threads calling MPI
            concurrently. Note that this level of threading is not supported by MS MPI.
            </summary>
        </member>
        <member name="T:MPI.Environment">
            <summary>Provides MPI initialization, finalization, and environmental queries.</summary>
            <remarks>
              The <c>Environment</c> class provides initialization, finalization, and
              environmental query operations for the Message Passing Interface.
              Users must initialize the MPI environment before using any of the
              MPI facilities by creating an instanceof the Environment class.
            </remarks>
        </member>
        <member name="M:MPI.Environment.#ctor(System.String[]@)">
            <summary>
              Initializes the MPI environment. This variant of the <see cref="T:MPI.Environment"/> constructor 
              initializes the MPI environment with the <see cref="F:MPI.Threading.Serialized"/> threading model.
            </summary>
            <param name="args">
              Arguments passed to the <c>Main</c> function in your program. MPI 
              may use some of these arguments for its initialization, and will remove 
              them from this argument before returning.
            </param>
            <remarks>
              This routine must be invoked before using any other MPI facilities. 
              Be sure to call <c>Dispose()</c> to finalize the MPI environment before exiting!
            </remarks>
            <example>This simple program initializes MPI and writes out the rank of each processor:
            <code>
            using MPI;
            
            public class Hello 
            {
                static void Main(string[] args)
                {
                    using (MPI.Environment env = new MPI.Environment(ref args))
                    {
                        System.Console.WriteLine("Hello, from process number " 
                            + MPI.Communicator.world.Rank.ToString() + " of "
                            + MPI.Communicator.world.Size.ToString());
                    }
                }
            }
            </code>
            </example>
        </member>
        <member name="M:MPI.Environment.#ctor(System.String[]@,MPI.Threading)">
            <summary>Initializes the MPI environment.</summary>
            <param name="args">
              Arguments passed to the <c>Main</c> function in your program. MPI 
              may use some of these arguments for its initialization, and will remove 
              them from this argument before returning.
            </param>
            <param name="threading">
              The level of threading support requested of the MPI implementation. The
              implementation will attempt to provide this level of threading support.
              However, the actual level of threading support provided will be published
              via the <see cref="P:MPI.Environment.Threading"/> property.
            </param>
            <remarks>
              This routine must be invoked before using any other MPI facilities. 
              Be sure to call <c>Dispose()</c> to finalize the MPI environment before exiting!
            </remarks>
            <example>This simple program initializes MPI and writes out the rank of each processor:
            <code>
            using MPI;
            
            public class Hello 
            {
                static void Main(string[] args)
                {
                    using (MPI.Environment env = new MPI.Environment(ref args))
                    {
                        System.Console.WriteLine("Hello, from process number " 
                            + MPI.Communicator.world.Rank.ToString() + " of "
                            + MPI.Communicator.world.Size.ToString());
                    }
                }
            }
            </code>
            </example>
        </member>
        <member name="M:MPI.Environment.Dispose">
            <summary>
             Finalizes the MPI environment. Users must call this routine to shut down MPI.
            </summary>
        </member>
        <member name="M:MPI.Environment.Finalize">
            <summary>
              Verifies that the MPI environment has been finalized by calling <c>Dispose()</c>.
            </summary>
        </member>
        <member name="M:MPI.Environment.Abort(System.Int32)">
            <summary>
            Terminates all processes.
            </summary>
            <param name="errorCode">An error code that will be returned to the invoking environment.</param>
        </member>
        <member name="M:MPI.Environment.TranslateErrorIntoException(System.Int32)">
            <summary>
            Translates an MPI error code into an appropriate exception, then throws that exception.
            </summary>
            <param name="errorCode">
              The MPI error code, returned from one of the <see cref="T:MPI.Unsafe"/> class's <c>MPI</c> routines.
            </param>
        </member>
        <member name="F:MPI.Environment.providedThreadLevel">
            <summary>
            The level of threading support actually provided by MPI.
            </summary>
        </member>
        <member name="P:MPI.Environment.Initialized">
            <summary>
            Determine whether the MPI environment has been initialized.
            </summary>
        </member>
        <member name="P:MPI.Environment.Finalized">
            <summary>
            Determine whether the MPI environment has been finalized.
            </summary>
        </member>
        <member name="P:MPI.Environment.Threading">
            <summary>
            The level of threading support provided by the MPI library.
            </summary>
            
            <remarks>
            This value describes whether and how the MPI library can be
            used in multi-threaded programs. The threading level is
            requested when the MPI library is initialized in the
            <see cref="M:MPI.Environment.#ctor(System.String[]@,MPI.Threading)"/>
            constructor.
            </remarks>
        </member>
        <member name="P:MPI.Environment.IsMainThread">
            <summary>
            Determines whether the calling thread is the main MPI thread. Will return
            true for the thread that called the <see cref="T:MPI.Environment"/> constructor
            to initialize MPI. The main thread is particularly important when the threading
            mode is <see cref="F:MPI.Threading.Funneled"/>, because in that model only the
            main thread can invoke MPI routines.
            </summary>
        </member>
        <member name="P:MPI.Environment.ProcessorName">
            <summary>
            Returns the name of the currently executing processor.
            This name does not have any specific form, but typically
            identifies the computere on which the process is executing.
            </summary>
        </member>
        <member name="P:MPI.Environment.Time">
            <summary>
            Returns the time, in seconds, since some arbitrary time in the past.
            This value is typically used for timing parallel applications.
            </summary>
        </member>
        <member name="P:MPI.Environment.TimeResolution">
            <summary>
            Returns the resolution of <see cref="P:MPI.Environment.Time"/>, in seconds.
            </summary>
        </member>
        <member name="P:MPI.Environment.IsTimeGlobal">
            <summary>
            Determines whether the <see cref="P:MPI.Environment.Time"/> value is synchronized
            across all MPI processes (i.e., if <see cref="P:MPI.Environment.Time"/> is a global 
            value).
            </summary>
        </member>
        <member name="P:MPI.Environment.MaxTag">
            <summary>
            Returns the maximum allowed tag value for use with MPI's 
            point-to-point operations.
            </summary>
        </member>
        <member name="P:MPI.Environment.HostRank">
            <summary>
            Returns the rank of the "host" process, if any.
            </summary>
        </member>
        <member name="P:MPI.Environment.IORank">
            <summary>
            Returns the rank of the process (or processes) that can perform I/O via the
            normal language facilities. If no such rank exists, the result will be null; if 
            every process can perform I/O, this will return the value
            <see cref="F:MPI.Communicator.anySource"/>.
            </summary>
        </member>
        <member name="T:MPI.ReductionOperation`1">
            <summary>
              A reduction operation that combines two values to produce a third value.
            </summary>
              
            <remarks>
              Reduction operations are used by various collective operations such as 
              <see cref="M:MPI.Intracommunicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/>.
              Note that any operation used as a reduction operation must be associative, 
              e.g., op(x, op(y, z)) == op(op(x, y), z). These operations need not be 
              commutative.
            </remarks>
        </member>
        <member name="T:MPI.SerializedMessageHeader">
            <summary>
            Structure sent on the main MPI communicator for the <see cref="T:MPI.Communicator"/>
            class to indicate that serialized data will be coming on the shadow communicator.
            </summary>
        </member>
        <member name="F:MPI.SerializedMessageHeader.tag">
            <summary>
            The tag of the message containing the serialized data, which will come via
            the shadow communicator.
            </summary>
        </member>
        <member name="F:MPI.SerializedMessageHeader.bytes">
            <summary>
            Number of bytes in the serialized data.
            </summary>
        </member>
        <member name="T:MPI.MessageTruncatedException">
            <summary>
            An exception thrown when an MPI message has been truncated on receive. 
            </summary>
        </member>
        <member name="M:MPI.MessageTruncatedException.#ctor(System.String)">
            <summary>
            Create a new exception stating that a received message has been truncated.
            </summary>
            <param name="message">The message associated with this exception.</param>
        </member>
        <member name="T:MPI.GraphCommunicator">
            <summary>
              A GraphCommunicator is a form of <see cref="T:MPI.Intracommunicator"/> where the processes are
              connected in an arbitrary graph topology.
            </summary>
            
            <remarks>
              With a GraphCommunicator, the topology of the processes is specified as an arbitrary graph, 
              where each process will communicate with its neighbors in the graph. Graph topologies are
              more general than the Cartesian topology provided by a <see cref="T:MPI.CartesianCommunicator"/>, 
              but they also provide less information to the user and the underlying MPI implementation.
            </remarks>
        </member>
        <member name="M:MPI.GraphCommunicator.Map(MPI.Intracommunicator,System.Int32[][])">
            <summary>
              Returns a recommended configuration for a new Graph communicator.
            </summary>
            <param name="oldcomm">
              The existing communicator.
            </param>
            <param name="edges">
              An array of edges as for the constructor <see cref="T:MPI.GraphCommunicator"/>.
            </param>
            <returns>
              The new rank of the calling process.
            </returns>
        </member>
        <member name="M:MPI.GraphCommunicator.#ctor(MPI.Intracommunicator,System.Int32[][],System.Boolean)">
            <summary>
              Constructs a new Graph communicator.
            </summary>
            <param name="oldComm">
              An existing communicator from which to construct the Graph communicator.
            </param>
            <param name="edges">
              A jagged array of adjacency information (so, for example, process i is adjacent to all 
              of the processes whose ranks are listed in edges[i]).
            </param>
            <param name="reorder">
              Whether the ranking may be reordered or not.
            </param>
        </member>
        <member name="M:MPI.GraphCommunicator.NeighborsOf(System.Int32)">
            <summary>
              Retrieve the neighbors of another process.
            </summary>
            <param name="rank">
              The rank of the process to be queried.
            </param>
            <returns>
              The neighbors of the process.
            </returns>
        </member>
        <member name="P:MPI.GraphCommunicator.Edges">
            <summary>
              The array of adjacency information. Entry x in Edges[i][j] indicates i is adjacent to x.
            </summary>
        </member>
        <member name="P:MPI.GraphCommunicator.NumEdges">
            <summary>
              The number of edges in the communicator.
            </summary>
        </member>
        <member name="P:MPI.GraphCommunicator.Neighbors">
            <summary>
              The neighbors of the current process.
            </summary>
        </member>
        <member name="T:MPI.Group">
            <summary>
              The <c>Group</c> class provides the ability to manipulate sets of MPI processes.
            </summary>
        </member>
        <member name="F:MPI.Group.NoProcess">
            <summary>
            Sentinel value used to indicate that a particular process is not part of a group.
            </summary>
        </member>
        <member name="F:MPI.Group.empty">
            <summary>
            A predefined group containing no processes.
            </summary>
        </member>
        <member name="M:MPI.Group.#ctor">
            <summary>
              Users cannot construct a Group directly. They typically will retrieve a group from
              a communicator, from other groups, or, in rare cases, adopt one from the low-level
              interface.
            </summary>
        </member>
        <member name="M:MPI.Group.Adopt(System.Int32)">
            <summary>
              Adopts a low-level MPI group that was created with any of the low-level MPI facilities.
              The resulting <c>Group</c> object will manage the lifetime of the low-level MPI group,
              and will call <see cref="M:MPI.Unsafe.MPI_Group_free(System.Int32@)"/> when it is disposed or finalized.
            </summary>
            <remarks>
              This constructor should only be used in rare cases where the program 
              is manipulating MPI groups through the low-level MPI interface.
            </remarks>
        </member>
        <member name="M:MPI.Group.Finalize">
            <summary>
            Finalizer that frees the MPI group.
            </summary>
        </member>
        <member name="M:MPI.Group.Dispose">
            <summary>
            Free the MPI group explicitly.
            </summary>
        </member>
        <member name="M:MPI.Group.op_BitwiseOr(MPI.Group,MPI.Group)">
            <summary>
            Constructs the union of two groups, containing those processes that are either in
            <paramref name="group1"/> or <paramref name="group2"/>.
            </summary>
        </member>
        <member name="M:MPI.Group.op_BitwiseAnd(MPI.Group,MPI.Group)">
            <summary>
            Constructs the intersection of two groups, containing those processes that are in
            both <paramref name="group1"/> and <paramref name="group2"/>.
            </summary>
        </member>
        <member name="M:MPI.Group.op_Subtraction(MPI.Group,MPI.Group)">
            <summary>
            Constructs the difference of two groups, containing those processes that are in
            <paramref name="group1"/> but not <paramref name="group2"/>.
            </summary>
        </member>
        <member name="M:MPI.Group.IncludeOnly(System.Int32[])">
            <summary>
            Create a subset of this group that includes only the processes with the
            given ranks.
            </summary>
        </member>
        <member name="M:MPI.Group.Exclude(System.Int32[])">
            <summary>
            Create a subset of this group that includes all of the processes in this
            group except those identified by the given ranks.
            </summary>
        </member>
        <member name="M:MPI.Group.TranslateRanks(System.Int32[],MPI.Group)">
            <summary>
              Translates the ranks of processes in this group to the ranks of the same processes within a different group.
            </summary>
            <param name="ranks">The rank values in this group that will be translated.</param>
            <param name="other">The group whose ranks we are translating to.</param>
            <returns>
              An integer array containing the ranks of the processes in <paramref name="other"/> that correspond to
              the ranks of the same processes in this group. For processes that are in this group but not 
              <paramref name="other"/>, the resulting array will contain <see cref="F:MPI.Group.NoProcess"/>.
            </returns>
        </member>
        <member name="M:MPI.Group.Compare(MPI.Group)">
            <summary>
              Compare two MPI groups.
            </summary>
            <list>
              <listheader>
                <term>Value</term>
                <description>Description</description>
              </listheader>
            <item>
              <term><see cref="F:MPI.Comparison.Identical"/></term>
              <description>The two <c>Group</c> objects represent the same group.</description>
            </item>
            <item>
              <term><see cref="F:MPI.Comparison.Congruent"/></term>
              <description>
                The two <c>Group</c> objects contain the same processes with the same ranks,
                but represent different groups.
              </description>
            </item>
            <item>
              <term><see cref="F:MPI.Comparison.Similar"/></term>
              <description>
                The two <c>Group</c> objects contain the same processes, but with different ranks.
              </description>
            </item>
            <item>
              <term><see cref="F:MPI.Comparison.Unequal"/></term>
              <descsription>The two <c>Group</c> objects are different.</descsription>
            </item>
            </list>      
        </member>
        <member name="F:MPI.Group.group">
            <summary>
            The low-level MPI group handle.
            </summary>
        </member>
        <member name="P:MPI.Group.Size">
            <summary>
              The number of processes within this group.
            </summary>
        </member>
        <member name="P:MPI.Group.Rank">
            <summary>
              The rank of the calling process within this group. This will be a value in [0, <see cref="P:MPI.Group.Size"/>-1).
            </summary>
        </member>
        <member name="T:MPI.Intercommunicator">
            <summary>
              Intercommunicators are <see cref="T:MPI.Communicator"/>s that contain two disjoint groups of 
              processes, an each process can communicate only with processes in the other group.
            </summary>
             
            <remarks>
              Intercommunicators effectively create a communication pattern that follows a bipartite graph.
              Call the two disjoint groups of processes A and B. Any process in group A can send
              a message to or receive a message from any process in group B, and vice-versa. However, there
              is no way to use an intercommunicator to send messages among the processes within a group.
              Intercommunicators are often useful in large MPI applications that tie together many, smaller
              modules. Typically, each module will have its own intracommunicators and the modules will 
              interact with each other via intercommunicators.
            </remarks>
        </member>
        <member name="F:MPI.Intercommunicator.Root">
            <summary>
            Indicates that this process is the root for a collective operation. The root process should
            pass this constant in as the value for root for calls to collective operations.
            Other processes in the same group as root should use <see cref="F:MPI.Intercommunicator.Null">Null</see>. Processes
            in the group sending to the root should use the root process' local rank.
            </summary>
        </member>
        <member name="F:MPI.Intercommunicator.Null">
            <summary>
            For a collective operation, indicates that this process is the same group as the root but is 
            not the root (and thus is not receiving any data). These processes should pass this is as
            the root for calls to collective communication procedures. Processes
            in the group sending to the root should use the root process' local rank. The root process
            should pass in <see cref="F:MPI.Intercommunicator.Root">Root</see>.
            </summary>
        </member>
        <member name="F:MPI.Intercommunicator.collectiveTag">
            <summary>
            A tag on the <see cref="F:MPI.Communicator.shadowComm"/> reserved for communications that are part of collective operations.
            </summary>
        </member>
        <member name="M:MPI.Intercommunicator.#ctor">
            <summary>
            Intercommunicators can only be constructed from other communicators or adopted
            from low-level intercommunicators.
            </summary>
        </member>
        <member name="M:MPI.Intercommunicator.#ctor(MPI.Intracommunicator,System.Int32,MPI.Intracommunicator,System.Int32,System.Int32)">
            <summary>
            Constructs a new intercommunicator that provides communication between two groups of processes,
            specified by intracommunicators. This is a collective operation involving all of the processes
            in the communicators passed via <paramref name="localComm"/>.
            </summary>
            <param name="localComm">
              Intracommunicator whose processes will form the local group in the resulting 
              intercommunication. Since there are two groups in an intercommunicator (each of which is
              identified by an intracommunicator), each process will provide the communicator for its
              own "local" group. Note that a process cannot be in both groups.
            </param>
            <param name="localLeader">
              The rank of the "leader" in the local group. Each of the processes must provide
              the same leader rank as all of the other processes within its group. 
            </param>
            <param name="bridgeComm">
              A communicator that permits communication between the leaders of the two groups. 
              This communicator is used internally to construct the intercommunicator, so there must
              not be any pending communication that might interfere with point-to-pointer messages with
              tag <paramref name="tag"/>. This parameter is only significant for the processes
              that are leaders of their group.
            </param>
            <param name="remoteLeader">
              The rank of the remote group's leader within the communicator <paramref name="bridgeComm"/>.
            </param>
            <param name="tag">
              A tag value that indicates which intercommunicator is used to transmit setup messages across
              the communicator <paramref name="bridgeComm"/>. Pick a tag value that will not interfere
              with other communicator on <paramref name="bridgeComm"/>.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.Adopt(System.Int32)">
            <summary>
              Adopts a low-level MPI intercommunicator that was created with any of the low-level MPI facilities.
              The resulting <c>Intercommunicator</c> object will manage the lifetime of the low-level MPI intercommunicator,
              and will call <see cref="M:MPI.Unsafe.MPI_Comm_free(System.Int32@)"/> when it is disposed or finalized.
            </summary>
            <remarks>
              This constructor should only be used in rare cases where the program 
              is manipulating MPI intercommunicators through the low-level MPI interface.
            </remarks>
        </member>
        <member name="M:MPI.Intercommunicator.Merge(System.Boolean)">
            <summary>
            Merge all of the processes in both groups of an intercommunicator into a single
            intracommunicator. While the intercommunicator only allows communication among processes
            in different groups, the merged intracommunicator allows communication among all of the 
            processes in either group, regardless of which group the source and target were in.
            </summary>
            <param name="upper">
              All of the processes within each group should provide the same value. If one group's
              processes provide the value <c>true</c> while the other group's processes provide the value
              <c>false</c>, the group providing the value <c>true</c> will receive the higher-numbered
              ranks in the resulting intracommunicator. If all processes provide the same value, 
              the order of the union is arbitrary.
            </param>
            <returns>The new intracommunicator.</returns>
        </member>
        <member name="M:MPI.Intercommunicator.Allgather``1(``0)">
            <summary>
            Gathers individual values provided by each processor from the other group into an array of values in which the
            ith element of the array corresponds to the value provided by the processor with rank i.
            The resulting array of values is available to all of the processors. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">
              The value supplied by this processor, which will be placed into the element with index <see cref="P:MPI.Communicator.Rank"/>
              in the array sent to processes in the other group.
            </param>
            <returns> 
              An array of values, where the ith value in the array is a copy of the <paramref name="value"/>
              argument provided by the processor from the other group with rank i.
            </returns>
        </member>
        <member name="M:MPI.Intercommunicator.Allgather``1(``0,``0[]@)">
            <summary>
            Gathers individual values provided by each processor from the other group into an array of values in which the
            ith element of the array corresponds to the value provided by the processor with rank i.
            The resulting array of values is available to all of the processors.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValue">
              The value supplied by this processor, which will be placed into the element with index <see cref="P:MPI.Communicator.Rank"/>
              in the array sent to processes in the other group.
            </param>
            <param name="outValues">
              An array of values, where the ith value in the array is a copy of the <paramref name="inValue"/>
              argument provided by the processor from the other group with rank i.
              Supply this argument when you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.AllgatherFlattened``1(``0[],System.Int32,``0[]@)">
            <summary>
              Gathers invidividual values provided by each processor from the other group into an array of values.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The values supplied by this processor.
            </param>
            <param name="count">
              The number of items to be received by each process in this group.
            </param>
            <param name="outValues">
              An array of values supplied by all processes of the other group. Supply this argument when 
              you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.AllgatherFlattened``1(``0[],System.Int32[],``0[]@)">
            <summary>
            Gathers invididual values provided by each processor into an array of values.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The values supplied by this processor.
            </param>
            <param name="counts">
              The numbers of items to be received by each process in this group.
            </param>
            <param name="outValues">
              An array of values supplied by all processes. Supply this argument when 
              you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.Allreduce``1(``0,MPI.ReductionOperation{``0})">
            <summary>
              <c>Allreduce</c> is a collective algorithm that combines the values supplied by each process of the other group
              into a single value available to all processes. The values are combined in a user-defined way, specified via 
              a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> are the values provided by the 
              N processes in the communicator, the result will be the value <c>value1 op value2 op ... op valueN</c>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">The local value that will be combined with the values provided by other processes
              and sent to each process in the other group.</param>
            <param name="op">
              The operation used to combine two values. This operation must be associative.
            </param>
            <returns>The result of the reduction. The same value will be returned to all processes in a group.</returns>
        </member>
        <member name="M:MPI.Intercommunicator.Allreduce``1(``0[],MPI.ReductionOperation{``0})">
            <summary>
              <c>Allreduce</c> is a collective algorithm that combines the values supplied by each process of the other group
              into a single value available to all processes. The values are combined in a user-defined way, specified via 
              a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> are the values provided by the 
              N processes in the communicator, the result will be the value <c>value1 op value2 op ... op valueN</c>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">The local values that will be combined with the values provided by other processes
              and sent to each process in the other group.</param>
            <param name="op">
              The operation used to combine two values. This operation must be associative.
            </param>
            <returns>
              The values that result from combining all of the values in <paramref name="values"/> (as provided by the 
              processes of the other group)
              element-by-element. If needed, this array will be resized to the same size as <paramref name="inValues"/>.
            </returns>
        </member>
        <member name="M:MPI.Intercommunicator.Allreduce``1(``0[],MPI.ReductionOperation{``0},``0[]@)">
            <summary>
              <c>Allreduce</c> is a collective algorithm that combines the values supplied by each process of the other group
              into a single value available to all processes. The values are combined in a user-defined way, specified via 
              a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> are the values provided by the 
              N processes in the communicator, the result will be the value <c>value1 op value2 op ... op valueN</c>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The local values that will be combined with the values provided by other processes
              and sent to each process in the other group.</param>
            <param name="op">
              The operation used to combine two values. This operation must be associative.
            </param>
            <param name="outValues">
              The values that result from combining all of the values in <paramref name="values"/> (as provided by the 
              processes of the other group)
              element-by-element. If needed, this array will be resized to the same size as <paramref name="inValues"/>.
              Supply this argument when you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.Alltoall``1(``0[])">
            <summary>
            Collective operation in which every process in one group sends data to every process of the other group. 
            <c>Alltoall</c> differs from <see cref="M:MPI.Intercommunicator.Allgather``1(``0)"/> in that a given process can send different
            data to all of the other processes, rather than contributing the same piece of data to all
            processes. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              The array of values that will be sent to each process in the other group. The ith value in this array
              will be sent to the process with rank i.
            </param>
            <returns>
              An array of values received from all of the processes in the other group. The jth value in this
              array will be the value sent to the calling process from the process with rank j.
            </returns>
        </member>
        <member name="M:MPI.Intercommunicator.Alltoall``1(``0[],``0[]@)">
            <summary>
            Collective operation in which every process in one group sends data to every process of the other group. 
            <c>Alltoall</c> differs from <see cref="M:MPI.Intercommunicator.Allgather``1(``0)"/> in that a given process can send different
            data to all of the other processes, rather than contributing the same piece of data to all
            processes. 
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The array of values that will be sent to each process. The ith value in this array
              will be sent to the process with rank i.
            </param>
            <param name="outValues">
              An array of values received from all of the processes in the other group. The jth value in this
              array will be the value sent to the calling process from the process with rank j.
              Supply this argument when you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.AlltoallFlattened``1(``0[],System.Int32[],System.Int32[],``0[]@)">
            <summary>
            Collective operation in which every process in one group sends data to every process of the other group. 
            <c>Alltoall</c> differs from <see cref="M:MPI.Intercommunicator.Allgather``1(``0)"/> in that a given process can send different
            data to all of the other processes, rather than contributing the same piece of data to all
            processes.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              The array of values that will be sent to each process. <paramref name="sendCounts">sendCounts</paramref>[i]
              worth of data will be sent to process i.
            </param>
            <param name="sendCounts">
              The numbers of items to be sent to each process in the other group.
            </param>
            <param name="recvCounts">
              The numbers of items to be received by each process in this group.
            </param>
            <param name="outValues">
              The array of values received from all of the other processes.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.Broadcast``1(``0@,System.Int32)">
            <summary>
            Broadcast a value from the <paramref name="root"/> process to all processes in the other group.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">
              The value to be broadcast. At the <paramref name="root"/> process, this value is
              read (but not written). At leaf group processes, this value will be replaced with
              the value at the root. At non-root processes in the root group, this value is not read or written.
            </param>
            <param name="root">
              The rank of the process that is broadcasting the value out to
              all of the processes in the non-root group.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.Broadcast``1(``0[]@,System.Int32)">
            <summary>
            Broadcast an array from the <paramref name="root"/> process to all processes in the other group.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              The array of values to be broadcast. At the <paramref name="root"/> process, this value is
              read (but not written); at processes in the leaf group, this value will be replaced with
              the value at the root; and at non-root root group processes, this value is not read or written.
              Note that the receiving processes must already have allocated
              enough space in the array to receive data from the root, prior to calling <c>Broadcast</c>.
            </param>
            <param name="root">
              The rank of the process that is broadcasting the values out to
              all of the processes in the non-root group.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.Gather``1(``0,System.Int32)">
            <summary>
              Gather the values from each process in the non-root group into an array of values at the 
              <paramref name="root"/> process. On the root process, the pth element of the result
              will be equal to the <paramref name="value"/> parameter of the process
              with rank <c>p</c> in the other group when this routine returns.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">The value contributed by this process. Only significant at non-root group processes.</param>
            <param name="root">
              Used to indicate the process gathering the data.
              At the root, should be <see cref="F:MPI.Intercommunicator.Root"/>. At leaf group processes
              should be the rank of the root process in the root group. At non-root processes in the root group,
              should be <see cref="F:MPI.Intercommunicator.Null"/>.
            </param>
            <returns>
              At the root, an array containing the <paramref name="value"/>s supplied by each of the processes
              in the non-root group. All other processes receive <c>null</c>.
            </returns>
        </member>
        <member name="M:MPI.Intercommunicator.Gather``1(System.Int32)">
            <summary>
              Gather the values from each process in the non-root group into an array of values at the 
              <paramref name="root"/> process. On the root process, the pth element of the result
              will be equal to the <paramref name="value"/> parameter of the process
              with rank <c>p</c> in the other group when this routine returns. This variant can be called only
              by the root process or by members of the leaf (non-root) group.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="root">
              Used to indicate the process gathering the data.
              At the root, should be <see cref="F:MPI.Intercommunicator.Root"/>. At leaf group processes
              should be the rank of the root process in the root group. At non-root processes in the root group,
              should be <see cref="F:MPI.Intercommunicator.Null"/>.
            </param>
            <returns></returns>
        </member>
        <member name="M:MPI.Intercommunicator.Gather``1">
            <summary>
              Gather the values from each process in the non-root group into an array of values at the 
              <paramref name="root"/> process. Use this variant only on non-root processes in
              the root group.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
        </member>
        <member name="M:MPI.Intercommunicator.Gather``1(``0,System.Int32,``0[]@)">
            <summary>
              Gather the values from each process in the non-root group into an array of values at the 
              <paramref name="root"/> process. On the root process, the pth element of the result
              will be equal to the <paramref name="value"/> parameter of the process
              with rank <c>p</c> in the other group when this routine returns. Use this variant of the routine 
              when you want to pre-allocate storage for the <paramref name="outValues"/> array.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValue">The value contributed by this process. Only significant at processes 
              in the leaf (non-root) group.
            </param>
            <param name="root">
              Used to indicate the process gathering the data.
              At the root, should be <see cref="F:MPI.Intercommunicator.Root"/>. At leaf group processes
              should be the rank of the root process in the root group. At non-root processes in the root group,
              should be <see cref="F:MPI.Intercommunicator.Null"/>.
            </param>
            <param name="outValues">
              An array that will store the values contributed by each process.
              This value is only significant at the <paramref name="root"/>, and
              can be omitted by non-root processes.
              Supply this argument when you have pre-allocated space for the resulting array.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.GatherFlattened``1(System.Int32,``0[]@)">
            <summary>
              Similar to <see cref="M:MPI.Intercommunicator.Gather``1(``0,System.Int32)"/> but here all values are aggregated into one large array.
              Use this variant at the root process.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="count">
              The number of elements contributed by each process of the leaf group.
            </param>
            <param name="outValues">
              An array in which to store the aggregated, gathered values.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.GatherFlattened``1(System.Int32)">
            <summary>
              Similar to <see cref="M:MPI.Intercommunicator.Gather``1(``0,System.Int32)"/> but here all values are aggregated into one large array.
              Use this variant at the root process.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="count">
              The number of elements contributed by each process of the leaf group.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.GatherFlattened``1(``0[],System.Int32)">
            <summary>
              Similar to <see cref="M:MPI.Intercommunicator.Gather``1(``0,System.Int32)"/> but here all values are aggregated into one large array.
              Use this variant at leaf group processes.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The values to be gathered.</param>
            <param name="root">
              Used to indicate the process gathering the data.
              At the root, should be <see cref="F:MPI.Intercommunicator.Root"/>. At leaf group processes
              should be the rank of the root process in the root group. At non-root processes in the root group,
              should be <see cref="F:MPI.Intercommunicator.Null"/>.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.GatherFlattened``1">
            <summary>
              Similar to <see cref="M:MPI.Intercommunicator.Gather``1(``0,System.Int32)"/> but here all values are aggregated into one large array.
              Use this variant at non-root processes in the root group.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
        </member>
        <member name="M:MPI.Intercommunicator.GatherFlattened``1(System.Int32[],``0[]@)">
            <summary>
              Similar to <see cref="M:MPI.Intercommunicator.Gather``1(``0,System.Int32)"/> but here all values are aggregated into one large array.
              Use this variant at the root process.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="counts">
              The number of elements to be received from each process in the leaf group.
            </param>
            <param name="outValues">
              The aggregated, gathered values. Supply this parameter if you have preallocated space for the operation.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.GatherFlattened``1(System.Int32[])">
            <summary>
              Similar to <see cref="M:MPI.Intercommunicator.Gather``1(``0,System.Int32)"/> but here all values are aggregated into one large array.
              Use this variant at the root process.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="counts">
              The number of elements to be received from each process in the leaf group.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)">
            <summary>
              <c>Reduce</c> is a collective algorithm that combines the values supplied by each 
              process in the leaf group into a 
              single value available at the designated <paramref name="root"/> process. The values are combined 
              in a user-defined way, specified via a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> 
              are the values provided by the N processes in the communicator, the result will be the value 
              <c>value1 op value2 op ... op valueN</c>. This result is only
              available to the <paramref name="root"/> process. If all processes need the result of the reduction,
              use <see cref="M:MPI.Intercommunicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="value">The local value that will be combined with the values provided by other leaf group processes.</param>
            <param name="op">
              The operation used to combine two values. This operation must be associative.
            </param>
            <param name="root">
              The rank of the process that is the root of the reduction operation, which will receive the result
              of the reduction operation in its <paramref name="outValue"/> argument.
            </param>
            <returns>
              On the root, returns the result of the reduction operation. The other processes receive a default value.
            </returns>
        </member>
        <member name="M:MPI.Intercommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32)">
            <summary>
              <c>Reduce</c> is a collective algorithm that combines the values supplied by each 
              process in the leaf group into a 
              single value available at the designated <paramref name="root"/> process. This particular variant of
              <see cref="M:MPI.Intercommunicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)"/> applies to each of the elements
              of the provided arrays. Each process must provide arrays of the same length, and the values at each
              array index are combined 
              in a user-defined way, specified via a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> 
              are the ith values provided by the N processes in the communicator, the ith result will be the value 
              <c>value1 op value2 op ... op valueN</c>. The resulting array is only
              available to the <paramref name="root"/> process. If all processes need the result of the reduction,
              use <see cref="M:MPI.Intercommunicator.Allreduce``1(``0[],MPI.ReductionOperation{``0},``0[]@)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">The local values that will be combined with the values provided by other leaf group processes.</param>
            <param name="op">
              The operation used to combine two values. This operation must be associative.
            </param>
            <param name="root">
              On leaf group processes, the rank of the process that is the root of the reduction operation, to which this routine will
              return the result of the reduction operation. On the root process, should be <see cref="F:MPI.Intercommunicator.Root"/>. On non-root processes 
              in the root group, should be <see cref="F:MPI.Intercommunicator.Null"/>.
            </param>
            <returns>
              On the root, an array that contains the result of elementwise reduction on the <paramref name="values"/>
              arrays provided by each process. On all other processes, <c>null</c>.
            </returns>
        </member>
        <member name="M:MPI.Intercommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32,``0[]@)">
            <summary>
              <c>Reduce</c> is a collective algorithm that combines the values supplied by each 
              process in the leaf group into a 
              single value available at the designated <paramref name="root"/> process. This particular variant of
              <see cref="M:MPI.Intercommunicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)"/> applies to each of the elements
              of the provided arrays. Each process must provide arrays of the same length, and the values at each
              array index are combined 
              in a user-defined way, specified via a delegate. If <c>value1</c>, <c>value2</c>, ..., <c>valueN</c> 
              are the ith values provided by the N processes in the communicator, the ith result will be the value 
              <c>value1 op value2 op ... op valueN</c>. The resulting array is only
              available to the <paramref name="root"/> process. If all processes need the result of the reduction,
              use <see cref="M:MPI.Intercommunicator.Allreduce``1(``0[],MPI.ReductionOperation{``0},``0[]@)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The local values that will be combined with the values provided by other leaf group processes.</param>
            <param name="op">
              The operation used to combine two values. This operation must be associative.
            </param>
            <param name="root">
              At leaf group processes, the rank of the process that is the root of the reduction operation, which will receive the result
              of the reduction operation in its <paramref name="outValue"/> argument.
              At the root, should be <see cref="F:MPI.Intercommunicator.Root"/>. At non-root processes in the root group,
              should be <see cref="F:MPI.Intercommunicator.Null"/>.
            </param>
            <param name="outValues">
              The variable that will receive the results of the reduction operation. Only the <paramref name="root"/>
              process will receive the actual result; all other processes will receive a default-initialized value.
              When this collective operation returns, <paramref name="outValues"/> will be of the same length as
              <paramref name="inValues"/>.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.ReduceScatter``1(``0[],MPI.ReductionOperation{``0},System.Int32[])">
            <summary>
            A collective operation that first performs a reduction on the given <paramref name="values"/> 
            (see <see cref="M:MPI.Intracommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32)"/> from one group
            and then scatters the results by sending some elements to each process of the other group. 
            The reduction will be performed on the entire array of <paramref name="values"/> (like the array form of 
            <see cref="M:MPI.Intracommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32)"/>). Then, the array will
            be scattered, with process i receiving <paramref name="counts"/>[i] elements. The process
            with rank 0 will receive the first <c>counts[0]</c> elements, the process with rank 1 will 
            receive the next <c>counts[1]</c> elements, and so on.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              An array of values that will be reduced. This array must be the same length at every process within a group.
            </param>
            <param name="op">
              The operation used to combine the elements in <paramref name="values"/>.
              This operation must be associative.
            </param>
            <param name="counts">
              An array whose ith element states the number of elements from the reduced result 
              that will be returned to the process with rank i. This array should be the same at every
              process within a group (though the count values within the array need not be the same).
              The sum of all counts for one group must be the same as the sume of all counts for the other group.
            </param>
            <returns>
              An array of length <c>counts[Rank]</c> containing the reduced results destined for
              the calling process.
            </returns>
        </member>
        <member name="M:MPI.Intercommunicator.ReduceScatter``1(``0[],MPI.ReductionOperation{``0},System.Int32[],``0[]@)">
            <summary>
            A collective operation that first performs a reduction on the given <paramref name="values"/> 
            (see <see cref="M:MPI.Intracommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32)"/> from one group
            and then scatters the results by sending some elements to each process of the other group. 
            The reduction will be performed on the entire array of <paramref name="values"/> (like the array form of 
            <see cref="M:MPI.Intracommunicator.Reduce``1(``0[],MPI.ReductionOperation{``0},System.Int32)"/>). Then, the array will
            be scattered, with process i receiving <paramref name="counts"/>[i] elements. The process
            with rank 0 will receive the first <c>counts[0]</c> elements, the process with rank 1 will 
            receive the next <c>counts[1]</c> elements, and so on.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">
              An array of values that will be reduced. This array must be the same length at every process within a group.
            </param>
            <param name="op">
              The operation used to combine the elements in <paramref name="values"/>.
              This operation must be associative.
            </param>
            <param name="counts">
              An array whose ith element states the number of elements from the reduced result 
              that will be returned to the process with rank i. This array should be the same at every
              process within a group (though the count values within the array need not be the same).
              The sum of all counts for one group must be the same as the sume of all counts for the other group.
            </param>
            <param name="outValues">
              An array of length <c>counts[Rank]</c> containing the reduced results destined for
              the calling process.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.Scatter``1(``0[])">
            <summary>
            Scatters an array of values by sending the ith value of the array to processor i of the other group. 
            This variant of <c>Scatter</c> can only be called by the root process. Other processes
            should call the non-root variant of <see cref="M:MPI.Intercommunicator.Scatter``1(System.Int32)"/>.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="values">
              An array of values of length <see cref="P:MPI.Intercommunicator.RemoteSize"/>. 
              The ith value of this array (at the root process) will be sent to the ith processor of the other group.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.Scatter``1(System.Int32)">
            <summary>
            Scatters an array of values by sending the ith value of the array to processor i. 
            This variant of <c>Scatter</c> can only be called by a leaf group process. The root process
            should call the root-only variant of <see cref="M:MPI.Intercommunicator.Scatter``1(``0[])"/>. Other 
            root group processes should call <see cref="M:MPI.Intercommunicator.Scatter``1"/>
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="root">
              Rank of the "root" process, which will supply the array of values to be scattered.
            </param>
            <returns>
              The ith value of the <paramref name="values"/> array provided by the root process, where i is
              the rank of the calling process.
            </returns>
        </member>
        <member name="M:MPI.Intercommunicator.Scatter``1">
            <summary>
            Scatters an array of values by sending the ith value of the array to processor i. 
            This variant of <c>Scatter</c> can only be called by a non-root process in the root group. 
            The root process should either call the root-only variant of <see cref="M:MPI.Intercommunicator.Scatter``1(``0[])"/>.
            Leaf group processes should call <see cref="M:MPI.Intercommunicator.Scatter``1(System.Int32)"/>
            </summary>
            <typeparam name="T"></typeparam>
        </member>
        <member name="M:MPI.Intercommunicator.ScatterFromFlattened``1(``0[],System.Int32)">
            <summary>
              Scatter a one dimensional array to all processes of the other group, 
              where multiple items are sent to each process.
              This version should be called by the root process.
              (If the number of items to be sent is different at each process, see 
              <see cref="M:MPI.Intercommunicator.ScatterFromFlattened``1(``0[],System.Int32[],System.Int32,``0[]@)"/>
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The array to be scattered. Only significant at the root.</param>
            <param name="count">
              The number of items to be received by each leaf group process. If T is a value type (primitive or
              structure) count must be the same at each process (not just at the root). If T must 
              be serialized, count is ignored at processes other than the root.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.ScatterFromFlattened``1(``0[],System.Int32[])">
            <summary>
              Scatter a one dimensional array to all processes of the other group, where the number of data items sent to 
              each process may differ. This version should be called by the root process.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The array to be scattered.</param>
            <param name="counts">
              The number of items to be received by each process of the leaf group.
            </param>
        </member>
        <member name="M:MPI.Intercommunicator.ScatterFromFlattened``1(System.Int32,System.Int32,``0[]@)">
            <summary>
              Scatter a one dimensional array to all processes of the other group, where multiple items are sent to each process.
              This version should be called by leaf group processes.
              (If the number of items to be sent is different at each process, see 
              <see cref="M:MPI.Intercommunicator.ScatterFromFlattened``1(``0[],System.Int32[],System.Int32,``0[]@)"/>
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="count">
              The number of items to be received by each process of the leaf group. If T is a value type (primitive or
              structure) count must be the same at every process . If T must 
              be serialized, count is ignored at processes other than the root.
            </param>
            <param name="root">The rank of the root process.</param>
            <param name="outValues">The array to write to at the receiving process. Does not have to be preallocated.</param>
        </member>
        <member name="M:MPI.Intercommunicator.ScatterFromFlattened``1(System.Int32[],System.Int32,``0[]@)">
            <summary>
              Scatter a one dimensional array to all processes of the other group, where the number of data items sent to 
              each process may differ. This version should be called by members of the leaf group.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="counts">
              The number of items to be received by each process. If T must 
              be serialized (i.e. is not a value type), counts is ignored at processes other than the root.
            </param>
            <param name="root">The rank of the root process.</param>
            <param name="outValues">The array to write to at the receiving process. Does not have to be preallocated.</param>
        </member>
        <member name="M:MPI.Intercommunicator.ScatterFromFlattened``1">
            <summary>
              Scatter a one dimensional array to all processes of the other group, where the number of data items sent to 
              each process may differ. This version should be called by processes in the root group,
              other than the root.
            </summary>
        </member>
        <member name="M:MPI.Intercommunicator.ScatterFromFlattened``1(``0[],System.Int32,System.Int32,``0[]@)">
            <summary>
              Scatter a one dimensional array to all processes of the other group, where multiple items are sent to each process.
              (If the number of items to be sent is different, see 
              <see cref="M:MPI.Intercommunicator.ScatterFromFlattened``1(``0[],System.Int32[],System.Int32,``0[]@)"/>
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The array to be scattered. Only significant at the root.</param>
            <param name="count">
              The number of items to be received by each process. If T is a value type (primitive or
              structure) count must be the same at every process. If T must 
              be serialized, count is ignored at processes other than the root.
            </param>
            <param name="root">
              Used to indicate the process gathering the data.
              At the root, should be <see cref="F:MPI.Intercommunicator.Root"/>. At leaf group processes
              should be the rank of the root process in the root group. At non-root processes in the root group,
              should be <see cref="F:MPI.Intercommunicator.Null"/>.
            </param>
            <param name="outValues">The array to write to at the receiving process. Does not have to be preallocated.</param>
        </member>
        <member name="M:MPI.Intercommunicator.ScatterFromFlattened``1(``0[],System.Int32[],System.Int32,``0[]@)">
            <summary>
              Scatter a one dimensional array to all processes of the other group, where the number of data items sent to 
              each process may differ.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
            <param name="inValues">The array to be scattered. Only significant at the root.</param>
            <param name="counts">
              The number of items to be received by each process. If T must 
              be serialized (i.e. is not a value type), counts is ignored at processes other than the root.
            </param>
            <param name="root">
              Used to indicate the process gathering the data.
              At the root, should be <see cref="F:MPI.Intercommunicator.Root"/>. At leaf group processes
              should be the rank of the root process in the root group. At non-root processes in the root group,
              should be <see cref="F:MPI.Intercommunicator.Null"/>.
            </param>
            <param name="outValues">The array to write to at the receiving process. Does not have to be preallocated.</param>
        </member>
        <member name="P:MPI.Intercommunicator.RemoteSize">
            <summary>
              Returns the number of processes in the remote group of this intercommunicator. 
            </summary>
        </member>
        <member name="P:MPI.Intercommunicator.RemoteGroup">
            <summary>
            Retrieve the group containing all of the remote processes in this intercommunicator.
            </summary>
        </member>
        <member name="T:MPI.Operation`1">
            <summary>
            The <c>Operation</c> class provides reduction operations for use with the
            reduction collectives.
            </summary>
            
            <remarks>
            The <c>Operation</c> class is used with the reduction collective communication operations, such as
            <see cref="M:MPI.Intracommunicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/>. For example, 
            the <see cref="P:MPI.Operation`1.Add"/> property is a delegate that adds two values of type
            <typeparamref name="T"/>, while <see cref="P:MPI.Operation`1.Min"/> returns the minimum of the
            two values. The reduction operations provided by this class should be 
            preferred to hand-written reduction operations (particularly for built-in types)
            because it enables additional optimizations in the MPI library.
            
            <para>The <c>Operation</c> class also has a second role for users that require
            access to the low-level MPI interface. Creating an instance of the <c>Operation</c>
            class will find or create an appropriate <c>MPI_Op</c> for that reduction
            operation. This <c>MPI_Op</c>, accessible through the <see cref="P:MPI.Operation`1.Op"/> property,
            can be used with low-level MPI reduction operations directly.</para>
            </remarks>
            <typeparam name="T">The type of data used in these operations.</typeparam>
        </member>
        <member name="M:MPI.Operation`1.BuildMethod(System.String,System.Reflection.Emit.OpCode,System.String)">
            <summary>
            Synthesize a new mpiDelegateMethod associated with a given operation.
            </summary>
            <param name="name">The name of the mpiDelegateMethod to build, e.g., "add".</param>
            <param name="opcode">The <c>OpCode</c> used for primitive types.</param>
            <param name="methodName">The name of the overloaded mpiDelegateMethod used for class types.</param>
            <returns>Returns a new delegate implementing the given operation.</returns>
        </member>
        <member name="F:MPI.Operation`1.min">
            <summary>
            Cached intermediate of the reduction operation that computes the minimum of 
            two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.max">
            <summary>
            Cached intermediate of the reduction operation that computes the maximum of 
            two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.add">
            <summary>
            Cached intermediate of the reduction operation that adds two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.multiply">
            <summary>
            Cached intermediate of the reduction operation that multiplies two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.logicalAnd">
            <summary>
            Cached intermediate of the reduction operation that computes the logical
            AND of two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.logicalOr">
            <summary>
            Cached intermediate of the reduction operation that computes the logical
            OR of two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.bitwiseAnd">
            <summary>
            Cached intermediate of the reduction operation that computes the bitwise
            AND of two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.bitwiseOr">
            <summary>
            Cached intermediate of the reduction operation that computes the bitwise
            OR of two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.exclusiveOr">
            <summary>
            Cached intermediate of the reduction operation that computes the bitwise
            exclusive OR of two values.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.Kind">
            <summary>
            The MPI datatype classification of the type <typeparamref name="T"/>.
            </summary>
        </member>
        <member name="M:MPI.Operation`1.GetDatatypeKind">
            <summary>
              Retrieves the kind of datatype for the type <typeparamref name="T"/>. 
              Used only to initialize <see cref="F:MPI.Operation`1.Kind"/>.
            </summary>
        </member>
        <member name="M:MPI.Operation`1.GetPredefinedOperation(MPI.ReductionOperation{`0})">
            <summary>
            Determine the predefined <c>MPI_Op</c> that is associated with
            this reduction operation. If no such <c>MPI_Op</c> exists, 
            returns <see cref="F:MPI.Unsafe.MPI_OP_NULL"/>.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.UseGeneratedUserOps">
            <summary>
            When true, we will use user-defined MPI operations generated on-the-fly for
            reduction operations on value types. Otherwise, we will use the more generic,
            static MPI operations.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.mpiDelegateMethod">
            <summary>
            The dynamically-generated method that provides a delegate with a
            signature compatible with <c>MPI_User_function</c> that calls
            the user's <see cref="T:MPI.ReductionOperation`1"/> repeatedly.
            </summary>
        </member>
        <member name="M:MPI.Operation`1.MakeMPIDelegate(MPI.ReductionOperation{`0})">
            <summary>
            Creates a new MPI delegate from a reduction operation.
            </summary>
        </member>
        <member name="M:MPI.Operation`1.#ctor(MPI.ReductionOperation{`0})">
            <summary>
            Create a user-defined MPI operation based on the given reduction operation.
            </summary>
            <param name="op">The reduction operation.</param>
        </member>
        <member name="M:MPI.Operation`1.Dispose">
            <summary>
            Free the MPI operation that this object wraps, but only if it is not a 
            predefined MPI operation.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.wrapper">
            <summary>
            The wrapper around the user's reduction operation.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.mpiOp">
            <summary>
            The actual <c>MPI_Op</c> corresponding to this operation.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.Min">
            <summary>
            Reduction operation that computes the minimum of two values.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.Max">
            <summary>
            Reduction operation that computes the maximum of two values.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.Add">
            <summary>
            Reduction operation that adds two values.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.Multiply">
            <summary>
            Reduction operation that multiply two values.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.LogicalAnd">
            <summary>
            Reduction operation that computes the logical AND of two values,
            including integral types.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.LogicalOr">
            <summary>
            Reduction operation that computes the logical OR of two values,
            including integral types.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.BitwiseAnd">
            <summary>
            Reduction operation that computes the bitwise AND of two values.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.BitwiseOr">
            <summary>
            Reduction operation that computes the bitwise OR of two values.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.ExclusiveOr">
            <summary>
            Reduction operation that computes the bitwise exclusive OR of two values.
            </summary>
        </member>
        <member name="P:MPI.Operation`1.Op">
            <summary>
            The MPI operation that can corresponds to the user's reduction operation,
            This operation may be either a predefined MPI reduction operation or a 
            user-defined <c>MPI_Op</c> created by the <c>Operation</c> constructor.
            </summary>
        </member>
        <member name="T:MPI.Operation`1.DatatypeKind">
            <summary>
            The kind of MPI datatype. MPI classifies the predefined data types into 
            several different categories. This classification is used primarily to determine 
            which predefined reduction operations are supported by the low-level MPI
            implementation.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.DatatypeKind.CInteger">
            <summary>
            C integer types, such as the predefined integral types in C# and .NET.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.DatatypeKind.FortranInteger">
            <summary>
            Fortran integer types. At present, there are no C# or .NET types that 
            are classified this way.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.DatatypeKind.FloatingPoint">
            <summary>
            Floating point types, such as the predefined <c>float</c> and <c>double</c> types.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.DatatypeKind.Logical">
            <summary>
            The MPI logical type. At present, there are no C# or .NET types that 
            are classified this way.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.DatatypeKind.Complex">
            <summary>
            The MPI complex type. At present, there are no C# or .NET types that 
            are classified this way.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.DatatypeKind.Byte">
            <summary>
            The MPI byte type, which corresponds to the <c>byte</c> type in C#.
            </summary>
        </member>
        <member name="F:MPI.Operation`1.DatatypeKind.Other">
            <summary>
            Any type that does not fit into one of the other MPI datatype classifications.
            </summary>
        </member>
        <member name="T:MPI.Operation`1.WrapReductionOperation">
            <summary>
            Wraps a <see cref="T:MPI.ReductionOperation`1"/> to provide an MPI-compatible interface
            for use with <see cref="M:MPI.Unsafe.MPI_Op_create(System.IntPtr,System.Int32,System.Int32@)"/>.
            </summary>
        </member>
        <member name="M:MPI.Operation`1.WrapReductionOperation.#ctor(MPI.ReductionOperation{`0})">
            <summary>
            Construct a new reduction operation wrapper.
            </summary>
        </member>
        <member name="M:MPI.Operation`1.WrapReductionOperation.Apply(System.Void*,System.Void*,System.Int32*,System.Int32*)">
            <summary>
            Applies a reduction operation to each of the values in <paramref name="invec"/> and 
            <paramref name="inoutvec"/>, writing the results back into the corresponding
            position in <paramref name="inoutvec"/>.
            </summary>
            <param name="invec">Incoming values of type <c>T</c>.</param>
            <param name="inoutvec">Incoming values of type <c>T</c>. The results of the reduction
            operation will be written back to this memory.</param>
            <param name="len">
              The length of the <paramref name="invec"/> and <paramref name="inoutvec"/> arrays.
            </param>
            <param name="datatype">
              The MPI datatype for the data stored in <paramref name="invec"/> and <paramref name="outvec"/>.
              This should be the same as the intermediate of <see cref="M:MPI.DatatypeCache.GetDatatype(System.Type)"/> applied to the
              type <c>T</c>.
            </param>
        </member>
        <member name="T:MPI.Request">
            <summary>
            A non-blocking communication request.
            </summary>
            <remarks>
            Each request object refers to a single
            communication operation, such as non-blocking send 
            (see <see cref="M:MPI.Communicator.ImmediateSend``1(``0,System.Int32,System.Int32)"/>)
            or receive. Non-blocking operations may progress in the background, and can complete
            without any user intervention. However, it is crucial that outstanding communication
            requests be completed with a successful call to <see cref="M:MPI.Request.Wait"/> or <see cref="M:MPI.Request.Test"/>
            before the request object is lost.
            </remarks>
        </member>
        <member name="M:MPI.Request.Wait">
            <summary>
            Wait until this non-blocking operation has completed.
            </summary>
            <returns>
              Information about the completed communication operation.
            </returns>
        </member>
        <member name="M:MPI.Request.Test">
            <summary>
            Determine whether this non-blocking operation has completed.
            </summary>
            <returns>
            If the non-blocking operation has completed, returns information
            about the completed communication operation. Otherwise, returns
            <c>null</c> to indicate that the operation has not completed.
            </returns>
        </member>
        <member name="M:MPI.Request.Cancel">
            <summary>
            Cancel this communication request.
            </summary>
        </member>
        <member name="T:MPI.ReceiveRequest">
            <summary>
            A non-blocking receive request. 
            </summary>
            <remarks>
            This class allows one to test a receive
            request for completion, wait for completion of a request, cancel a request,
            or extract the value received by this communication request.
            </remarks>
        </member>
        <member name="M:MPI.ReceiveRequest.GetValue">
            <summary>
            Retrieve the value received via this communication. The value
            will only be available when the communication has completed.
            </summary>
            <returns>The value received by this communication.</returns>
        </member>
        <member name="T:MPI.ValueTypeSendRequest">
            <summary>
            A request that corresponds to a non-blocking send of either a 
            single value or an array of values, where the type of the value
            (or values) is a value type with an associated MPI datatype.
            Underneath, this request is a single <c>MPI_Request</c> object
            coupled with a handle to pinned memory and some additional 
            information about the send itself.
            </summary>
        </member>
        <member name="M:MPI.ValueTypeSendRequest.#ctor(System.Int32,System.Int32,System.Runtime.InteropServices.GCHandle)">
            <summary>
            Initialize a single request for a non-blocking send that has already been initiated.
            </summary>
            <param name="request">The request object.</param>
            <param name="count">The number of elements transmitted in this communication.</param>
            <param name="handle">A handle pointing to pinned memory that will be unpinned after
            this operation completes.</param>
        </member>
        <member name="M:MPI.ValueTypeSendRequest.Cleanup">
            <summary>
            Cleanup any resources that we're currently holding. This typically
            involves unpinning the memory associated with this request.
            </summary>
        </member>
        <member name="F:MPI.ValueTypeSendRequest.cachedStatus">
            <summary>
            If we've already completed this operation, this caches the status information
            so we can return it again later.
            </summary>
        </member>
        <member name="F:MPI.ValueTypeSendRequest.request">
            <summary>
            The underlying MPI request object that provides a handle to this
            non-blocking send.
            </summary>
        </member>
        <member name="F:MPI.ValueTypeSendRequest.count">
            <summary>
            The number of elements sent by this operation. 
            </summary>
        </member>
        <member name="F:MPI.ValueTypeSendRequest.handle">
            <summary>
            Handle to the pinned memory used in this request.
            </summary>
        </member>
        <member name="T:MPI.TwoRequests">
            <summary>
            A structure storing two requests. Use this to keep the two requests in memory
            next to each other, so that we can complete or test them with a single MPI call.
            </summary>
        </member>
        <member name="T:MPI.SerializedSendRequest">
            <summary>
            A request for a serialized send, which typically consists of two <c>MPI_Request</c> objects.
            This class manages pinning and unpinning the data for both send requests, and testing/completing
            both requests in the same operation.
            </summary>
        </member>
        <member name="M:MPI.SerializedSendRequest.#ctor(MPI.Communicator,System.Int32,System.Int32,MPI.UnmanagedMemoryStream,System.Int32)">
            <summary>
            Initiate an MPI non-blocking send operation managed by this Request object.
            </summary>
            <param name="comm">The communicator over which the initial message will be sent.</param>
            <param name="dest">The destination rank for this message.</param>
            <param name="tag">The message tag.</param>
            <param name="stream">The stream of bytes that should be transmitted.</param>
            <param name="count">The number of serialized objects stored in <paramref name="buffer"/>.</param>
        </member>
        <member name="M:MPI.SerializedSendRequest.Cleanup">
            <summary>
            Clean up whatever resources we are holding.
            </summary>
        </member>
        <member name="F:MPI.SerializedSendRequest.cachedStatus">
            <summary>
            If we've already completed this operation, this caches the status information
            so we can return it again later.
            </summary>
        </member>
        <member name="F:MPI.SerializedSendRequest.count">
            <summary>
            The number of elements transmitted by this operation. 
            </summary>
        </member>
        <member name="F:MPI.SerializedSendRequest.headerObj">
            <summary>
            Header sent in the first message corresponding to the serialized send.
            </summary>
        </member>
        <member name="F:MPI.SerializedSendRequest.tagAllocator">
            <summary>
            Tag allocator user by the communicator to temporarily retrieve tags
            for sending serialized data.
            </summary>
        </member>
        <member name="F:MPI.SerializedSendRequest.headerHandle">
            <summary>
            Handle that pins down the header object, so that it does not move.
            </summary>
        </member>
        <member name="F:MPI.SerializedSendRequest.stream">
            <summary>
            Stream containing the serialized representation of the data to send
            in unmanaged memory (that map be owned by MPI).
            </summary>
        </member>
        <member name="F:MPI.SerializedSendRequest.requests">
            <summary>
            The two outstanding MPI requests. The first request contains the send request
            for the buffer, while the second request contains the send for the length
            (which may be <see cref="F:MPI.Unsafe.MPI_REQUEST_NULL"/>).
            </summary>
        </member>
        <member name="T:MPI.ValueReceiveRequest`1">
            <summary>
            A non-blocking receive request for a single value of value type. This
            request contains only a single <c>MPI_Request</c> object, which will
            receive into its own <c>value</c> member.
            </summary>
        </member>
        <member name="M:MPI.ValueReceiveRequest`1.Cleanup">
            <summary>
            Cleanup any resources we're still holding on to.
            </summary>
        </member>
        <member name="F:MPI.ValueReceiveRequest`1.cachedStatus">
            <summary>
            If we've already completed this operation, this caches the status information
            so we can return it again later.
            </summary>
        </member>
        <member name="F:MPI.ValueReceiveRequest`1.handle">
            <summary>
            Handle to <c>this</c>, which will be pinned so that <c>value</c>'s
            address will remain the same.
            </summary>
        </member>
        <member name="F:MPI.ValueReceiveRequest`1.request">
            <summary>
            The MPI request associated with the non-blocking receive.
            </summary>
        </member>
        <member name="F:MPI.ValueReceiveRequest`1.value">
            <summary>
            The actual value we'll be receiving
            </summary>
        </member>
        <member name="T:MPI.ValueArrayReceiveRequest`1">
            <summary>
            A non-blocking receive request for an array of values of value type. This
            request contains only a single <c>MPI_Request</c> object, which will
            receive into the given array.
            </summary>
        </member>
        <member name="M:MPI.ValueArrayReceiveRequest`1.Cleanup">
            <summary>
            Cleanup any resources we're still holding on to.
            </summary>
        </member>
        <member name="F:MPI.ValueArrayReceiveRequest`1.cachedStatus">
            <summary>
            If we've already completed this operation, this caches the status information
            so we can return it again later.
            </summary>
        </member>
        <member name="F:MPI.ValueArrayReceiveRequest`1.handle">
            <summary>
            Handle to <c>this</c>, which will be pinned so that <c>value</c>'s
            address will remain the same.
            </summary>
        </member>
        <member name="F:MPI.ValueArrayReceiveRequest`1.request">
            <summary>
            The MPI request associated with the non-blocking receive.
            </summary>
        </member>
        <member name="F:MPI.ValueArrayReceiveRequest`1.array">
            <summary>
            The actual array we'll be receiving into.
            </summary>
        </member>
        <member name="T:MPI.SerializedReceiveRequest`1">
            <summary>
            A request for a serialized receive, which uses a two-stage receive process. The first stage
            receives the size of the serialized data. The second stage receives the actual data and, 
            upon completion, deserializes that data.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
        </member>
        <member name="M:MPI.SerializedReceiveRequest`1.SetupSerializedReceive(MPI.Unsafe.MPI_Status)">
            <summary>
            Given the header data, this routine will set up the receive of the
            serialized data on the shadow communicator.
            </summary>
            <param name="status">
              The status message returned from the completion of the first
              receive (of the header data).
            </param>
        </member>
        <member name="F:MPI.SerializedReceiveRequest`1.cachedStatus">
            <summary>
            If we've already completed this operation, this caches the status information
            so we can return it again later.
            </summary>
        </member>
        <member name="F:MPI.SerializedReceiveRequest`1.originalTag">
            <summary>
            The tag value used by the original received message. We store this value when we
            receive the header rather that storing the complete status object, since the
            status object from the second receive is just as good.
            </summary>
        </member>
        <member name="F:MPI.SerializedReceiveRequest`1.handle">
            <summary>
            Handle that pins down either the header object (when receiving the header)
            or the stream's buffer (when receiving the serialized data).
            </summary>
        </member>
        <member name="F:MPI.SerializedReceiveRequest`1.request">
            <summary>
            Request object that corresponds to the currently-active request.
            </summary>
        </member>
        <member name="F:MPI.SerializedReceiveRequest`1.shadowComm">
            <summary>
            "Shadow" communicator through which we will receive the serialized data.
            </summary>
        </member>
        <member name="F:MPI.SerializedReceiveRequest`1.stream">
            <summary>
            Stream that will receive the serialized data.
            </summary>
        </member>
        <member name="F:MPI.SerializedReceiveRequest`1.headerObj">
            <summary>
            Message header to be received in the first stage.
            </summary>
        </member>
        <member name="F:MPI.SerializedReceiveRequest`1.value">
            <summary>
            The value we are receiving.
            </summary>
        </member>
        <member name="T:MPI.SerializedArrayReceiveRequest`1">
            <summary>
            A request for a serialized receive of an array of values. The communication behavior is 
            identical to (and implemented as) <see cref="T:MPI.SerializedReceiveRequest`1"/>, but
            here we need to copy the values from the de-serialized array into the array provided 
            by the user.
            </summary>
            <typeparam name="T">Any serializable type.</typeparam>
        </member>
        <member name="M:MPI.SerializedArrayReceiveRequest`1.CopyResults(MPI.CompletedStatus)">
            <summary>
            Copies the results from the received array into the user-provided array
            and returns a new status object with the appropriate count.
            </summary>
        </member>
        <member name="F:MPI.SerializedArrayReceiveRequest`1.request">
            <summary>
            The request that handles receipt of the actual array, since we cannot
            de-serialize an array into an existing array.
            </summary>
        </member>
        <member name="F:MPI.SerializedArrayReceiveRequest`1.array">
            <summary>
            The user-provided array, where values will be copied once the receive
            has completed.
            </summary>
        </member>
        <member name="F:MPI.SerializedArrayReceiveRequest`1.cachedStatus">
            <summary>
            Cached copy of the status object returned from this request. When non-null,
            this communication has already completed, so we just return this object.
            </summary>
        </member>
        <member name="T:MPI.RequestList">
            <summary>
            A request list contains a list of outstanding MPI requests. 
            </summary>
            
            <remarks>
            The requests in a <c>RequestList</c>
            are typically non-blocking send or receive operations (e.g.,
            <see cref="M:MPI.Communicator.ImmediateSend``1(``0,System.Int32,System.Int32)"/>,
            <see cref="M:MPI.Communicator.ImmediateReceive``1(System.Int32,System.Int32)"/>). The
            request list provides the ability to operate on the set of MPI requests
            as a whole, for example by waiting until all requests complete before
            returning or testing whether any of the requests have completed.
            </remarks>
        </member>
        <member name="M:MPI.RequestList.#ctor">
            <summary>
            Create a new, empty request list.
            </summary>
        </member>
        <member name="M:MPI.RequestList.Add(MPI.Request)">
            <summary>
            Add a new request to the request list.
            </summary>
            <param name="request">The request to add.</param>
        </member>
        <member name="M:MPI.RequestList.Remove(MPI.Request)">
            <summary>
            Remove a request from the request list.
            </summary>
            <param name="request">Request to remove.</param>
        </member>
        <member name="M:MPI.RequestList.WaitAny">
            <summary>
            Waits until any request has completed. That request will then be removed 
            from the request list and returned.
            </summary>
            <returns>The completed request, which has been removed from the request list.</returns>
        </member>
        <member name="M:MPI.RequestList.TestAny">
            <summary>
            Determines whether any request has completed. If so, that request will be removed
            from the request list and returned. 
            </summary>
            <returns>
              The first request that has completed, if any. Otherwise, returns <c>null</c> to
              indicate that no request has completed.
            </returns>
        </member>
        <member name="M:MPI.RequestList.WaitAll">
            <summary>
            Wait until all of the requests has completed before returning.
            </summary>
            <returns>A list containing all of the completed requests.</returns>
        </member>
        <member name="M:MPI.RequestList.TestAll">
            <summary>
            Test whether all of the requests have completed. If all of the
            requests have completed, the result is the list of requests. 
            Otherwise, the result is <c>null</c>.
            </summary>
            <returns>Either the list of all completed requests, or null.</returns>
        </member>
        <member name="M:MPI.RequestList.WaitSome">
            <summary>
            Wait for at least one request to complete, then return a list of
            all of the requests that have completed at this point.
            </summary>
            <returns>
              A list of all of the requests that have completed, which
              will contain at least one element.
            </returns>
        </member>
        <member name="M:MPI.RequestList.TestSome">
            <summary>
            Return a list of all requests that have completed.
            </summary>
            <returns>
              A list of all of the requests that have completed. If
              no requests have completed, returns <c>null</c>.
            </returns>
        </member>
        <member name="F:MPI.RequestList.requests">
            <summary>
            The actual list of requests.
            </summary>
        </member>
        <member name="P:MPI.RequestList.Count">
            <summary>
            Retrieves the number of elements in this list of requests.
            </summary>
        </member>
        <member name="T:MPI.Status">
            <summary>
            Contains information about a specific message transmitted via MPI.
            </summary>
        </member>
        <member name="M:MPI.Status.#ctor(MPI.Unsafe.MPI_Status)">
            <summary>
              Constructs a <code>Status</code> object from a low-level <see cref="T:MPI.Unsafe.MPI_Status"/> structure.
            </summary>
        </member>
        <member name="M:MPI.Status.Count(System.Type)">
            <summary>
            Determine the number of elements transmitted by the communication
            operation associated with this object.
            </summary>
            <param name="type">
              The type of data that will be stored in the message.
            </param>
            <returns>
              If the type of the data is a value type, returns the number
              of elements in the message. Otherwise, returns <c>null</c>,
              because the number of elements stored in the message won't
              be known until the message is received.
            </returns>
        </member>
        <member name="F:MPI.Status.status">
            <summary>
             The low-level MPI status object.
            </summary>
        </member>
        <member name="P:MPI.Status.Source">
            <summary>
            The rank of the process that sent the message.
            </summary>
        </member>
        <member name="P:MPI.Status.Tag">
            <summary>
            The tag used to send the message.
            </summary>
        </member>
        <member name="P:MPI.Status.Cancelled">
            <summary>
            Whether the communication was cancelled before it completed.
            </summary>
        </member>
        <member name="T:MPI.CompletedStatus">
            <summary>
            Information about a specific message that has already been
            transferred via MPI.
            </summary>
        </member>
        <member name="M:MPI.CompletedStatus.#ctor(MPI.Unsafe.MPI_Status,System.Int32)">
            <summary>
              Constructs a <code>Status</code> object from a low-level <see cref="T:MPI.Unsafe.MPI_Status"/> structure
              and a count of the number of elements received.
            </summary>
        </member>
        <member name="M:MPI.CompletedStatus.Count">
            <summary>
            Determines the number of elements in the transmitted message.
            </summary>
        </member>
        <member name="F:MPI.CompletedStatus.count">
            <summary>
            The number of elements in the message.
            </summary>
        </member>
        <member name="T:MPI.TagAllocator">
            <summary>
            Allocates unique tags for the various point-to-point communications that are used within 
            the MPI.NET implementation but must remain hidden from the user. All of the routines
            in the <c>TagAllocator</c> class are thread-safe when MPI is running in a mode that supports
            multi-threading.
            </summary>
        </member>
        <member name="M:MPI.TagAllocator.#ctor">
            <summary>
            Initialize a new tag allocator
            </summary>
        </member>
        <member name="M:MPI.TagAllocator.AllocateSendTag">
            <summary>
            Allocate a new, unique tag for a single serialized send. The tag
            must be returned via <see cref="M:MPI.TagAllocator.ReturnSendTag(System.Int32)"/>.
            </summary>
            <returns>A new, unique tag for use with this serialized send.</returns>
        </member>
        <member name="M:MPI.TagAllocator.AllocateSendTagImpl">
            <summary>
            Actual implementation of <see cref="M:MPI.TagAllocator.AllocateSendTag"/>.
            </summary>
        </member>
        <member name="M:MPI.TagAllocator.ReturnSendTag(System.Int32)">
            <summary>
            Returns a tag allocated via <see cref="M:MPI.TagAllocator.AllocateSendTag"/>.
            </summary>
            <param name="tag">The tag to return.</param>
        </member>
        <member name="M:MPI.TagAllocator.ReturnSendTagImpl(System.Int32)">
            <summary>
            Implementation of <see cref="M:MPI.TagAllocator.ReturnSendTag(System.Int32)"/>.
            </summary>
            <param name="tag">Tag value being returned.</param>
        </member>
        <member name="F:MPI.TagAllocator.nextSendTag">
            <summary>
            The next unique tag that will be given out for a serialized send. This
            value starts at zero and increases as more tags are requested.
            </summary>
        </member>
        <member name="F:MPI.TagAllocator.returnedSendTags">
            <summary>
            Stack of send tags that have been returned out-of-order. We always
            exhaust these tags first, before allocating a new send tag by increasing
            <see cref="F:MPI.TagAllocator.nextSendTag"/>.
            </summary>
        </member>
        <member name="T:MPI.UnmanagedMemoryStream">
            <summary>
            A stream object that is backed by unmanaged memory. The behavior of <c>UnmanagedMemoryStream</c> is similar to 
            that of the .NET <see cref="T:System.IO.MemoryStream"/>, because it manages memory internally and will re-allocate 
            its internal buffers as necessary to accommodate additional writes (the latter is not supported by 
            <see cref="T:System.IO.UnmanagedMemoryStream"/>). The memory used by this class is allocated either via MPI's
            memory allocation routines (the default) or the <see cref="T:System.Runtime.InteropServices.Marshal"/> class's 
            unmanaged memory allocation routines.
            </summary>
        </member>
        <member name="M:MPI.UnmanagedMemoryStream.#ctor">
            <summary>
            Create a new, empty unmanaged memory stream. The memory stream can be resized by 
            setting <see cref="P:MPI.UnmanagedMemoryStream.Length"/>, calling <see cref="M:MPI.UnmanagedMemoryStream.SetLength(System.Int64)"/>, or writing to
            the stream.
            </summary>
        </member>
        <member name="M:MPI.UnmanagedMemoryStream.#ctor(System.Boolean)">
            <summary>
            Create a new, empty unmanaged memory stream. The memory stream can be resized by 
            setting <see cref="P:MPI.UnmanagedMemoryStream.Length"/>, calling <see cref="M:MPI.UnmanagedMemoryStream.SetLength(System.Int64)"/>, or writing to
            the stream.
            </summary>
            <param name="defaultMemoryFromMPI">
              Whether this stream should first try to allocate its memory from MPI. If
              MPI cannot provide memory, or if this parameter is <c>false</c>, memory
              will be allocated from the unmanaged heap.
            </param>
        </member>
        <member name="M:MPI.UnmanagedMemoryStream.#ctor(System.Int64)">
            <summary>
            Create a new unmanaged memory stream with storage for a certain number of bytes of
            data. The length of the stream will be set to <paramref name="bytes"/>, but the 
            contents of the stream after initialization are undefined.
            </summary>
            <param name="bytes">The number of bytes in the new stream.</param>
        </member>
        <member name="M:MPI.UnmanagedMemoryStream.#ctor(System.Int64,System.Boolean)">
            <summary>
            Create a new unmanaged memory stream with storage for a certain number of bytes of
            data. The length of the stream will be set to <paramref name="bytes"/>, but the 
            contents of the stream after initialization are undefined.
            </summary>
            <param name="bytes">The number of bytes in the new stream.</param>
            <param name="defaultMemoryFromMPI">
              Whether this stream should first try to allocate its memory from MPI. If
              MPI cannot provide memory, or if this parameter is <c>false</c>, memory
              will be allocated from the unmanaged heap.
            </param>
        </member>
        <member name="M:MPI.UnmanagedMemoryStream.Flush">
            <summary>
            "Flush" the contents of the stream. This operation does nothing.
            </summary>
        </member>
        <member name="M:MPI.UnmanagedMemoryStream.Read(System.Byte[],System.Int32,System.Int32)">
            <summary>
            Read <paramref name="count"/> bytes from the current position in the stream, and place
            them into the <paramref name="buffer"/> starting at the given <paramref name="offset"/>.
            The position will be updated to the point after the last byte read.
            </summary>
            <param name="buffer">Array that will receive the data read from the stream.</param>
            <param name="offset">The position in the buffer where the first byte will be written.</param>
            <param name="count">The maximum number of bytes to copy into the buffer.</param>
            <returns>The actual number of bytes read into the buffer.</returns>
        </member>
        <member name="M:MPI.UnmanagedMemoryStream.Seek(System.Int64,System.IO.SeekOrigin)">
            <summary>
            Seek to a specific position in the stream.
            </summary>
            <param name="offset">Offset (in bytes) from the <paramref name="origin"/>.</param>
            <param name="origin">Where to start seeking from.</param>
            <returns>The new position in the stream.</returns>
        </member>
        <member name="M:MPI.UnmanagedMemoryStream.SetLength(System.Int64)">
            <summary>
            Set the length of the stream. If the new length of the stream 
            is larger than the old length, the contents of the stream from
            the old length to the new length are undefined.
            </summary>
            <param name="value">The new length.</param>
        </member>
        <member name="M:MPI.UnmanagedMemoryStream.Write(System.Byte[],System.Int32,System.Int32)">
            <summary>
            Write data into unmanaged memory. If the write would continue past the
            end of the memory stream, the memory stream is expanded.
            </summary>
            <param name="buffer">The buffer containing the data to write.</param>
            <param name="offset">The position in the buffer from which data should be read.</param>
            <param name="count">The number of bytes to write to the stream.</param>
        </member>
        <member name="M:MPI.UnmanagedMemoryStream.Reserve(System.Int64)">
            <summary>
            Reserve a certain amount of space in the buffer, to minimize the need for
            reallocations if you already know how much space will be needed.
            </summary>
            <param name="value">The number of bytes to reserve.</param>
        </member>
        <member name="M:MPI.UnmanagedMemoryStream.Dispose(System.Boolean)">
            <summary>
            Deallocate any memory allocated by this stream and
            dispose of the object.
            </summary>
            <param name="disposing">
              Whether this call was the result of calling dispose (<c>true</c>)
              or occurred in response to a finalizer (<c>false</c>).
            </param>
        </member>
        <member name="M:MPI.UnmanagedMemoryStream.Finalize">
            <summary>
            Releases the memory associated with the stream, if any.
            </summary>
        </member>
        <member name="M:MPI.UnmanagedMemoryStream.AllocateMemory(System.Int64,System.Boolean@)">
            <summary>
            Allocates unmanaged memory. The memory will either return memory allocated from
            MPI or from the unmanaged heap, depending on <paramref name="fromMPI"/>. Memory
            allocated in this way should be freed via <see cref="M:MPI.UnmanagedMemoryStream.FreeMemory(System.IntPtr,System.Boolean)"/>.
            </summary>
            <param name="bytes">The number of bytes to allocate.</param>
            <param name="fromMPI">
              If true, this routine will first try to allocate the memory
              via MPI's memory allocation routines (e.g. <see cref="M:MPI.Unsafe.MPI_Alloc_mem(System.IntPtr,System.Int32,System.IntPtr@)"/>),
              then will fall back to C#'s unmanaged memory allocation routines.
              This value will be updated to reflect where the memory actually came from.
            </param>
            <returns>A pointer to the newly-allocated memory.</returns>
        </member>
        <member name="M:MPI.UnmanagedMemoryStream.FreeMemory(System.IntPtr,System.Boolean)">
            <summary>
            Frees memory allocated via <see cref="M:MPI.UnmanagedMemoryStream.AllocateMemory(System.Int64,System.Boolean@)"/>.
            </summary>
            <param name="ptr">The pointer returned from <see cref="M:MPI.UnmanagedMemoryStream.AllocateMemory(System.Int64,System.Boolean@)"/>.</param>
            <param name="fromMPI">Whether this memory came from MPI or from the unmaanged heap.</param>
        </member>
        <member name="F:MPI.UnmanagedMemoryStream.DefaultMemoryFromMPI">
            <summary>
            Whether memory comes from the MPI allocation routines by default.
            </summary>
        </member>
        <member name="F:MPI.UnmanagedMemoryStream.storage">
            <summary>
            A pointer to unmanaged storage.
            </summary>
        </member>
        <member name="F:MPI.UnmanagedMemoryStream.storageFromMPI">
            <summary>
            True when the <see cref="F:MPI.UnmanagedMemoryStream.storage"/> is storage obtained from MPI
            via <see cref="M:MPI.Unsafe.MPI_Alloc_mem(System.IntPtr,System.Int32,System.IntPtr@)"/>.
            </summary>
        </member>
        <member name="F:MPI.UnmanagedMemoryStream.capacity">
            <summary>
            The number of bytes that <see cref="F:MPI.UnmanagedMemoryStream.storage"/> refers to.
            </summary>
        </member>
        <member name="F:MPI.UnmanagedMemoryStream.count">
            <summary>
            The number of bytes in the buffer.
            </summary>
        </member>
        <member name="F:MPI.UnmanagedMemoryStream.position">
            <summary>
            Current position within the stream.
            </summary>
        </member>
        <member name="P:MPI.UnmanagedMemoryStream.CanRead">
            <summary>
            Whether the stream can be read from. Always <c>true</c>.
            </summary>
        </member>
        <member name="P:MPI.UnmanagedMemoryStream.CanSeek">
            <summary>
            Whether one can seek in the stream. Always <c>true</c>.
            </summary>
        </member>
        <member name="P:MPI.UnmanagedMemoryStream.CanWrite">
            <summary>
            Whether on can write to the stream. Always <c>true</c>.
            </summary>
        </member>
        <member name="P:MPI.UnmanagedMemoryStream.Length">
            <summary>
            The length of the stream, in bytes.
            </summary>
        </member>
        <member name="P:MPI.UnmanagedMemoryStream.Position">
            <summary>
            Reports or sets the position in the stream. 
            </summary>
        </member>
        <member name="P:MPI.UnmanagedMemoryStream.Buffer">
            <summary>
            Retrieve a pointer to the unmanaged memory buffer. Since this
            buffer is pointing into unmanaged memory, it does not need to be
            pinned.
            </summary>
        </member>
        <member name="P:MPI.UnmanagedMemoryStream.Capacity">
            <summary>
            The amount of space in the unmanaged memory buffer. This can be
            larger than the length of the stream. If you know how many bytes
            will be written to the stream, you might want to set the capacity
            (either via this property or through <see cref="M:MPI.UnmanagedMemoryStream.Reserve(System.Int64)"/>) large
            enough to avoid resizing the stream multiple times.
            </summary>
        </member>
        <member name="T:MPI.Unsafe">
            <summary>
            Direct, low-level interface to the system MPI library.
            </summary>
            
            <remarks>
            This low-level interface provides direct access to the unmanaged
            MPI library provided by the system. It is by nature unsafe, and
            should only be used by programmers experienced both in the use 
            of MPI from lower-level languages (e.g., C, Fortran) and with an
            understanding of the interaction between managed and unmanaged
            code, especially those issues that pertain to memory 
            pinning/unpinning.
            
            <para>A second use of the Unsafe class is purely for documentation reasons. Each member of the 
            Unsafe class corresponds to an entity in the standard, C MPI, and the documentation of each member
            of Unsafe will point to the corresponding functionality within MPI.NET. Programmers already familiar
            with the Message Passing Interface in C can use the Unsafe class as a reference to better understand
            how to apply their knowledge of C MPI to MPI.NET programs.</para>
            </remarks>
        </member>
        <member name="F:MPI.Unsafe.MPI_COMM_WORLD">
            <summary>
              Predefined communicator containing all of the MPI processes. See <see cref="F:MPI.Communicator.world"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_COMM_SELF">
            <summary>
              Predefined communicator containing only the calling process. See <see cref="F:MPI.Communicator.self"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_COMM_NULL">
            <summary>
              Predefined communicator representing "no communicator". In the higher-level interface, this
              is represented by a <c>null</c> <see cref="T:MPI.Communicator"/> object.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_CHAR">
            <summary>
            A single character. There is no equivalent to this type in C# or .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_SIGNED_CHAR">
            <summary>
            A single, signed character. This is equivalent to the <c>sbyte</c> type
            in C# and the <c>System.SByte</c> type in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_UNSIGNED_CHAR">
            <summary>
            A single, unsigned character. There is no equivalent to this type in C# or .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_BYTE">
            <summary>
            A single byte. This is equivalent to the <c>byte</c> type
            in C# and the <c>System.Byte</c> type in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_WCHAR">
            <summary>
            A single, wide character. The equivalent is <c>char</c> in C# and <c>System.Char</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_SHORT">
            <summary>
            A signed short integer. This is equivalent to the <c>short</c> type in C# and
            <c>System.Int16</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_UNSIGNED_SHORT">
            <summary>
            An unsigned short integer. This is equivalent to the <c>ushort</c> type in C# and
            <c>System.UInt16</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_INT">
            <summary>
            A signed integer. This is equivalent to the <c>int</c> type in C# and
            <c>System.Int32</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_UNSIGNED">
            <summary>
            An unsigned integer. This is equivalent to the <c>uint</c> type in C# and
            <c>System.UInt32</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_LONG">
            <summary>
            A long signed integer. There is no equivalent in C# or .NET, because the 64-bit 
            integer in C# and .NET is mapped to <see cref="F:MPI.Unsafe.MPI_LONG_LONG_INT"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_UNSIGNED_LONG">
            <summary>
            A long unsigned integer. There is no equivalent in C# or .NET, because the 64-bit 
            unsigned integer in C# and .NET is mapped to <see cref="F:MPI.Unsafe.MPI_UNSIGNED_LONG_LONG"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_FLOAT">
            <summary>
            A single-precision floating-point value. The equivalent is <c>float</c> in C#
            and <c>System.Single</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_DOUBLE">
            <summary>
            A double-precision floating-point value. The equivalent is <c>double</c> in C#
            and <c>System.Double</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_LONG_DOUBLE">
            <summary>
            An extended-precision floating-point value. There is no equivalent in C# or .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_LONG_LONG_INT">
            <summary>
            A long long signed integer. The equivalent is <c>long</c> in C# and
            <c>System.Int64</c> in .NET. This is a synonym for <see cref="F:MPI.Unsafe.MPI_LONG_LONG"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_UNSIGNED_LONG_LONG">
            <summary>
            A long long unsigned integer. The equivalent is <c>ulong</c> in C# and
            <c>System.UInt64</c> in .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_LONG_LONG">
            <summary>
            A long long signed integer. The equivalent is <c>long</c> in C# and
            <c>System.Int64</c> in .NET. This is a synonym for <see cref="F:MPI.Unsafe.MPI_LONG_LONG_INT"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_PACKED">
            <summary>
            A special data type used to indicate data that has been packed with <see cref="M:MPI.Unsafe.MPI_Pack(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32@,System.Int32)"/>.
            This type is only used by the lowest-level MPI operations. The .NET equivalent is the 
            <see cref="T:MPI.DatatypeCache.Packed"/> type.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_DATATYPE_NULL">
            <summary>
            A special datatype value that indicates "no datatype".
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_IDENT">
            <summary>
            Constant used in comparisons of MPI objects to denote that two objects are identical.
            See <see cref="F:MPI.Comparison.Identical"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_CONGRUENT">
            <summary>
            Constant used in comparisons of MPI objects to denote that two objects are congruent,
            meaning that the objects act the same way but are not identical.
            See <see cref="F:MPI.Comparison.Congruent"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_SIMILAR">
            <summary>
            Constant used in comparisons of MPI objects to denote that two objects are similar,
            but assign different ranks to each of the processes.
            See <see cref="F:MPI.Comparison.Similar"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_UNEQUAL">
            <summary>
            Constant used in comparisons of MPI objects to denote that two objects are completely
            different.
            See <see cref="F:MPI.Comparison.Unequal"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_PROC_NULL">
            <summary>
            Special value for the source or dest argument to any communication operation,
            which indicates that the communication is a no-op. Not supported in MPI.NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ROOT">
            <summary>
            Special value used for Intercommunicator collective operations which indicates the root
            process for a collective operation.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_GROUP_EMPTY">
            <summary>
            An empty group containing no processes. See <see cref="F:MPI.Group.empty"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_GROUP_NULL">
            <summary>
            A constant used to indicate the "null" group of processes. Corresponds to a null <see cref="T:MPI.Group"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_INFO_NULL">
            <summary>
            A special info key used to indicate that no extra information is being
            passed into a routine.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_UNDEFINED">
            <summary>
            "Undefined" value used to identify when a rank is not a part of a group.
            See <see cref="F:MPI.Group.NoProcess"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_CART">
            <summary>
              A constant used to indicate whether a communicator has a Cartesian topology.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_GRAPH">
            <summary>
              A constant used to indicate whether a communicator has a Graph topology.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_KEYVAL_INVALID">
            <summary>
            Special key value that indicates an invalid key.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_NULL_COPY_FN">
            <summary>
            Special "null" copy function that indicates that an attribute should not be copied.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_NULL_DELETE_FN">
            <summary>
            Special "null" deletion function that indicates that no delete function should
            be called when an attribute is removed from a communicator.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_THREAD_SINGLE">
            <summary>
            Indicates that the MPI program is single-threaded. See <see cref="F:MPI.Threading.Single"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_THREAD_FUNNELED">
            <summary>
            Indicates that the MPI program is multi-threaded, but all MPI operations will be called
            from the main thread. See <see cref="F:MPI.Threading.Funneled"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_THREAD_SERIALIZED">
            <summary>
            Indicates that the MPI program is multi-threaded, but only one thread will call into MPI
            at any given time. See <see cref="F:MPI.Threading.Serialized"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_THREAD_MULTIPLE">
            <summary>
            Indicates that the MPI program is multi-threaded, and any thread can call into MPI
            at any time. See <see cref="F:MPI.Threading.Multiple"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_TAG_UB">
            <summary>
            Predefined attribute key that can be used to determine the maximum
            tag value that users are allowed to provide to a communication 
            request.
            See <see cref="P:MPI.Environment.MaxTag"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_HOST">
            <summary>
            Predefined attribute key that can be used to determine the rank of
            the host process associated with <see cref="F:MPI.Unsafe.MPI_COMM_WORLD"/>.
            If there is no host, the result will be <see cref="F:MPI.Unsafe.MPI_PROC_NULL"/>.
            See <see cref="P:MPI.Environment.HostRank"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_IO">
            <summary>
            Predefined attribute key that can be used to determine the rank of
            the process than can perform I/O via the language-standard I/O
            mechanism. If every process can provided language-standard I/O, the
            resulting value will be <see cref="F:MPI.Unsafe.MPI_ANY_SOURCE"/>; if no process
            can support language-standard I/O, the result will be 
            <see cref="F:MPI.Unsafe.MPI_PROC_NULL"/>.
            See <see cref="P:MPI.Environment.IORank"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_WTIME_IS_GLOBAL">
            <summary>
            Predefined attribute key that can be used to determine whether the
            clocks (accessed via <see cref="M:MPI.Unsafe.MPI_Wtime"/>) are synchronized
            across all processes.
            See <see cref="P:MPI.Environment.IsTimeGlobal"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_MAX_PROCESSOR_NAME">
            <summary>
            The maximum length of the string returned by <see cref="M:MPI.Unsafe.MPI_Get_processor_name(System.Byte[],System.Int32@)"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_MAX_ERROR_STRING">
            <summary>
            The maximum number of characters that can occur in an error string returned from 
            <see cref="M:MPI.Unsafe.MPI_Error_string(System.Int32,System.Byte[],System.Int32@)"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_SUCCESS">
            <summary>
            Error value indicating no error.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_BUFFER">
            <summary>
            Error class indicating an invalid buffer pointer.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_COUNT">
            <summary>
            Error class indicating an invalid count argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_TYPE">
            <summary>
            Error class indicating an invalid data type argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_TAG">
            <summary>
            Error class indicating an invalid tag argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_COMM">
            <summary>
            Error class indicating an invalid communicator.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_RANK">
            <summary>
            Error class indicating an invalid rank.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_ROOT">
            <summary>
            Error class indicating an invalid root.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_TRUNCATE">
            <summary>
            Error class indicating that a message was truncated on receive.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_GROUP">
            <summary>
            Error class indicating an invalid group argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_OP">
            <summary>
            Error class indicating an invalid operation argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_REQUEST">
            <summary>
            Error class indicating an invalid request argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_TOPOLOGY">
            <summary>
            Error class indicating an invalid topology for a communicator argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_DIMS">
            <summary>
            Error class indicating an invalid dimension argument (for cartesian communicators).
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_ARG">
            <summary>
            Error class indicating an invalid argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_OTHER">
            <summary>
            Error class indicating an error that is know, but not described by other MPI
            error classes.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_UNKNOWN">
            <summary>
            Error class indicating that an unkown error occurred.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_INTERN">
            <summary>
            Error class indicating that an internal error occurred in the MPI implementation.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_IN_STATUS">
            <summary>
            Error class indicating that the actual error code is in the status argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_PENDING">
            <summary>
            Error class indicating that a request is still pending.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_FILE">
            <summary>
            Error class indicating an invalid file handle argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_ACCESS">
            <summary>
            Error class indicating that permission was denied when accessing a file.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_AMODE">
            <summary>
            Error class indicating that the amode argument passed to MPI_File_open is invalid.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_BAD_FILE">
            <summary>
            Error class indicating an invalid file name.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_FILE_EXISTS">
            <summary>
            Error class indicating that the file already exists.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_FILE_IN_USE">
            <summary>
            Error class indicating that the file is already in use.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_NO_SPACE">
            <summary>
            Error class indicating that there is not enough space for the file.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_NO_SUCH_FILE">
            <summary>
            Error class indicating that no such file exists.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_IO">
            <summary>
            Error class indicating an I/O error.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_READ_ONLY">
            <summary>
            Error class indicating that the file is read-only.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_CONVERSION">
            <summary>
            Error class indicating that an error occurred in a user-supplied data conversion function.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_DUP_DATAREP">
            <summary>
            Error class indicating that conversion functions could not be registered because a conversion
            function has already been registered for this data representation identifier.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_UNSUPPORTED_DATAREP">
            <summary>
            Error class indicating that an unsupported data representation was passed to <c>MPI_FILE_SET_VIEW</c>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_INFO">
            <summary>
            Error class indicating an invalid info argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_INFO_KEY">
            <summary>
            Error class indicating an invalid info key.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_INFO_VALUE">
            <summary>
            Error class indicating an invalid info value.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_INFO_NOKEY">
            <summary>
            Error class indicating that the requested info key is not defined.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_NAME">
            <summary>
            Error class indicating that an attempt has been made to look up a service
            name that has not been published.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_NO_MEM">
            <summary>
            Error class indicating that no memory is available when trying to allocate
            memory with <c>MPI_Alloc_mem</c>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_NOT_SAME">
            <summary>
            Error class indicating that a collective argument is not the same on all processes,
            or collective routines were called in a different order.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_PORT">
            <summary>
            Error class indicating that a named port does not exist or has been closed.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_QUOTA">
            <summary>
            Error class indicating that the user's quota has been exceeded.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_SERVICE">
            <summary>
            Error class indicating that an attempt to unpublish a service 
            name that has already been unpublished or never was published.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_SPAWN">
            <summary>
            Error class indicating that an attempt to spawn a process has failed.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_UNSUPPORTED_OPERATION">
            <summary>
            Error class indicating that an operation is unsupported.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_WIN">
            <summary>
            Error class indicating an invalid window argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_BASE">
            <summary>
            Error class indicating an invalid base argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_LOCKTYPE">
            <summary>
            Error class indicating an invalid locktype argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_KEYVAL">
            <summary>
            Error class indicating an invalid attribute key.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_RMA_CONFLICT">
            <summary>
            Error class indicating that there were conflicting accesses within a window.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_RMA_SYNC">
            <summary>
            Error class indicating that RMA calls were incorrectly synchronized.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_SIZE">
            <summary>
            Error class indicating an invalid size argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_DISP">
            <summary>
            Error class indicating an invalid displacement argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_ASSERT">
            <summary>
            Error class indicating an invalid assert argument.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERR_LASTCODE">
            <summary>
            The last valid error code for a predefined error class.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERRORS_ARE_FATAL">
            <summary>
            Predefined error handler that indicates that the MPI program should be terminated
            if an error occurs. This is the default error handler in the low-level MPI, which
            is overridden by MPI.NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERRORS_RETURN">
            <summary>
            Predefined error handler that indicates that the MPI routine that detected an error 
            should return an error code. MPI.NET uses this error handler to translate MPI 
            errors into program exceptions.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ERRHANDLER_NULL">
            <summary>
            Predefined error handler that represents "no" error handler.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ANY_SOURCE">
            <summary>
            Predefined value for the "source" parameter to MPI receive or probe operations, 
            which indicates that a message from any process may be matched.
            See <see cref="F:MPI.Communicator.anySource"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_ANY_TAG">
            <summary>
            Predefined value for the "tag" parameter to MPI receive or probe operations,
            which indicates that a message with any tag may be matched.
            See <see cref="F:MPI.Communicator.anyTag"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_MAX">
            <summary>
            Compute the maximum value via an MPI reduction operation. 
            See <see cref="P:MPI.Operation`1.Max"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_MIN">
            <summary>
            Compute the minimum value via an MPI reduction operation. 
            See <see cref="P:MPI.Operation`1.Min"/>.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_SUM">
            <summary>
            Compute the sum via an MPI reduction operation. 
            See <see cref="P:MPI.Operation`1.Add"/>
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_PROD">
            <summary>
            Compute the product via an MPI reduction operation.
            See <see cref="P:MPI.Operation`1.Multiply"/>
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_LAND">
            <summary>
            Compute the logical AND via an MPI reduction operation.
            See <see cref="P:MPI.Operation`1.LogicalAnd"/>
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_BAND">
            <summary>
            Compute the bitwise AND via an MPI reduction operation.
            See <see cref="P:MPI.Operation`1.BitwiseAnd"/>
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_LOR">
            <summary>
            Compute the logical OR via an MPI reduction operation.
            See <see cref="P:MPI.Operation`1.LogicalOr"/>
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_BOR">
            <summary>
            Compute the bitwise OR via an MPI reduction operation.
            See <see cref="P:MPI.Operation`1.BitwiseOr"/>
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_LXOR">
            <summary>
            Compute the logical exclusive OR via an MPI reduction operation.
            There is no high-level operation corresponding to this predefined
            MPI reduction.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_BXOR">
            <summary>
            Compute the bitwise exclusive OR via an MPI reduction operation.
            See <see cref="P:MPI.Operation`1.ExclusiveOr"/>
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_MINLOC">
            <summary>
            Compute the minimum value and location of that value via
            an MPI reduction operation. There is no high-level operation
            corresponding to this predefined MPI reduction.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_MAXLOC">
            <summary>
            Compute the maximum value and location of that value via
            an MPI reduction operation. There is no high-level operation
            corresponding to this predefined MPI reduction.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_OP_NULL">
            <summary>
            Placeholder operation that indicates "no operation".
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_REQUEST_NULL">
            <summary>
            Constant that indicates a "null" MPI request, meaning that there is no such request.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_BOTTOM">
            <summary>
            A special marker used for the "buf" parameter to point-to-point operations
            and some collectives that indicates that the derived datatype contains absolute 
            (rather than relative) addresses. The use of <c>MPI_BOTTOM</c> is not recommended.
            This facility is unused in C# and .NET.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_STATUS_IGNORE">
            <summary>
            Constant used to indicate that the <c>MPI_Status</c> argument of an MPI
            operation will be ignored.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_STATUSES_IGNORE">
            <summary>
            Constant used to indicate that the array of <c>MPI_Status</c> arguments
            to an MPI operation will be ignored.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_contiguous(System.Int32,System.Int32,System.Int32@)">
            <summary>
            Creates a new datatype from a contiguous block of values
            of the same type. Not used by MPI.NET.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_vector(System.Int32,System.Int32,System.Int32,System.Int32,System.Int32@)">
            <summary>
            Creates a new datatype from a strided block of values of
            the same type. Not used by MPI.NET.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_hvector(System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32@)">
            <summary>
            Creates a new datatype from a strided block of values of
            the same type. Not used by MPI.NET.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_indexed(System.Int32,System.Int32[],System.Int32[],System.Int32,System.Int32@)">
            <summary>
            Creates a new datatype from discontiguous blocks of values
            of the same type. Not used by MPI.NET.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_hindexed(System.Int32,System.Int32[],System.IntPtr[],System.Int32,System.Int32@)">
            <summary>
            Creates a new datatype from discontiguous blocks of values
            of the same type. Not used by MPI.NET.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_struct(System.Int32,System.Int32[],System.IntPtr[],System.Int32[],System.Int32@)">
            <summary>
            Creates a new datatype from a structure containing
            discontiguous blocks of different types. 
            This is the most general type constructor, and is used by
            the <see cref="T:MPI.DatatypeCache"/> to 
            create MPI datatypes from .NET value types.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Pack(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32@,System.Int32)">
            <summary>
            Packs (serializes) data into a byte buffer. This serialized representation can be transmitted via MPI
            with the datatype <see cref="F:MPI.Unsafe.MPI_PACKED"/>and unpacked with <see cref="M:MPI.Unsafe.MPI_Unpack(System.IntPtr,System.Int32,System.Int32@,System.IntPtr,System.Int32,System.Int32,System.Int32)"/>. Serialization 
            in MPI.NET is automatic, so this routine is very rarely used.
            </summary>
            <param name="inbuf">
              Pointer to the input buffer, containing <paramref name="incount"/> values with the MPI datatype
              <paramref name="datatype"/>.
            </param>
            <param name="incount">The number of values in <paramref name="inbuf"/>.</param>
            <param name="datatype">The MPI datatype of the values in <paramref name="inbuf"/>.</param>
            <param name="outbuf">
              A pointer to the buffer of bytes into which we will be packing the serialized representation
              of the data in <paramref name="inbuf"/>.
            </param>
            <param name="outsize">The size (in bytes) of <paramref name="outbuf"/>.</param>
            <param name="position">
              A pointer to the position (in bytes) into <paramref name="outbuf"/> where the packed data 
              will be written. This position will be updated to the next available position in the buffer
              once the serialized data has been written.
            </param>
            <param name="comm">The communicator over which the packed data will be sent.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Unpack(System.IntPtr,System.Int32,System.Int32@,System.IntPtr,System.Int32,System.Int32,System.Int32)">
            <summary>
            Unpacks (deserializes) data from a byte buffer. The serialized representation will have been
            packed by <see cref="M:MPI.Unsafe.MPI_Pack(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32@,System.Int32)"/> and (possibly) transmitted via MPI using the datatype 
            <see cref="F:MPI.Unsafe.MPI_PACKED"/>. Serialization in MPI.NET is automatic, so this routine is very
            rarely used.
            </summary>
            <param name="inbuf">A pointer to the buffer of bytes that will be unpacked.</param>
            <param name="insize">The number of bytes in <paramref name="inbuf"/>.</param>
            <param name="position">
              A pointer to the position (in bytes) inside the buffer from which data will be unpacked.
              This position will be updated to reflect the position of the next value in the buffer
              after the data has been unpacked.
            </param>
            <param name="outbuf">
            A pointer to the buffer where the unpacked data will be written. This buffer contains
            values whose MPI datatype is <paramref name="datatype"/>.
            </param>
            <param name="outcount">Number of values that will be stored into <paramref name="outbuf"/>.</param>
            <param name="datatype">The type of data in <paramref name="outbuf"/>.</param>
            <param name="comm">
              The communicator for which the packing was performed (and which may have been used to transmit the
              serialized data).
            </param>
            <returns></returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Pack_size(System.Int32,System.Int32,System.Int32,System.Int32@)">
            <summary>
            Determine the maximum amount of space that packing <paramref name="incount"/> values with the
            MPI datatype <paramref name="datatype"/> will require. This routine is useful for allocating
            buffer space when packing data with <see cref="M:MPI.Unsafe.MPI_Pack(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32@,System.Int32)"/>.
            </summary>
            <param name="incount">
              The number of elements of type <paramref name="datatype"/> to be packed.
            </param>
            <param name="datatype">The type of data to be packed.</param>
            <param name="comm">The communicator over which the packed data would be transmitted.</param>
            <param name="size">
              A pointer to an integer. This integer will receive the maximum number of bytes required to
              pack the data. However, it is possible that when calling <see cref="M:MPI.Unsafe.MPI_Pack(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32@,System.Int32)"/>, fewer bytes
              will be required to pack the actual data.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Address(System.IntPtr,System.IntPtr@)">
            <summary>
            Converts a pointer into an address for use with MPI. In many cases, this operation is simply a
            cast from the pointer's value to an integer.
            </summary>
            <param name="location">A pointer to the memory whose address will be returned.</param>
            <param name="address">
              A pointer to the integer address value that will be replaced with the address pointed to
              by <paramref name="location"/>.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_extent(System.Int32,System.IntPtr@)">
            <summary>
            Determines the extent of the datatype.
            </summary>
            <param name="datatype">The datatype to query.</param>
            <param name="extent">Receives the extent of <paramref name="datatype"/>.</param>
            <returns></returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_size(System.Int32,System.Int32@)">
            <summary>
            Computes the size of a datatype.
            </summary>
            <param name="datatype">The MPI datatype.</param>
            <param name="size">
              Pointer to an integer, which will be assigned the size of the data type (in bytes).
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_commit(System.Int32@)">
            <summary>
            Completes creation of an MPI datatype. This routine will be called
            automatically when the MPI datatype is being generated via reflection
            in <see cref="M:MPI.DatatypeCache.GetDatatype(System.Type)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Type_free(System.Int32@)">
            <summary>
            Frees an MPI datatype.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_size(System.Int32,System.Int32@)">
            <summary>
            Determine the number of processes in a group. See <see cref="P:MPI.Group.Size"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_rank(System.Int32,System.Int32@)">
            <summary>
            Determine the rank of the calling process in a group. See <see cref="P:MPI.Group.Rank"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_translate_ranks(System.Int32,System.Int32,System.Int32[],System.Int32,System.Int32[])">
            <summary>
              Translate the ranks of processes in one group into those processes' corresponding
              ranks in another group. See <see cref="M:MPI.Group.TranslateRanks(System.Int32[],MPI.Group)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_compare(System.Int32,System.Int32,System.Int32@)">
            <summary>
            Compare two groups. See <see cref="M:MPI.Group.Compare(MPI.Group)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_group(System.Int32,System.Int32@)">
            <summary>
            Retrieve the group associated with a communicator. See <see cref="P:MPI.Communicator.Group"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_union(System.Int32,System.Int32,System.Int32@)">
            <summary>
            Create a group from the union of two groups. See <see cref="M:MPI.Group.op_BitwiseOr(MPI.Group,MPI.Group)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_intersection(System.Int32,System.Int32,System.Int32@)">
            <summary>
            Create a group from the intersection of two groups. See <see cref="M:MPI.Group.op_BitwiseAnd(MPI.Group,MPI.Group)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_difference(System.Int32,System.Int32,System.Int32@)">
            <summary>
            Create a group from the difference of two groups. See <see cref="M:MPI.Group.op_Subtraction(MPI.Group,MPI.Group)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_incl(System.Int32,System.Int32,System.Int32[],System.Int32@)">
            <summary>
            Create a subgroup containing the processes with specific ranks in an existing group. 
             See <see cref="M:MPI.Group.IncludeOnly(System.Int32[])"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_excl(System.Int32,System.Int32,System.Int32[],System.Int32@)">
            <summary>
            Create a subgroup containing all processes in existing group except those specified. 
            See <see cref="M:MPI.Group.Exclude(System.Int32[])"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_range_incl(System.Int32,System.Int32,System.Int32[],System.Int32@)">
            <summary>
            Create a subgroup of processes in a group, based on a set of (first, last, stride) rank triples. 
            Note: this precise functionality is not exposed directly in the normal MPI layer; however, the
            same semantics can be attained with <see cref="M:MPI.Group.IncludeOnly(System.Int32[])"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_range_excl(System.Int32,System.Int32,System.Int32[],System.Int32@)">
            <summary>
            Create a subgroup of processes containing all of the processes in the source group except those described
            by one of th provided(first, last, stride) rank triples. 
            Note: this precise functionality is not exposed directly in the normal MPI layer; however, the
            same semantics can be attained with <see cref="M:MPI.Group.Exclude(System.Int32[])"/>.
            </summary>        
        </member>
        <member name="M:MPI.Unsafe.MPI_Group_free(System.Int32@)">
            <summary>
              Frees a group. This routine will be invoked automatically by <see cref="M:MPI.Group.Dispose"/>
              or the finalizer for <see cref="T:MPI.Group"/>.
            </summary>
            <param name="group"></param>
            <returns></returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_size(System.Int32,System.Int32@)">
            <summary>
            Determines the number of processes in the communicator. See <see cref="P:MPI.Communicator.Size"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_rank(System.Int32,System.Int32@)">
            <summary>
            Determines the rank of the calling process in the communicator. See <see cref="P:MPI.Communicator.Rank"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_compare(System.Int32,System.Int32,System.Int32@)">
            <summary>
            Compare two communicators. See <see cref="M:MPI.Communicator.Compare(MPI.Communicator)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_dup(System.Int32,System.Int32@)">
            <summary>
            Duplicates a communicator, creating a new communicator with the same processes and ranks
            as the original. See <see cref="M:MPI.Communicator.Clone"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_create(System.Int32,System.Int32,System.Int32@)">
            <summary>
            Creates a new communicator from a subgroup of the processes in an existing communicator.
            See <see cref="M:MPI.Communicator.Create(MPI.Group)"/>.
            </summary>
            <param name="comm">
              The existing communicator, from which the new communicator will be created.
            </param>
            <param name="group">
              A group specifying zero or more processes in the communicator <paramref name="comm"/>.
            </param>
            <param name="newcomm">
              A pointer that points to the newly-created communicator, which will contain all of
              the processes in the group. The order of the processes in this new communicator will 
              be the same as the order of those processes in the original communicator.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_split(System.Int32,System.Int32,System.Int32,System.Int32@)">
            <summary>
              Splits a communicator into several new communicators, based on the colors provided.
              See <see cref="M:MPI.Communicator.Split(System.Int32,System.Int32)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Cart_create(System.Int32,System.Int32,System.Int32*,System.Int32*,System.Int32,System.Int32*)">
            <summary>
              Creates a new Cartesian communicator from another
              communicator. See <see cref="M:MPI.CartesianCommunicator.#ctor(MPI.Intracommunicator,System.Int32,System.Int32[],System.Boolean[],System.Boolean)"/>.
            </summary>
            <param name="comm">
              Existing communicator from which the new communicator will be created.
            </param>
            <param name="ndims">
              Number of dimensions for Cartesian communicator's grid to have.
            </param>
            <param name="dims">
              Array to specify sizes in each dimension.
            </param>
            <param name="periods">
              Array of logical values (0s and 1s) indicating whether grid should be periodic in each dimension 
              (i.e. if the last communicator and the first in each dimension are directly connected).
            </param>
            <param name="reorder">
              Logical value indicating whether ranks may be reordered or not.
            </param>
            <param name="newcomm">
              Output parameter for new communicator.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Cart_sub(System.Int32,System.Int32*,System.Int32*)">
            <summary>
              Create a lesser dimensional grid from an existing
              Cartesian communicator. See <see cref="M:MPI.CartesianCommunicator.Subgrid(System.Int32[])"/>.
            </summary>
            <param name="comm">
              The existing communicator.
            </param>
            <param name="remain_dims">
              A logical array indicating whether a dimension in the existing should be kept (1) or dropped (0).
            </param>
            <param name="newcomm">
              The new communicator.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Graph_create(System.Int32,System.Int32,System.Int32*,System.Int32*,System.Int32,System.Int32*)">
            <summary>
              Create a topological communicator with a graph topology,
              where any rank can be connected to any other rank. See
              <see cref="M:MPI.GraphCommunicator.#ctor(MPI.Intracommunicator,System.Int32[][],System.Boolean)"/>.
            </summary>
            <param name="comm">
              An existing Intracommunicator to use to create the new communicator.
            </param>
            <param name="nnodes">
              The number of nodes the graph will have.
            </param>
            <param name="index">
              An array indicating the starting index in <paramref name="edges"/> of the edges for each vertex.
            </param>
            <param name="edges">
              An array of edge targets, indexed by <paramref name="index"/>.
            </param>
            <param name="reorder">
              Logical indicating whether ranks can be reordered.
            </param>
            <param name="newComm">
              The new communicator.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_free(System.Int32@)">
            <summary>
              Frees a communicator. This routine will be invoked automatically by <see cref="M:MPI.Communicator.Dispose"/>
              or the finalizer for <see cref="T:MPI.Communicator"/>.
            </summary>
            <param name="comm">The communicator to free.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Keyval_create(MPI.Unsafe.MPI_Copy_function,MPI.Unsafe.MPI_Delete_function,System.Int32@,System.IntPtr)">
            <summary>
            Creates a new MPI attribute that can be attached to communicators. This routine is
            automatically involved by MPI.NET when a new <see cref="T:MPI.Attribute"/> is created.
            See <see cref="M:MPI.Attribute.Create``1(MPI.AttributeDuplication)"/>
            </summary>
            <param name="copy_fn">
              The function used to copy the attribute when a communicator is
              duplicated.
            </param>
            <param name="delete_fn">
              The function used to delete the attribute when it is removed
              from a communicator.
            </param>
            <param name="keyval">
              Will receive a new integer that identifies this attribute.
            </param>
            <param name="extra_state">
              A user-defined pointer that includes extra information to be 
              passed to the copy and delete functions. This pointer can
              be used to store more information about the attribute itself.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Keyval_free(System.Int32@)">
            <summary>
            Frees an attribute with the given key value. The user must ensure that
            this attribute has been deleted from all communicators before calling
            this routine. This operation is performed by <see cref="M:MPI.Attribute.Dispose"/>.
            </summary>
            <param name="keyval">The attribute's key value.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Attr_put(System.Int32,System.Int32,System.IntPtr)">
            <summary>
            Sets the value of an attribute on a communicator. Attributes in MPI.NET are
            handled through the <see cref="T:MPI.AttributeSet"/> class, an instance of which is associated
            with each communicator.
            </summary>
            <param name="comm">The communicator.</param>
            <param name="keyval">The attribute to set.</param>
            <param name="attribute_val">The new value to place into the communicator.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Attr_get(System.Int32,System.Int32,System.IntPtr,System.Int32@)">
            <summary>
            Retrieves the value of an attribute on a communicator. Attributes in MPI.NET are
            handled through the <see cref="T:MPI.AttributeSet"/> class, an instance of which is associated
            with each communicator.
            </summary>
            <param name="comm">The communicator.</param>
            <param name="keyval">The attribute to get.</param>
            <param name="attribute_val">
              Will receive the value stored for this attribute on this 
              communicator, if one exists. The result is only valid
              if <paramref name="flag"/> is non-zero.
            </param>
            <param name="flag">
              Will receive a boolean value stating whether the attribute is stored
              with this communicator.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Attr_delete(System.Int32,System.Int32)">
            <summary>
            Deletes an attribute stored on the communicator. Attributes in MPI.NET are
            handled through the <see cref="T:MPI.AttributeSet"/> class, an instance of which is associated
            with each communicator.
            </summary>
            <param name="comm">The communicator.</param>
            <param name="keyval">The attribute to delete.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_test_inter(System.Int32,System.Int32@)">
            <summary>
            Determine whether a communicator is an intercommunicator. In MPI.NET, intercommunicators 
            will have type <see cref="T:MPI.Intercommunicator"/>.
            </summary>
            <param name="comm">The communicator.</param>
            <param name="flag">
              Pointer to a flag, which will be set to a non-zero value if <paramref name="comm"/>
              is an intercommunicator.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_remote_size(System.Int32,System.Int32@)">
            <summary>
            Determines the number of processes in the remote group of an intercommunicator.
            See <see cref="P:MPI.Intercommunicator.RemoteSize"/>.
            </summary>
            <param name="comm">The intercommunicator.</param>
            <param name="size">
              Will receive the number of processes in the remote group of 
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Comm_remote_group(System.Int32,System.Int32@)">
            <summary>
            Retrieves the remote group from an intercommunicator.
            See <see cref="P:MPI.Intercommunicator.RemoteGroup"/>.
            </summary>
            <param name="comm">The intercommunicator.</param>
            <param name="group">
              Will receive the group containing all of the processes in the remote group 
              of <paramref name="comm"/>.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Intercomm_create(System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32@)">
            <summary>
            Create a new intercommunicator from two disjoint intracommunicators.
            See <see cref="M:MPI.Intercommunicator.#ctor(MPI.Intracommunicator,System.Int32,MPI.Intracommunicator,System.Int32,System.Int32)"/>.
            </summary>
            <param name="local_comm">The local communicator.</param>
            <param name="local_leader">The leader of the local communicator.</param>
            <param name="bridge_comm">
              Communicator that bridges the intercommunicators, allowing the leaders to communicate.
            </param>
            <param name="remote_leader">
              The rank of the remote group's leader within <paramref name="bridge_comm"/>.
            </param>
            <param name="tag">Tag used for communication to create the intercommunicator.</param>
            <param name="newintercomm">Will receive the new intercommunicator.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Intercomm_merge(System.Int32,System.Int32,System.Int32@)">
            <summary>
            Merge the two groups in an intercommunicator into a single intracommunicator.
            See <see cref="M:MPI.Intercommunicator.Merge(System.Boolean)"/>
            </summary>
            <param name="intercomm">The intercommunicator.</param>
            <param name="high">
              Whether this group of processes has the higher ranks in the resuling intercommunicator.
            </param>
            <param name="newintracomm">The resulting intracommunicator.</param>
            <returns></returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Cartdim_get(System.Int32,System.Int32*)">
            <summary>
              Gets the number of dimensions in the Cartesian communicator. 
              See <see cref="P:MPI.CartesianCommunicator.Dimensions"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Cart_get(System.Int32,System.Int32,System.Int32*,System.Int32*,System.Int32*)">
            <summary>
              Retrieves the primary topological information on a Cartesian communicator: the number of dimensions,
              the size in each dimension, the periodicity in each dimension. Also gives the coordinates of the 
              calling process. See <see cref="P:MPI.CartesianCommunicator.Dimensions"/>, 
              <see cref="P:MPI.CartesianCommunicator.Periodic"/>, and <see cref="P:MPI.CartesianCommunicator.Coordinates"/>.
            </summary>
            <param name="comm">
              The communicator.
            </param>
            <param name="ndims">
             The number of dimensions.
            </param>
            <param name="dims">
              Output parameter for size of each dimension. Should be as long as <paramref name="ndims"/>.
            </param>
            <param name="periods">
              Output parameter for periodicity in each dimension. 0 for false, 1 for true. Should be as long as <paramref name="ndims"/>.
            </param>
            <param name="coords">
              Output parameter for coordinates of calling process. Should be as long as <paramref name="ndims"/>.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Cart_rank(System.Int32,System.Int32*,System.Int32*)">
            <summary>
              Determines the rank of a process in the Cartesian communicator given its coordinates.
              See <see cref="M:MPI.CartesianCommunicator.GetCartesianRank(System.Int32[])"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Cart_coords(System.Int32,System.Int32,System.Int32,System.Int32*)">
            <summary>
              Determines the coordinates of a process given its rank in the Cartesian communicator.
              See <see cref="M:MPI.CartesianCommunicator.GetCartesianCoordinates(System.Int32)"/>.
            </summary>
            <param name="comm">
              The communicator.
            </param>
            <param name="rank">
              The rank of the process in the Cartesian communicator.
            </param>
            <param name="maxdims">
              Length of <paramref name="coords"/>
            </param>
            <param name="coords">
              Output parameter.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Cart_shift(System.Int32,System.Int32,System.Int32,System.Int32*,System.Int32*)">
            <summary>
              Calculates the necessary source and destination ranks for shifting data over the
              Cartesian communicator. See <see cref="M:MPI.CartesianCommunicator.Shift(System.Int32,System.Int32,System.Int32@,System.Int32@)"/>.
            </summary>
            <param name="comm">
              The communicator.
            </param>
            <param name="direction">
              An integer specifying which of the dimensions along which to shift.
            </param>
            <param name="disp">
              How far to shift (negative values means "downward," positive values "upward").
            </param>
            <param name="rank_source">
              Output parameter for rank to receive from.
            </param>
            <param name="rank_dest">
              Output parameter for rank to send to.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Cart_map(System.Int32,System.Int32,System.Int32*,System.Int32*,System.Int32@)">
            <summary>
              Returns a recommended configuration for a new Cartesian grid.
              See <see cref="M:MPI.CartesianCommunicator.Map(MPI.Intracommunicator,System.Int32,System.Int32[],System.Boolean[])"/>.
            </summary>
            <param name="comm">
              The existing communicator.
            </param>
            <param name="ndims">
              The number of dimensions for the Cartesian grid.
            </param>
            <param name="dims">
              An array of length <paramref name="ndims"/> indicating the size of the grid in each dimension.
            </param>
            <param name="periods">
              A logical array of length <paramref name="ndims"/> indicating whether the grid is periodic in any given dimension.
            </param>
            <param name="newrank">
              The new rank of the calling process.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Topo_test(System.Int32,System.Int32@)">
            <summary>
              Find out the communicator topology. In MPI.NET, one queries the communicator's topology by
              looking at the type, e.g., <see cref="T:MPI.Intracommunicator"/>, <see cref="T:MPI.GraphCommunicator"/>, or
              <see cref="T:MPI.CartesianCommunicator"/>.
            </summary>
            <param name="comm">
              The communicator.
            </param>
            <param name="status">
              Value indicating communicator topology; one of MPI_CART, MPI_GRAPH, or MPI_UNDEFINED (if the communicator
              has no topology).
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Graphdims_get(System.Int32,System.Int32@,System.Int32@)">
            <summary>
              Retrieve the dimensions of a Graph communicator. See <see cref="P:MPI.GraphCommunicator.Edges"/>
              and <see cref="P:MPI.GraphCommunicator.NumEdges"/>.
            </summary>
            <param name="comm">
              The communicator.
            </param>
            <param name="nnodes">
              Output parameter for the number of nodes in the graph.
            </param>
            <param name="nedges">
              Output parameter for the number of edges in the graph.
            </param>
            <returns></returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Graph_get(System.Int32,System.Int32,System.Int32,System.Int32*,System.Int32*)">
            <summary>
              Retrieve the index and edges arrays used to create the graph communicator. See 
              <see cref="P:MPI.GraphCommunicator.Edges"/> and <see cref="P:MPI.GraphCommunicator.NumEdges"/>.
            </summary>
            <param name="comm">
              The communicator.
            </param>
            <param name="maxindex">
              The size of <paramref name="index"/>.
            </param>
            <param name="maxedges">
              The size of <paramref name="edges"/>
            </param>
            <param name="index">
              Output array in which to store the index array.
            </param>
            <param name="edges">
              Output array in which to store the edges array.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Graph_neighbors_count(System.Int32,System.Int32,System.Int32@)">
            <summary>
              Retrieve the number of neighbors of a node. See <see cref="P:MPI.GraphCommunicator.Neighbors"/>
              and <see cref="M:MPI.GraphCommunicator.NeighborsOf(System.Int32)"/>.
            </summary>
            <param name="comm">
              The communicator.
            </param>
            <param name="rank">
              The rank of the node of interest.
            </param>
            <param name="nneighbors">
              Output parameter to store the number of neighbors.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Graph_neighbors(System.Int32,System.Int32,System.Int32,System.Int32*)">
            <summary>
              Retrieve a list of the neighbors of a node. See <see cref="P:MPI.GraphCommunicator.Neighbors"/>
              and <see cref="M:MPI.GraphCommunicator.NeighborsOf(System.Int32)"/>.
            </summary>
            <param name="comm">
              The communicator.
            </param>
            <param name="rank">
              The rank of the node of interest.
            </param>
            <param name="maxneighbors">
              The size of <paramref name="neighbors"/>.
            </param>
            <param name="neighbors">
              Output array to store the list of neighbors.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Graph_map(System.Int32,System.Int32,System.Int32*,System.Int32*,System.Int32@)">
            <summary>
              Returns a recommended configuration for a new Graph communicator.
              See <see cref="M:MPI.GraphCommunicator.Map(MPI.Intracommunicator,System.Int32[][])"/>.
            </summary>
            <param name="comm">
              The existing communicator.
            </param>
            <param name="nnodes">
              The number of nodes to assume.
            </param>
            <param name="index">
              An index array to use (where the semantics is the same as for <see cref="M:MPI.Unsafe.MPI_Graph_create(System.Int32,System.Int32,System.Int32*,System.Int32*,System.Int32,System.Int32*)"/>.
            </param>
            <param name="edges">
              An array of edges as for the constructor <see cref="M:MPI.Unsafe.MPI_Graph_create(System.Int32,System.Int32,System.Int32*,System.Int32*,System.Int32,System.Int32*)"/>.
            </param>
            <param name="newrank">
              The new rank of the calling process.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Dims_create(System.Int32,System.Int32,System.Int32*)">
            <summary>
              Suggest a shape for a new Cartesian communicator, given the number of dimensions.
              See <see cref="M:MPI.CartesianCommunicator.ComputeDimensions(System.Int32,System.Int32,System.Int32[]@)"/>.
            </summary>
            <param name="nnodes">
              The number of nodes the grid will contain.
            </param>
            <param name="ndims">
              The number of dimensions the grid will have.
            </param>
            <param name="dims">
              An array indicating the size in each dimension. Any nonzero entries will be left
              alone; only 0 values will be changed to shape the grid.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Init(System.Int32@,System.Byte**@)">
            <summary>
            Initializes MPI. This routine must be called before any other MPI routine.
            It will be invoked by the <see cref="T:MPI.Environment"/> constructor.
            </summary>
            <param name="argc">The number of arguments in <paramref name="argv"/>.</param>
            <param name="argv">
              A pointer to an array of C-style strings containing all of
              the command-line arguments.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Finalize">
            <summary>
            Finalizes (shuts down) MPI. This routine must be called before exiting the
            program. It will be invoked by <see cref="M:MPI.Environment.Dispose"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Initialized(System.Int32@)">
            <summary>
            Determine whether MPI has already been initialized. See <see cref="P:MPI.Environment.Initialized"/>.
            </summary>
            <param name="flag">Will be set to a non-zero value if the MPI environment has been initialized.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Finalized(System.Int32@)">
            <summary>
            Determine whether MPI has already been finalized. See <see cref="P:MPI.Environment.Finalized"/>.
            </summary>
            <param name="flag">Will be set to a non-zero value if the MPI environment has been finalized.</param>
            <returns></returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Abort(System.Int32,System.Int32)">
            <summary>
            Aborts the current MPI program. See <see cref="M:MPI.Environment.Abort(System.Int32)"/> 
            and <see cref="M:MPI.Communicator.Abort(System.Int32)"/>.
            </summary>
            <param name="comm">The communicator that will be used to abort the program.</param>
            <param name="errcode">The error code to be returned from the MPI process.</param>
            <returns>This routine does not return.</returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Init_thread(System.Int32@,System.Byte**@,System.Int32,System.Int32@)">
            <summary>
            Initializes the MPI library with thread support. This operation subsumes <see cref="M:MPI.Unsafe.MPI_Init(System.Int32@,System.Byte**@)"/>.
            See <see cref="M:MPI.Environment.#ctor(System.String[]@,MPI.Threading)"/>.
            </summary>
            <param name="argc">Pointer to the number of arguments passed on the command line.</param>
            <param name="argv">Pointer to the command-line arguments (array of C-style strings).</param>
            <param name="required">
              The threading level required by the caller, which must be one of the <c>MPI_THREAD_*</c> 
              constants.
            </param>
            <param name="provided">
              Returns the actual threading level that the MPI implementation is providing, which will be
              one of the <c>MPI_THREAD_*</c> constants.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Is_thread_main(System.Int32@)">
            <summary>
            Determine whether the calling thread is the main MPI thread (that called <see cref="M:MPI.Unsafe.MPI_Init(System.Int32@,System.Byte**@)"/>
            or <see cref="M:MPI.Unsafe.MPI_Init_thread(System.Int32@,System.Byte**@,System.Int32,System.Int32@)"/>. See <see cref="P:MPI.Environment.IsMainThread"/>.
            </summary>
            <param name="flag">Returns whether the calling thread is the main thread.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Query_thread(System.Int32@)">
            <summary>
            Determine the level of threading support provided by the MPI library. 
            See <see cref="P:MPI.Environment.Threading"/>.
            </summary>
            <param name="provided">Returns one of the <c>MPI_THREAD_*</c> constants.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Get_processor_name(System.Byte[],System.Int32@)">
            <summary>
            Retrieve the name of the processor or compute node that is currently executing.
            See <see cref="P:MPI.Environment.ProcessorName"/>.
            </summary>
            <param name="name">
              Pointer to an array of bytes that will, on return, contain the name of
              the currenly executing processor. If the processor name requires more
              than *<paramref name="len"/> ASCII characters, only the first 
              *<paramref name="len"/> characters will be written. To be sure that you
              will always get the full processor name, <paramref name="name"/> should
              refer to at least <see cref="F:MPI.Unsafe.MPI_MAX_PROCESSOR_NAME"/> characters.
            </param>
            <param name="len">
              A pointer to the number of bytes in the processor name. On input, this is 
              the number of bytes that <paramref name="name"/> refers to. When this
              function returns, this will point to the number of bytes in the actual name.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Errhandler_create(System.IntPtr,System.Int32@)">
            <summary>
            Creates a new MPI error handler from a user function. Attaching this error handler 
            to a communicator will invoke the user error handler when an error occurs. 
            This feature is not supported in MPI.NET; instead, MPI.NET installs its own error
            handler that translates MPI errors into .NET exceptions.
            </summary>
            <param name="function">The user's function.</param>
            <param name="errhandler">The newly-created error handler.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Errhandler_set(System.Int32,System.Int32)">
            <summary>
            Set the error handler for a given communicator. This feature is not supported in MPI.NET; 
            instead, MPI.NET installs its own error handler that translates MPI errors into .NET exceptions.
            </summary>
            <param name="comm">The communicator.</param>
            <param name="errhandler">The error handler.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Errhandler_get(System.Int32,System.Int32@)">
            <summary>
            Retrieve the error handler for a given communicator. This feature is not supported in MPI.NET; 
            instead, MPI.NET installs its own error handler that translates MPI errors into .NET exceptions.
            </summary>
            <param name="comm">The communicator.</param>
            <param name="errhandler">The error handler attached to the communicator.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Errhandler_free(System.Int32@)">
            <summary>
            Free a user-defined error handler that was created with <see cref="M:MPI.Unsafe.MPI_Errhandler_create(System.IntPtr,System.Int32@)"/>.
            This feature is not supported in MPI.NET; instead, MPI.NET installs its own error handler 
            that translates MPI errors into .NET exceptions.
            </summary>
            <param name="errhandler"></param>
            <returns></returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Error_string(System.Int32,System.Byte[],System.Int32@)">
            <summary>
            Retrieves an error string corresponding to the given MPI error code. Used internally by
            MPI.NET to provide an error string in MPI.NET exceptions.
            </summary>
            <param name="errorcode">The error code.</param>
            <param name="text">Byte array that will be filled with a string describing the error.</param>
            <param name="resultlen">Returns the number of elements placed into the buffer <paramref name="text"/>.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Error_class(System.Int32,System.Int32@)">
            <summary>
            Maps an MPI error code into an error class. Error classes describe (in general) what kind of
            error occurred, and can be used to provide better information to the user. The MPI_ERR_* constants
            give the various error classes present in MPI. Used internally by MPI.NET.
            </summary>
            <param name="errorcode">The error code returned from MPI.</param>
            <param name="errorclass">Set to the error class</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Send(System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Send a message to another process within the communicator. 
            See <see cref="M:MPI.Communicator.Send``1(``0,System.Int32,System.Int32)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Recv(System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,MPI.Unsafe.MPI_Status@)">
            <summary>
            Receive a message from another process within the communicator. 
            See <see cref="M:MPI.Communicator.Receive``1(System.Int32,System.Int32)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Sendrecv(System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,MPI.Unsafe.MPI_Status@)">
            <summary>
              Simultaneously send and receive a message from another process within the communicator.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Test_cancelled(MPI.Unsafe.MPI_Status@,System.Int32@)">
            <summary>
            Determine whether a particular communication operation was cancelled.
            See <see cref="P:MPI.Status.Cancelled"/>.
            </summary>
            <param name="status">Status object</param>
            <param name="flag">Will be set to a non-zero value if the communnication was cancelled.</param>
            <returns>Error code.</returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Get_count(MPI.Unsafe.MPI_Status@,System.Int32,System.Int32@)">
            <summary>
            Determine the number of elements transmitted by a communication operation.
            See <see cref="M:MPI.Status.Count(System.Type)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Isend(System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32@)">
            <summary>
            An immediate (non-blocking) point-to-point send. 
            See <see cref="M:MPI.Communicator.ImmediateSend``1(``0,System.Int32,System.Int32)"/>.
            </summary>
            <param name="buf">Buffer of data to send.</param>
            <param name="count">The number of elements in <paramref name="buf"/>.</param>
            <param name="datatype">The type of data in <paramref name="buf"/>.</param>
            <param name="dest">Rank of the destination process.</param>
            <param name="tag">Tag used to transmit this data.</param>
            <param name="comm">Communicator through which this data will be sent.</param>
            <param name="request">Receives a request object that can be used to query this communication.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Irecv(System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32@)">
            <summary>
            A non-blocking receive that posts the intent to receive a value. The actual receive will be
            completed when the corresponding request is completed.
            See <see cref="M:MPI.Communicator.ImmediateReceive``1(System.Int32,System.Int32)"/>.
            </summary>
            <param name="buf">Buffer that will receive message data.</param>
            <param name="count">Number of elements in <paramref name="buf"/>.</param>
            <param name="datatype">Type of data stored in <paramref name="buf"/>.</param>
            <param name="source">Rank of the processor that will initiate this message, or <see cref="F:MPI.Unsafe.MPI_ANY_SOURCE"/>.</param>
            <param name="tag">Message tag used to identify the message, or <see cref="F:MPI.Unsafe.MPI_ANY_TAG"/>.</param>
            <param name="comm">Communicator through which the message will be sent.</param>
            <param name="request">Receives a request object that can be used to query this communication.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Wait(System.Int32@,MPI.Unsafe.MPI_Status@)">
            <summary>
            Wait until the given request has completed. See <see cref="M:MPI.Request.Wait"/>.
            </summary>
            <param name="request">Request object.</param>
            <param name="status">Will receive the status of the completed operation.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Test(System.Int32@,System.Int32@,MPI.Unsafe.MPI_Status@)">
            <summary>
            Test whether the given request has completed. See <see cref="M:MPI.Request.Test"/>.
            </summary>
            <param name="request">Request object.</param>
            <param name="flag">Will be set to a non-zero value if the request has completed.</param>
            <param name="status">Will receive the status of the completed operation.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Request_free(System.Int32@)">
            <summary>
            Free the resources associated with a request.
            </summary>
            <param name="request">The request that will be freed.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Waitany(System.Int32,System.Int32[],System.Int32@,MPI.Unsafe.MPI_Status@)">
            <summary>
            Waits until any of the given MPI requests completes before returning.
            See <see cref="M:MPI.RequestList.WaitAny"/>.
            </summary>
            <param name="count">The number of requests in <paramref name="array_of_requests"/>.</param>
            <param name="array_of_requests">An array of MPI request objects.</param>
            <param name="index">Receives the index of the request that completed.</param>
            <param name="status">Receives the status of the completed request.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Testany(System.Int32,System.Int32[],System.Int32@,System.Int32@,MPI.Unsafe.MPI_Status@)">
            <summary>
            Test whether any of the MPI requests has completed.
            See <see cref="M:MPI.RequestList.TestAny"/>.
            </summary>
            <param name="count">The number of requests in <paramref name="array_of_requests"/>.</param>
            <param name="array_of_requests">An array of MPI request objects.</param>
            <param name="index">Receives the index of the request that completed (if <paramref name="flag"/> is non-zero).</param>
            <param name="flag">Will be set to a non-zero value if a request has completed.</param>
            <param name="status">Receives the status of the completed request (if <paramref name="flag"/> is non-zero).</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Waitall(System.Int32,System.Int32*,MPI.Unsafe.MPI_Status[])">
            <summary>
            Wait until all of the given MPI requests have completed before returning.
            See <see cref="M:MPI.RequestList.WaitAll"/>.
            </summary>
            <param name="count">The number of requests (and statuses).</param>
            <param name="array_of_requests">An array of MPI request objects to be completed.</param>
            <param name="array_of_statuses">An array of MPI status objects, to be filled in by the completed requests.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Testall(System.Int32,System.Int32*,System.Int32@,MPI.Unsafe.MPI_Status[])">
            <summary>
            Test whether all of the given MPI requests have been completed.
            See <see cref="M:MPI.RequestList.TestAll"/>.
            </summary>
            <param name="count">The number of requests (and statuses).</param>
            <param name="array_of_requests">An array of MPI request objects to be completed.</param>
            <param name="flag">Will be set to a non-zero value if all requests have completed.</param>
            <param name="array_of_statuses">An array of MPI status objects, to be filled in by the completed requests (if <paramref name="flag"/> is non-zero).</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Waitsome(System.Int32,System.Int32[],System.Int32@,System.Int32[],MPI.Unsafe.MPI_Status[])">
            <summary>
            Wait until some MPI requests have completed, then provide a list of all of the requests that have completed.
            See <see cref="M:MPI.RequestList.WaitSome"/>.
            </summary>
            <param name="incount">The number of requests in <paramref name="array_of_requests"/>.</param>
            <param name="array_of_requests">The array of requests to be completed.</param>
            <param name="outcount">Receives the number of requests that have been completed.</param>
            <param name="array_of_indices">
              An array that will receive the indices into <paramref name="array_of_requests"/> of the
              completed requests.
            </param>
            <param name="array_of_statuses">
              Array containing the completed status information that corresponds to the completed
              requests whose indices are in <paramref name="array_of_indices"/>.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Testsome(System.Int32,System.Int32[],System.Int32@,System.Int32[],MPI.Unsafe.MPI_Status[])">
            <summary>
            Providing a list of all of the requests that have completed, without waiting for any
            requests to complete. See <see cref="M:MPI.RequestList.TestSome"/>.
            </summary>
            <param name="incount">The number of requests in <paramref name="array_of_requests"/>.</param>
            <param name="array_of_requests">The array of requests to be completed.</param>
            <param name="outcount">Receives the number of requests that have been completed.</param>
            <param name="array_of_indices">
              An array that will receive the indices into <paramref name="array_of_requests"/> of the
              completed requests.
            </param>
            <param name="array_of_statuses">
              Array containing the completed status information that corresponds to the completed
              requests whose indices are in <paramref name="array_of_indices"/>.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Iprobe(System.Int32,System.Int32,System.Int32,System.Int32@,MPI.Unsafe.MPI_Status@)">
            <summary>
            Test whether a message is available. See <see cref="M:MPI.Communicator.ImmediateProbe(System.Int32,System.Int32)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Probe(System.Int32,System.Int32,System.Int32,MPI.Unsafe.MPI_Status@)">
            <summary>
            Wait until a message is available. See <see cref="M:MPI.Communicator.Probe(System.Int32,System.Int32)"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Cancel(System.Int32@)">
            <summary>
            Cancel an outstanding MPI communication request. See <see cref="M:MPI.Request.Cancel"/>.
            </summary>
            <param name="request">The request to be cancelled.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Wtime">
            <summary>
            Returns a floating point number of seconds, since some time in the past
            See <see cref="P:MPI.Environment.Time"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Wtick">
            <summary>
            Returns a resolution of <see cref="M:MPI.Unsafe.MPI_Wtime"/>, in seconds.
            See <see cref="P:MPI.Environment.TimeResolution"/>.
            </summary>
        </member>
        <member name="M:MPI.Unsafe.MPI_Allreduce(System.IntPtr,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Perform a parallel reduction operation that summarizes the results from the input provided
            by all of the processes in the communicator. Semantically, this is equivalent to an
            <see cref="M:MPI.Unsafe.MPI_Reduce(System.IntPtr,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)"/> to an arbitrary root followed by an <see cref="M:MPI.Unsafe.MPI_Bcast(System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)"/> from 
            that process.
            See <see cref="M:MPI.Intracommunicator.Allreduce``1(``0,MPI.ReductionOperation{``0})"/>
            </summary>
            <param name="sendbuf">
              Buffer containing the "outgoing" values contributed by the calling process to the reduction operation.
            </param>
            <param name="recvbuf">
              Buffer that will receive the results of the parallel reduction.
            </param>
            <param name="count">
              The number of elements in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.
            </param>
            <param name="datatype">
              The type of data in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.
            </param>
            <param name="op">
              The MPI reduction operation to use, which may be one of the predefined reduction operations
              or a user-defined operation created with <see cref="M:MPI.Unsafe.MPI_Op_create(System.IntPtr,System.Int32,System.Int32@)"/>.
            </param>
            <param name="comm">
              The communicator over which the reduction will occur.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Allgather(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)">
            <summary>
            Gather the values provided by each process into an array containing the contributions of all
            of the processes. This operation is equivalent to a <see cref="M:MPI.Unsafe.MPI_Gather(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)"/> to an arbitrary
            root followed by an <see cref="M:MPI.Unsafe.MPI_Bcast(System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)"/> from that root.
            See <see cref="M:MPI.Intracommunicator.Allgather``1(``0)"/>
            </summary>
            <param name="sendbuf">Buffer containing the values that will be sent from this process.</param>
            <param name="sendcount">The number of elements to send.</param>
            <param name="sendtype">The datatype describing the send buffer.</param>
            <param name="recvbuf">Buffer that will contain all of the values contributed by every process.</param>
            <param name="recvcount">The number of elements to receive from each process.</param>
            <param name="recvtype">The type of data that will be stored in the receive buffer.</param>
            <param name="comm">The communicator over which data will be gathered.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Allgatherv(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32[],System.Int32[],System.Int32,System.Int32)">
            <summary>
            Gather the values provided by each process into an array containing the contributions of all
            of the processes. This operation differs from <see cref="M:MPI.Unsafe.MPI_Allgather(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)"/> in that it 
            permits different processes to provide a different number of elements to be gathered.
            See <see cref="M:MPI.Intracommunicator.Allgather``1(``0)"/>
            </summary>
            <param name="sendbuf">Buffer containing the values that will be sent from this process.</param>
            <param name="sendcount">The number of elements to send.</param>
            <param name="sendtype">The datatype describing the send buffer.</param>
            <param name="recvbuf">Buffer that will contain all of the values contributed by every process.</param>
            <param name="recvcounts">
              An array whose ith element is the number of elements to be received from the process 
              with rank i.
            </param>
            <param name="displs">
              An array whose ith element is the offset (in <paramref name="recbuf"/>) at which the
              data from process i should be placed.
            </param>
            <param name="recvtype">The type of data that will be stored in the receive buffer.</param>
            <param name="comm">The communicator over which data will be gathered.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Alltoall(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)">
            <summary>
            Transmits data from every process in a communicator to every other process in the communicator.
            Similar to <see cref="M:MPI.Unsafe.MPI_Allgather(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)"/>, except that each process can send different data to
            every other process. To send a different amount of data to each process, use 
            <see cref="M:MPI.Unsafe.MPI_Alltoallv(System.IntPtr,System.Int32[],System.Int32[],System.Int32,System.IntPtr,System.Int32[],System.Int32[],System.Int32,System.Int32)"/> or <see cref="M:MPI.Unsafe.MPI_Alltoallw(System.IntPtr,System.Int32[],System.Int32[],System.Int32[],System.IntPtr,System.Int32[],System.Int32[],System.Int32[],System.Int32)"/>.
            See <see cref="M:MPI.Intracommunicator.Alltoall``1(``0[])"/>.
            </summary>
            <param name="sendbuf">
              Buffer containing the data to send from this process. The ith position in this buffer  
              contains the data that will be sent to the process with rank i.
            </param>
            <param name="sendcount">The number of elements to send to each process.</param>
            <param name="sendtype">The type of data stored in <paramref name="sendbuf"/>.</param>
            <param name="recvbuf">
              Buffer that will receive data sent from other processes to this process.
            </param>
            <param name="recvcount">
              The number of elements that will be received from each process.
            </param>
            <param name="recvtype">The type of data stored in <paramref name="recvbuf"/>.</param>
            <param name="comm">The communicator used for collective communication.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Alltoallv(System.IntPtr,System.Int32[],System.Int32[],System.Int32,System.IntPtr,System.Int32[],System.Int32[],System.Int32,System.Int32)">
            <summary>
            Transmits data from every process in a communicator to every other process in the communicator.
            Similar to <see cref="M:MPI.Unsafe.MPI_Allgatherv(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32[],System.Int32[],System.Int32,System.Int32)"/>, except that each process can send different data to
            every other process. If all of your processes send the same amount of data to each other, use
            the simpler <see cref="M:MPI.Unsafe.MPI_Alltoall(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)"/>; if you need the data sent to different processes to
            have different datatypes, use <see cref="M:MPI.Unsafe.MPI_Alltoallw(System.IntPtr,System.Int32[],System.Int32[],System.Int32[],System.IntPtr,System.Int32[],System.Int32[],System.Int32[],System.Int32)"/>.
            See <see cref="M:MPI.Intracommunicator.Alltoall``1(``0[])"/>.
            </summary>
            <param name="sendbuf">
              Buffer containing the data to send from this process. The ith position in this buffer  
              contains the data that will be sent to the process with rank i.
            </param>
            <param name="sendcounts">
              An array whose ith element contains the number of elements to be send to the process with
              rank i.
            </param>
            <param name="sdispls">
              An array whose ith element contains the offsets into <paramref name="sendbuf"/> where the
              data destined for the process with rank i begins.
            </param>
            <param name="sendtype">The type of data in <paramref name="sendbuf"/>.</param>
            <param name="recvbuf">
              Buffer that will receive data sent from other processes to this process.
            </param>
            <param name="recvcounts">
              An array whose jth element contains the number of elements that will be received from the
              process with rank j.
            </param>
            <param name="rdispls">
              An array whose jth element contains the offset into <paramref name="recvbuf"/> where the
              data received from the process with rank j begins.
            </param>
            <param name="recvtype">The type of data in <paramref name="recvbuf"/>.</param>
            <param name="comm">The communicator used for collective communication.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Alltoallw(System.IntPtr,System.Int32[],System.Int32[],System.Int32[],System.IntPtr,System.Int32[],System.Int32[],System.Int32[],System.Int32)">
            <summary>
            Transmits data from every process in a communicator to every other process in the communicator.
            Similar to <see cref="M:MPI.Unsafe.MPI_Allgatherv(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32[],System.Int32[],System.Int32,System.Int32)"/>, except that each process can send different data to
            every other process. If all of your processes send the same amount of data to each other, use
            the simpler <see cref="M:MPI.Unsafe.MPI_Alltoall(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)"/>; if the volume of data sent to each process can be different
            but all of the data has the same type, use <see cref="M:MPI.Unsafe.MPI_Alltoallv(System.IntPtr,System.Int32[],System.Int32[],System.Int32,System.IntPtr,System.Int32[],System.Int32[],System.Int32,System.Int32)"/>.
            See <see cref="M:MPI.Intracommunicator.Alltoall``1(``0[])"/>.
            </summary>
            <param name="sendbuf">
              Buffer containing the data to send from this process. The ith position in this buffer  
              contains the data that will be sent to the process with rank i.
            </param>
            <param name="sendcnts">
              An array whose ith element contains the number of elements to be send to the process with
              rank i.
            </param>
            <param name="sdispls">
              An array whose ith element contains the offsets into <paramref name="sendbuf"/> where the
              data destined for the process with rank i begins.
            </param>
            <param name="sendtypes">
              An array whose ith element contains the type of data that will be sent to rank i.
            </param>
            <param name="recvbuf">
              Buffer that will receive data sent from other processes to this process.
            </param>
            <param name="recvcnts">
              An array whose jth element contains the number of elements that will be received from the
              process with rank j.
            </param>
            <param name="rdispls">
              An array whose jth element contains the offset into <paramref name="recvbuf"/> where the
              data received from the process with rank j begins.
            </param>
            <param name="recvtypes">
              An array whose jth element contains the type of data that will be received from the process
              with rank j.
            </param>
            <param name="comm">The communicator used for collective communication.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Barrier(System.Int32)">
            <summary>
            A synchronization barrier where no processor leaves the barrier until all processors have entered the barrier.
            See <see cref="M:MPI.Communicator.Barrier"/>.
            </summary>
            <param name="comm">The communicator whose processes will be synchronized.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Bcast(System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Broadcast a value from the root process to every process within the communication.
            
            See <see cref="M:MPI.Intracommunicator.Broadcast``1(``0@,System.Int32)"/>.
            </summary>
            <param name="buffer">
              Pointer to the data that will be broadcast. At the root, this buffer will be
              read; in all other processes, the buffer will be written.
            </param>
            <param name="count">The number of elements that <paramref name="buffer"/> points to.</param>
            <param name="datatype">The type of data stored in the <paramref name="buffer"/>.</param>
            <param name="root">The rank of the root processor, from which the data will be broadcast.</param>
            <param name="comm">The communicator over which the data will be transmitted.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Exscan(System.IntPtr,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Performs a partial exclusive reduction on the data, returning the result from combining the data provided 
            by the first P-1 processes to the process with rank P. 
            See <see cref="M:MPI.Intracommunicator.ExclusiveScan``1(``0,MPI.ReductionOperation{``0})"/>
            </summary>
            <param name="sendbuf">Buffer containing the data to contribute to the reduction.</param>
            <param name="recvbuf">Buffer that will receive the result of combining the first Rank values.</param>
            <param name="count">Number of values in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.</param>
            <param name="datatype">The type of data in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.</param>
            <param name="op">The reduction operation used to combine values.</param>
            <param name="comm">The communicator over which the communication will occur.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Gather(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Gather the values provided by each process into an array containing the contributions of all
            of the processes. This routine differs from <see cref="M:MPI.Unsafe.MPI_Allgather(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)"/> in that the results 
            are gathered to only the "root" process, which is identified by its <paramref name="rank"/> 
            in the communicator. 
            See <see cref="M:MPI.Intracommunicator.Gather``1(``0,System.Int32)"/>
            </summary>
            <param name="sendbuf">Buffer containing the values that will be sent from this process.</param>
            <param name="sendcount">The number of elements to send.</param>
            <param name="sendtype">The datatype describing the send buffer.</param>
            <param name="recvbuf">
              Buffer that will contain all of the values contributed by every process.
              This argument is only significant at the root.
            </param>
            <param name="recvcount">
              The number of elements to receive from each process.
              This argument is only significant at the root.
            </param>
            <param name="recvtype">
              The type of data that will be stored in the receive buffer.
              This argument is only significant at the root.
            </param>
            <param name="root">The rank of the "root" process.</param>
            <param name="comm">The communicator over which data will be gathered.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Gatherv(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32[],System.Int32[],System.Int32,System.Int32,System.Int32)">
            <summary>
            Gather the values provided by each process into an array containing the contributions of all
            of the processes. This routine differs from <see cref="M:MPI.Unsafe.MPI_Allgather(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32)"/> in that the results 
            are gathered to only the "root" process, which is identified by its <paramref name="rank"/> 
            in the communicator. 
            See <see cref="M:MPI.Intracommunicator.Gather``1(``0,System.Int32)"/>
            </summary>
            <param name="sendbuf">Buffer containing the values that will be sent from this process.</param>
            <param name="sendcount">The number of elements to send.</param>
            <param name="sendtype">The datatype describing the send buffer.</param>
            <param name="recvbuf">
              Buffer that will contain all of the values contributed by every process.
              This argument is only significant at the root.
            </param>
            <param name="recvcounts">
              An array whose ith element is the number of elements to receive from process i.
              This argument is only significant at the root.
            </param>
            <param name="displs">
              An array whose ith element contains the displacement (into <paramref name="recvbuf"/>) at
              which the data from process i will be placed. This argument is only significant at the root.
            </param>
            <param name="recvtype">
              The type of data that will be stored in the receive buffer.
              This argument is only significant at the root.
            </param>
            <param name="root">The rank of the "root" process.</param>
            <param name="comm">The communicator over which data will be gathered.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Reduce(System.IntPtr,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Perform a parallel reduction operation that summarizes the results from the data contributed
            by all of the processes in a communicator. Unlike <see cref="M:MPI.Unsafe.MPI_Allreduce(System.IntPtr,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)"/>, the results 
            of this operation are returned only to the process whose rank is equivalent to 
            <paramref name="root"/>, i.e., the "root" process.
            See <see cref="M:MPI.Intracommunicator.Reduce``1(``0,MPI.ReductionOperation{``0},System.Int32)"/>
            </summary>
            <param name="sendbuf">
              Buffer containing the "outgoing" values contributed by the calling process to the reduction operation.
            </param>
            <param name="recvbuf">
              Buffer that will receive the results of the parallel reduction. This argument is
              only significant at the root.
            </param>
            <param name="count">
              The number of elements in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.
            </param>
            <param name="datatype">
              The type of data in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.
            </param>
            <param name="op">
              The MPI reduction operation to use, which may be one of the predefined reduction operations
              or a user-defined operation created with <see cref="M:MPI.Unsafe.MPI_Op_create(System.IntPtr,System.Int32,System.Int32@)"/>.
            </param>
            <param name="root">
              Identifies the root process (which will receive the intermediate of the reduction) by
              its rank in the communicator <paramref name="comm"/>.
            </param>
            <param name="comm">
              The communicator over which the reduction will occur.
            </param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Reduce_scatter(System.IntPtr,System.IntPtr,System.Int32[],System.Int32,System.Int32,System.Int32)">
            <summary>
            The equivalent of a <see cref="M:MPI.Unsafe.MPI_Reduce(System.IntPtr,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)"/> followed by a <see cref="M:MPI.Unsafe.MPI_Scatterv(System.IntPtr,System.Int32[],System.Int32[],System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)"/>, performing a
            reduction on the data provided in <paramref name="sendbuf"/> and then scattering those results
            to all of the processes. See <see cref="M:MPI.Intracommunicator.ReduceScatter``1(``0[],MPI.ReductionOperation{``0},System.Int32[])"/>.
            </summary>
            <param name="sendbuf">Buffer containing the data to be reduced.</param>
            <param name="recvbuf">Buffer that will receive this process's results.</param>
            <param name="recvcounts">
              An array whose ith element gives the number of results that will go to the process with rank i.
            </param>
            <param name="datatype">The type of data in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.</param>
            <param name="op">The operation used to combine each element in <paramref name="sendbuf"/>.</param>
            <param name="comm">The communicator over which this collective will operate.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Scan(System.IntPtr,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Performs a partial reduction on the data, returning the result from combining the data provided 
            by the first P processes to the process with rank P. 
            See <see cref="M:MPI.Intracommunicator.Scan``1(``0,MPI.ReductionOperation{``0})"/>
            </summary>
            <param name="sendbuf">Buffer containing the data to contribute to the reduction.</param>
            <param name="recvbuf">Buffer that will receive the result of combining the first Rank values.</param>
            <param name="count">Number of values in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.</param>
            <param name="datatype">The type of data in <paramref name="sendbuf"/> and <paramref name="recvbuf"/>.</param>
            <param name="op">The reduction operation used to combine values.</param>
            <param name="comm">The communicator over which the communication will occur.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Scatter(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Scatters data from one process (the "root" process) to all of the processes in a communicator,
            with different parts of the data going to different processes. 
            See <see cref="M:MPI.Intracommunicator.Scatter``1(``0[],System.Int32)"/>.
            </summary>
            <param name="sendbuf">
              Buffer containing the data to be sent. Only significant at the root process.
            </param>
            <param name="sendcount">
              The number of elements to send to each process. Only significant at the root process.
            </param>
            <param name="sendtype">
              The type of data in <paramref name="sendbuf"/>. Only significant at the root process.
            </param>
            <param name="recvbuf">A buffer that will receive the calling process's part of the data.</param>
            <param name="recvcount">The number of elements to receive.</param>
            <param name="rectype">The type of data to receive.</param>
            <param name="root">The rank of the "root" process, which supplies the data.</param>
            <param name="comm">The communicator over which the data will be scattered.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Scatterv(System.IntPtr,System.Int32[],System.Int32[],System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Scatters data from one process (the "root" process) to all of the processes in a communicator,
            with different parts of the data going to different processes. Unlike <see cref="M:MPI.Unsafe.MPI_Scatter(System.IntPtr,System.Int32,System.Int32,System.IntPtr,System.Int32,System.Int32,System.Int32,System.Int32)"/>,
            different processes may receive different amounts of data.
            See <see cref="M:MPI.Intracommunicator.Scatter``1(``0[],System.Int32)"/>.
            </summary>
            <param name="sendbuf">
              Buffer containing the data to be sent. Only significant at the root process.
            </param>
            <param name="sendcounts">
              An array whose ith element contains the number of elements to send to process i.
              Only significant at the root process.
            </param>
            <param name="displs">
              An array whose ith element contains the offset (into <paramref name="sendbuf"/>)
              if the data to be sent to process i. Only significant at the root process.
            </param>
            <param name="sendtype">
              The type of data in <paramref name="sendbuf"/>. Only significant at the root process.
            </param>
            <param name="recvbuf">A buffer that will receive the calling process's part of the data.</param>
            <param name="recvcount">The number of elements to receive.</param>
            <param name="recvtype">The type of data to receive.</param>
            <param name="root">The rank of the "root" process, which supplies the data.</param>
            <param name="comm">The communicator over which the data will be scattered.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Op_create(System.IntPtr,System.Int32,System.Int32@)">
            <summary>
            Creates an MPI operation that invokes a user-provided function. The MPI operation
            can be used with various reduction operations. MPI.NET provides support for user-defined
            operations via the <see cref="T:MPI.Operation`1"/> class.
            </summary>
            <param name="function">A pointer to the user-defined function.</param>
            <param name="commute">Whether this function is commutative.</param>
            <param name="op">Receives the newly-created MPI operation.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Op_free(System.Int32@)">
            <summary>
            Frees an MPI operation created via <see cref="M:MPI.Unsafe.MPI_Op_create(System.IntPtr,System.Int32,System.Int32@)"/>. MPI.NET will automatically
            manage any operations it creates via <see cref="T:MPI.Operation`1"/> when the corresponding
            object is disposed of or finalized.
            </summary>
            <param name="op">The operation to be freed.</param>
        </member>
        <member name="M:MPI.Unsafe.MPI_Alloc_mem(System.IntPtr,System.Int32,System.IntPtr@)">
            <summary>
            Attempts to allocate (unmanaged) memory from MPI. This memory must be
            manually freed with a call to <see cref="M:MPI.Unsafe.MPI_Free_mem(System.IntPtr)"/>.
            This routine is used automatically by the <see cref="T:MPI.UnmanagedMemoryStream"/> to
            allocate memory when serializing or de-serializing objects within MPI.NET.
            </summary>
            <param name="size">The number of bytes to allocate.</param>
            <param name="info">
              Provides extra information to the MPI implementation that may
              help it allocate memory for a specific reason or from a specific
              memory pool.
            </param>
            <param name="baseptr">
              Will receive a pointer to the newly-allocated memory.
            </param>
            <returns>
              <see cref="F:MPI.Unsafe.MPI_SUCCESS"/> if memory is successfully allocated, 
              <see cref="F:MPI.Unsafe.MPI_ERR_NO_MEM"/> if no memory is available.
            </returns>
        </member>
        <member name="M:MPI.Unsafe.MPI_Free_mem(System.IntPtr)">
            <summary>
            Frees memory allocated with <see cref="M:MPI.Unsafe.MPI_Alloc_mem(System.IntPtr,System.Int32,System.IntPtr@)"/>.
            This routine is used automatically by the <see cref="T:MPI.UnmanagedMemoryStream"/> to
            allocate memory when serializing or de-serializing objects within MPI.NET.
            </summary>
            <param name="ptr">The pointer provided by <see cref="M:MPI.Unsafe.MPI_Alloc_mem(System.IntPtr,System.Int32,System.IntPtr@)"/>.</param>
        </member>
        <member name="M:MPI.Unsafe.ComparisonFromInt(System.Int32)">
            <summary>
            Converts the intermediate of an MPI_*_compare operation into a <see cref="T:MPI.Comparison"/> enum value.
            </summary>
        </member>
        <member name="T:MPI.Unsafe.MPI_Status">
            <summary>
            Low-level representation of the status of an MPI communication operation.
            </summary>
            <remarks>
            This structure is used internally by MPI.NET, and most users will instead see the MPI.NET version
            of the message-status class, <see cref="T:MPI.Status"/>. Unless you are interacting directly with the 
            low-level MPI interface (which is not typically recommended), use <see cref="T:MPI.Status"/> instead.
            </remarks>
        </member>
        <member name="F:MPI.Unsafe.MPI_Status.count">
            <summary>
            This field is only available if the underlying MPI is MS-MPI.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_Status.cancelled">
            <summary>
            This field is only available if the underlying MPI is MS-MPI.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_Status.MPI_SOURCE">
            <summary>
            The rank of the source of the MPI message.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_Status.MPI_TAG">
            <summary>
            The MPI message tag associated with this communication.
            </summary>
        </member>
        <member name="F:MPI.Unsafe.MPI_Status.MPI_ERROR">
            <summary>
            The MPI error code associated with this communication.
            TODO: We need a higher-level facility for dealing with this.
            </summary>
        </member>
        <member name="T:MPI.Unsafe.MPI_Copy_function">
            <summary>
            Delegate describing a low-level MPI function used to copy attribute values from a communicator 
            when the communicator is being duplicated. 
            </summary>
            <remarks>
            This function will 
            be defined automatically by MPI.NET when a new <see cref="T:MPI.Attribute"/> is created, and will vary
            depending on the type of the attribute and on the requested <see cref="T:MPI.AttributeDuplication"/>.
            </remarks>
            <param name="comm">The communicator being duplicated.</param>
            <param name="keyval">The attribute's key value.</param>
            <param name="extra_state">
              The extra state associated with the attribute, provided by the user 
              in <see cref="M:MPI.Unsafe.MPI_Keyval_create(MPI.Unsafe.MPI_Copy_function,MPI.Unsafe.MPI_Delete_function,System.Int32@,System.IntPtr)"/>.
            </param>
            <param name="attribute_val_in">
              The attribute value in the communicator to be duplicated.
            </param>
            <param name="attribute_val_out">
              A pointer to the attribute value that will be copied into the
              new communicator. The attribute value will be no larger than an IntPtr.
              The user only needs to set this value if the attribute will be copied,
              as determined by flag.
            </param>
            <param name="flag">
              Set this to a non-zero value to indicate that the attribute should
              be copied.
            </param>
        </member>
        <member name="T:MPI.Unsafe.MPI_Delete_function">
            <summary>
            Delegate describing a low-level MPI function that takes care of de-allocating 
            an attribute when it is deleted from a communicator (or the communicator itself
            is freed).
            </summary>
            
            <remarks>
            Often used when the attribute's value is a pointer to some per-communicator
            data, and the pointer needs to be freed. This function will be defined automatically
            by MPI.NET when a new <see cref="T:MPI.Attribute"/> is created, and will vary
            depending on the type of the attribute and on the requested <see cref="T:MPI.AttributeDuplication"/>.
            </remarks>
            <param name="comm">The communicator.</param>
            <param name="keyval">The attribute being removed from the communicator.</param>
            <param name="attribute_val">The value of this attribute in the communicator.</param>
            <param name="extra_state">
              The extra state provided by the user in <see cref="M:MPI.Unsafe.MPI_Keyval_create(MPI.Unsafe.MPI_Copy_function,MPI.Unsafe.MPI_Delete_function,System.Int32@,System.IntPtr)"/>.
            </param>
        </member>
    </members>
</doc>
